[
  {
    "title": "The Priority Map.",
    "authors": [],
    "year": 2025,
    "journal": "Australasian Journal of Philosophy",
    "url": "https://philpapers.org/rec/BUETPM-2",
    "abstract": "How can we argue, from neural facts, that representational states exhibit some specific representational structure? This paper approaches the question through a case study on the priority map-mechanism that underlies our capacity to orient visual attention. Computational models from cognitive neuroscience describe this mechanism as operating over neural topographic structures. These neural structures exhibit the functional profile of topographic representational structure. I argue that this fact warrants attributing topographic structure to the priority map mechanism's representational states.",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Perception and Disjunctive Belief: A New Problem for Ambitious Predictive Processing.",
    "authors": [],
    "year": 2024,
    "journal": "Australasian Journal of Philosophy",
    "url": "https://philpapers.org/rec/WEKPAD",
    "abstract": "Perception can't have disjunctive content. Whereas you can think that a box is blue or red, you can't see a box as being blue or red. Based on this fact, I develop a new problem for the ambitious predictive processing theory, on which the brain is a machine for minimizing prediction error, which approximately implements Bayesian inference. I describe a simple case of updating a disjunctive belief given perceptual experience of one of the disjuncts, in which Bayesian inference and predictive coding pull in opposite directions, with the former implying that one's confidence in the belief should increase, and the latter implying that it should decrease. Thus, predictive coding fails to approximately implement Bayesian inference across the interface between belief and perception. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Extending the Predictive Mind.",
    "authors": [],
    "year": 2024,
    "journal": "Australasian Journal of Philosophy",
    "url": "https://philpapers.org/rec/CLAETP-2",
    "abstract": "ABSTRACT How do intelligent agents spawn and exploit integrated processing regimes spanning brain, body, and world? The answer may lie in the ability of the biological brain to select actions and policies in the light of counterfactual predictions--predictions about what kinds of futures will result if such-and-such actions are launched. Appeals to the minimization of 'counterfactual prediction errors' (the ones that would result under various scenarios) already play a leading role in attempts to apply the basic toolkit of the neurocomputational theory known as 'predictive processing' to higher cognitive functions such as policy selection and planning. In this paper, I show that this also leads naturally to the discovery and use of extended processing regimes defined across heterogeneous mixtures of biological and non-biological resources. This solves a long-standing puzzle concerning the 'recruitment' of the right non-neural processing resources at the right time. It reveals how (and why) human brains spawn and maintain extended human minds. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Digital Souls: A Philosophy of Online DeathPatrick Stokes, Digital Souls: A Philosophy of Online Death, London: Bloomsbury Academic, 2021, pp. vi + 200, $90 (hardback) / $26.95 (paperback). [REVIEW]",
    "authors": [],
    "year": 2024,
    "journal": "Australasian Journal of Philosophy",
    "url": "https://philpapers.org/rec/ELDDSA",
    "abstract": "In Digital Souls, Patrick Stokes brings together a thoughtful account of personal identity and death with a range of examples from literature and real life, to help us think about the dead online....",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Profiling, Neutrality, and Social Equality.",
    "authors": [],
    "year": 2022,
    "journal": "Australasian Journal of Philosophy",
    "url": "https://philpapers.org/rec/ROSPNA",
    "abstract": "I argue that traditional views on which beliefs are subject only to purely epistemic assessment can reject demographic profiling, even when based on seemingly robust evidence. This is because the moral failures involved in demographic profiling can be located in the decision not to suspend judgment, rather than supposing that beliefs themselves are a locus of moral evaluation. A key moral reason to suspend judgment when faced with adverse demographic evidence is to promote social equality--this explains why positive profiling is dubious, along with more familiar cases of negative profiling, and why profiling is suspect even when no particular action is at stake. My suspension-based view, while compatible with revisionary normative positions, does not presuppose them. Philosophers of all stripes can reject demographic profiling both in thought and deed. ",
    "source_method": "year_navigation_2022_page_1"
  },
  {
    "title": "Two New Doubts about Simulation Arguments.",
    "authors": [],
    "year": 2022,
    "journal": "Australasian Journal of Philosophy",
    "url": "https://philpapers.org/rec/SUMTND",
    "abstract": "Various theorists contend that we may live in a computer simulation. David Chalmers in turn argues that the simulation hypothesis is a metaphysical hypothesis about the nature of our reality, rather than a sceptical scenario. We use recent work on consciousness to motivate new doubts about both sets of arguments. First, we argue that if either panpsychism or panqualityism is true, then the only way to live in a simulation may be as brains-in-vats, in which case it is unlikely that we live in a simulation. We then argue that if panpsychism or panqualityism is true, then viable simulation hypotheses are substantially sceptical scenarios. We conclude that the nature of consciousness has wide-ranging implications for simulation arguments. ",
    "source_method": "year_navigation_2022_page_1"
  },
  {
    "title": "Imaginative Constraints and Generative Models.",
    "authors": [],
    "year": 2021,
    "journal": "Australasian Journal of Philosophy",
    "url": "https://philpapers.org/rec/WILICA-13",
    "abstract": "ABSTRACT How can imagination generate knowledge when its contents are voluntarily determined? Several philosophers have recently answered this question by pointing to the constraints that underpin imagination when it plays knowledge-generating roles. Nevertheless, little has been said about the nature of these constraints. In this paper, I argue that the constraints that underpin sensory imagination come from the structure of causal probabilistic generative models, a construct that has been highly influential in recent cognitive science and machine learning. I highlight several attractions of this account, and I favourably contrast it with Peter Langland-Hassan's account of sensory imagination in terms of the forward models exploited in sensorimotor control. ",
    "source_method": "year_navigation_2021_page_1"
  },
  {
    "title": "Beyond Desire? Agency, Choice, and the Predictive Mind.",
    "authors": [],
    "year": 2020,
    "journal": "Australasian Journal of Philosophy",
    "url": "https://philpapers.org/rec/CLABDA-2",
    "abstract": "'Predictive Processing' is an emerging paradigm in cognitive neuroscience that depicts the human mind as an uncertainty management system that constructs probabilistic predictions of sensory s...",
    "source_method": "year_navigation_2020_page_1"
  },
  {
    "title": "Linguistics and the Parts of the Mind: Or How to Build a Machine Worth Talking To.",
    "authors": [],
    "year": 2019,
    "journal": "Australasian Journal of Philosophy",
    "url": "https://philpapers.org/rec/WOOLAT-9",
    "abstract": "Volume 97, Issue 3, September 2019, Page 625-628.",
    "source_method": "year_navigation_2019_page_1"
  },
  {
    "title": "Surfing Uncertainty: Prediction, Action and the Embodied Mind, by Andy Clark: New York: Oxford University Press, 2016, pp. xviii + 401, 19.99.",
    "authors": [],
    "year": 2018,
    "journal": "Australasian Journal of Philosophy",
    "url": "https://philpapers.org/rec/HUTSUP-2",
    "abstract": "",
    "source_method": "year_navigation_2018_page_1"
  },
  {
    "title": "The Complexity-Coherence Trade-Off in Cognition.",
    "authors": [],
    "year": 2025,
    "journal": "Mind",
    "url": "https://philpapers.org/rec/THOTCT-9",
    "abstract": "I present evidence for a systematic complexity-coherence trade-off in cognition. I show how feasible strategies for increasing cognitive complexity along three dimensions come at the expense of a heightened vulnerability to incoherence. I discuss two normative implications of the complexity-coherence trade-off: a novel challenge to coherence-based theories of bounded rationality and a new strategy for vindicating the rationality of seemingly irrational cognitions. I also discuss how the complexity-coherence trade-off sharpens recent descriptive challenges to dual process theories of cognition.",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "The Logic of Dynamical Systems is Relevant.",
    "authors": [],
    "year": 2025,
    "journal": "Mind",
    "url": "https://philpapers.org/rec/HORTLO-15",
    "abstract": "Lots of things are usefully modelled in science as dynamical systems: growing populations, flocking birds, engineering apparatus, cognitive agents, distant galaxies, Turing machines, neural networks. We argue that relevant logic is ideal for reasoning about dynamical systems, including interactions with the system through perturbations. Thus, dynamical systems provide a new applied interpretation of the abstract Routley-Meyer semantics for relevant logic: the worlds in the model are the states of the system, while the (in)famous ternary relation is a combination of perturbation and evolution in the system. Conversely, the logic of the relevant conditional provides sound and complete laws of dynamical systems. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Manipulation and Machine Induction.",
    "authors": [],
    "year": 2022,
    "journal": "Mind",
    "url": "https://philpapers.org/rec/LIUMAM",
    "abstract": "One type of soft-line reply to manipulation arguments, which I call 'the another-agent reply', focuses on the existence of some controlling agent and how this can undermine the actor's moral responsibility. A well-known challenge to this type of reply is the so-called 'machine induction' case. This paper provides an argument for why 'machine induction' presents no real challenge to the another-agent reply. It further argues that any soft-liner who does not leave room for the existence of some controlling agent in their explanation of why manipulation undermines responsibility will face a dilemma. Thus, instead of presenting a challenge to the another-agent reply, 'machine induction' actually presents a reason in support of it. ",
    "source_method": "year_navigation_2022_page_1"
  },
  {
    "title": "Blockheads! Essays on Ned Block's Philosophy of Mind and Consciousness, eds Adam Pautz and Daniel Stoljar.",
    "authors": [],
    "year": 2022,
    "journal": "Mind",
    "url": "https://philpapers.org/rec/PHIBEO-3",
    "abstract": "If Ned Block were a rockstar he would be Mick Jagger: sartorial, iconic, ever youthful, and still producing hit records after half a century. Fittingly, then, P.",
    "source_method": "year_navigation_2022_page_1"
  },
  {
    "title": "Representation in Cognitive Science, by Nicholas Shea. Oxford: Oxford University Press, 2018. Pp. 292.",
    "authors": [],
    "year": 2021,
    "journal": "Mind",
    "url": "https://philpapers.org/rec/GANRIC",
    "abstract": "A central component of the cognitive revolution is a commitment to explaining behaviour by reference to internal representations of the world. This core aspect.",
    "source_method": "year_navigation_2021_page_1"
  },
  {
    "title": "Why Be Random?",
    "authors": [],
    "year": 2021,
    "journal": "Mind",
    "url": "https://philpapers.org/rec/ICAWBR",
    "abstract": "When does it make sense to act randomly? A persuasive argument from Bayesian decision theory legitimizes randomization essentially only in tie-breaking situations. Rational behaviour in humans, non-human animals, and artificial agents, however, often seems indeterminate, even random. Moreover, rationales for randomized acts have been offered in a number of disciplines, including game theory, experimental design, and machine learning. A common way of accommodating some of these observations is by appeal to a decision-maker's bounded computational resources. Making this suggestion both precise and compelling is surprisingly difficult. Toward this end, I propose two fundamental rationales for randomization, drawing upon diverse ideas and results from the wider theory of computation. The first unifies common intuitions in favour of randomization from the aforementioned disciplines. The second introduces a deep connection between randomization and memory: access to a randomizing device is provably helpful for an agent burdened with a finite memory. Aside from fit with ordinary intuitions about rational action, the two rationales also make sense of empirical observations in the biological world. Indeed, random behaviour emerges more or less where it should, according to the proposal. ",
    "source_method": "year_navigation_2021_page_1"
  },
  {
    "title": "The Language of Thought: A New Philosophical Direction, by Susan Schneider.",
    "authors": [],
    "year": 2019,
    "journal": "Mind",
    "url": "https://philpapers.org/rec/SPRTLO-4",
    "abstract": "The Language of Thought: A New Philosophical Direction, by SchneiderSusan. Cambridge, MA: MIT Press, 2011. Pp. xii + 259.",
    "source_method": "year_navigation_2019_page_1"
  },
  {
    "title": "Surfing Uncertainty: Prediction, Action and The Embodied Mind, by Andy Clark.",
    "authors": [],
    "year": 2018,
    "journal": "Mind",
    "url": "https://philpapers.org/rec/NOASUP",
    "abstract": "Surfing Uncertainty: Prediction, Action and The Embodied Mind, by ClarkAndy. Oxford and New York: Oxford University Press, 2016.",
    "source_method": "year_navigation_2018_page_1"
  },
  {
    "title": "Conscious Experience: A Logical Inquiry.",
    "authors": [],
    "year": 2021,
    "journal": "The Philosophical Review",
    "url": "https://philpapers.org/rec/SETCEA",
    "abstract": "",
    "source_method": "year_navigation_2021_page_1"
  },
  {
    "title": "Exploring by Believing.",
    "authors": [],
    "year": 2021,
    "journal": "The Philosophical Review",
    "url": "https://philpapers.org/rec/AROEBB-2",
    "abstract": "Sometimes, we face choices between actions most likely to lead to valuable outcomes, and actions which put us in a better position to learn. These choices exemplify what is called the exploration/exploitation trade-off. In computer science and psychology, this trade-off has fruitfully been applied to modulating the way agents or systems make choices over time. This article extends the trade-off to belief. We can be torn between two ways of believing, one of which is expected to be more accurate in light of current evidence, whereas the other is expected to lead to more learning opportunity and accuracy in the long run. Further, it is sometimes rationally permissible to choose the latter. The article breaks down the features of action which give rise to the trade-off, and then argues that each feature applies equally well to belief. This conclusion is an instance of a systematic, foreseeable way in which what is rational to believe now depends on what one expects to be doing in the future. That is, epistemic rationality fundamentally concerns time. ",
    "source_method": "year_navigation_2021_page_1"
  },
  {
    "title": "Representation in Cognitive Science.",
    "authors": [],
    "year": 2021,
    "journal": "The Philosophical Review",
    "url": "https://philpapers.org/rec/RESRIC-2",
    "abstract": "",
    "source_method": "year_navigation_2021_page_1"
  },
  {
    "title": "Enactivist Interventions: Rethinking the Mind. [REVIEW]",
    "authors": [],
    "year": 2019,
    "journal": "The Philosophical Review",
    "url": "https://philpapers.org/rec/ARAEIR",
    "abstract": "",
    "source_method": "year_navigation_2019_page_1"
  },
  {
    "title": "Physical Computation: A Mechanistic Account.",
    "authors": [],
    "year": 2018,
    "journal": "The Philosophical Review",
    "url": "https://philpapers.org/rec/ISAPCA",
    "abstract": "",
    "source_method": "year_navigation_2018_page_1"
  },
  {
    "title": "Does calibration mean what they say it means; or, the reference class problem rises again: Does calibration mean what..",
    "authors": [],
    "year": 2025,
    "journal": "Philosophical Studies",
    "url": "https://philpapers.org/rec/HUDCMA-2",
    "abstract": "Discussions of statistical criteria for fairness commonly convey the normative significance of calibration within groups by invoking what risk scores \"mean.\" On the Same Meaning picture, group-calibrated scores \"mean the same thing\" (on average) across individuals from different groups and accordingly, guard against disparate treatment of individuals based on group membership. My contention is that calibration guarantees no such thing. Since concrete actual people belong to many groups, calibration cannot ensure the kind of consistent score interpretation that the Same Meaning picture implies matters for fairness, unless calibration is met within every group to which an individual belongs. Alas only perfect predictors may meet this bar. The Same Meaning picture thus commits a reference class fallacy by inferring from calibration within some group to the \"meaning\" or evidential value of an individual's score, because they are a member of that group. The reference class answer it presumes does not only lack justification; it is very likely wrong. I then show that the reference class problem besets not just calibration but other group statistical criteria that claim a close connection to fairness. Reflecting on the origins of this oversight opens a wider lens onto the predominant methodology in algorithmic fairness based on stylized cases. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "The hard proxy problem: proxies aren't intentional; they're intentional.",
    "authors": [],
    "year": 2025,
    "journal": "Philosophical Studies",
    "url": "https://philpapers.org/rec/JOHTHP",
    "abstract": "This paper concerns the proxy problem: often machine learning programs utilize seemingly innocuous features as proxies for socially-sensitive attributes, posing various challenges for the creation of ethical algorithms. I argue that to address this problem, we must first settle a prior question of what it means for an algorithm that only has access to seemingly neutral features to be using those features as \"proxies\" for, and so to be making decisions on the basis of, protected-class features. Borrowing resources from philosophy of mind and language, I argue that the answer depends on whether discrimination against those protected classes explains the algorithm's selection of individuals. This approach rules out standard theories of proxy discrimination in law and computer science that rely on overly intellectual views of agent intentions or on overly deflationary views that reduce proxy use to statistical correlation. Instead, my theory highlights two distinct ways an algorithm can reason using proxies: either the proxies themselves are meaningfully about the protected classes, highlighting a new kind of intentional content for philosophical theories in mind and language; or the algorithm explicitly represents the protected-class features themselves, and proxy discrimination becomes regular, old, run-of-the-mill discrimination. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Defining consciousness and denying its existence. Sailing between Charybdis and Scylla.",
    "authors": [],
    "year": 2025,
    "journal": "Philosophical Studies",
    "url": "https://philpapers.org/rec/KAMDCA-2",
    "abstract": "Ulysses, the strong illusionist, sails towards the Strait of Definitions. On his left, Charybdis defines \"phenomenal consciousness\" in a loaded manner, which makes it a problematic entity from a physicalist and naturalistic point of view. This renders illusionism attractive, but at the cost of committing a potential strawman against its opponents - phenomenal realists. On the right, Scylla defines \"phenomenal consciousness\" innocently. This seems to render illusionism unattractive. Against this, I show that Ulysses can pass the Strait of Definitions. He should sail straight towards Scylla. Supposedly innocent definitions land a concept that makes illusionism attractive without committing a strawman. Indeed, this concept, which captures what the phenomenal realist means, is explicitly innocent but implicitly loaded. Beyond the Strait lies another danger: the Sirens of Redefinitions. They incite our hero to redefine his terms to salvage verbally (weak) phenomenal realism - judged preferable to overt strong illusionism. Ulysses should resist the Sirens' songs and choose overt strong illusionism over its weak realist reformulation. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Algorithmic fairness and resentment.",
    "authors": [],
    "year": 2025,
    "journal": "Philosophical Studies",
    "url": "https://philpapers.org/rec/BABAFA-2",
    "abstract": "In this paper we develop a general theory of algorithmic fairness. Drawing on Johnson King and Babic's work on moral encroachment, on Gary Becker's work on labor market discrimination, and on Strawson's idea of resentment and indignation as responses to violations of the demand for goodwill toward oneself and others, we locate attitudes to fairness in an agent's utility function. In particular, we first argue that fairness is a matter of a decision-maker's relative concern for the plight of people from different groups, rather than of the outcomes produced for different groups. We then show how an agent's preferences, including in particular their attitudes to error, give rise to their decision thresholds. Tying these points together, we argue that the agent's relative degrees of concern for different groups manifest in a difference in decision thresholds applied to these groups. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Intention reconsideration in artificial agents: a structured account.",
    "authors": [],
    "year": 2025,
    "journal": "Philosophical Studies",
    "url": "https://philpapers.org/rec/CARIRI-4",
    "abstract": "An important module in the Belief-Desire-Intention architecture for artificial agents (which builds on Michael Bratman's work in the philosophy of action) focuses on the task of intention reconsideration. The theoretical task is to formulate principles governing when an agent ought to undo a prior committed intention and reopen deliberation. Extant proposals for such a principle, if sufficiently detailed, are either too task-specific or too computationally demanding. I propose that an agent ought to reconsider an intention whenever some incompatible prospect is sufficiently valuable along some dimension that can be assessed at zero or near-zero computational cost. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Informational richness and its impact on algorithmic fairness.",
    "authors": [],
    "year": 2025,
    "journal": "Philosophical Studies",
    "url": "https://philpapers.org/rec/DIBIRA",
    "abstract": "The literature on algorithmic fairness has examined exogenous sources of biases such as shortcomings in the data and structural injustices in society. It has also examined internal sources of bias as evidenced by a number of impossibility theorems showing that no algorithm can concurrently satisfy multiple criteria of fairness. This paper contributes to the literature stemming from the impossibility theorems by examining how informational richness affects the accuracy and fairness of predictive algorithms. With the aid of a computer simulation, we show that informational richness is the engine that drives improvements in the performance of a predictive algorithm, in terms of both accuracy and fairness. The centrality of informational richness suggests that classification parity, a popular criterion of algorithmic fairness, should be given relatively little weight. But we caution that the centrality of informational richness should be taken with a grain of salt in light of practical limitations, in particular, the so-called bias-variance trade off. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Autonomised harming.",
    "authors": [],
    "year": 2025,
    "journal": "Philosophical Studies",
    "url": "https://philpapers.org/rec/EGGAHT",
    "abstract": "This paper sketches elements of a theory of the ethics of autonomised harming: the phenomenon of delegating decisions about whether and whom to harm to artificial intelligence (AI) in self-driving cars and autonomous weapon systems. First, the paper elucidates the challenge of integrating non-human, artificial agents, which lack rights and duties, into our moral framework which relies on precisely these notions to determine the permissibility of harming. Second, the paper examines how potential differences between human agents and non-human, artificial agents might bear on the permissibility of delegating life-and death decisions to AI systems. Third, and finally, the paper explores a series of resulting complexities. These include the challenge of weighing autonomous systems' promise to reduce harm against the intrinsic value of rectificatory justice as well as the peculiar possibility that delegating harmful acts to AI might render ordinarily impermissible acts permissible. By illuminating what happens when we extend normative theory beyond its traditional boundaries, this discussion offers a starting point for assessing the moral permissibility of delegating consequential decisions to non-human, artificial agents. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Getting Machines to Do Your Dirty Work.",
    "authors": [],
    "year": 2025,
    "journal": "Philosophical Studies",
    "url": "https://philpapers.org/rec/FRAGMT-2",
    "abstract": "Autonomous systems are machines that can alter their behavior without direct human oversight or control. How ought we to program them to behave? A plausible starting point is given by the Reduction to Acts Thesis, according to which we ought to program autonomous systems to do whatever a human agent ought to do in the same circumstances. Although the Reduction to Acts Thesis is initially appealing, we argue that it is false: it is sometimes permissible to program a machine to do something that it would be wrong for a human to do. We advance two main arguments for this claim. First, the way an autonomous system will behave can be known in advance. This knowledge can indirectly affect the behavior of other agents, while the same may not be true at the time the system actually executes its programming. Second, a lack of knowledge of the identities of the victims and beneficiaries can provide a justification during the programming phase that would be unavailable to an agent at the time the autonomous system executes its programming. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "What we owe to decision-subjects: beyond transparency and explanation in automated decision-making.",
    "authors": [],
    "year": 2025,
    "journal": "Philosophical Studies",
    "url": "https://philpapers.org/rec/GRAWWO-3",
    "abstract": "The ongoing explosion of interest in artificial intelligence is fueled in part by recently developed techniques in machine learning. Those techniques allow automated systems to process huge amounts of data, utilizing mathematical methods that depart from traditional statistical approaches, and resulting in impressive advancements in our ability to make predictions and uncover correlations across a host of interesting domains. But as is now widely discussed, the way that those systems arrive at their outputs is often opaque, even to the experts who design and deploy them. Is it morally problematic to make use of opaque automated methods when making high-stakes decisions, like whether to issue a loan to an applicant, or whether to approve a parole request? Many scholars answer in the affirmative. However, there is no widely accepted explanation for why transparent systems are morally preferable to opaque systems. We argue that the use of automated decision-making systems sometimes violates duties of consideration that are owed by decision-makers to decision-subjects, duties that are both epistemic and practical in character. Violations of that kind generate a weighty consideration against the use of opaque decision systems. In the course of defending our approach, we show that it is able to address three major challenges sometimes leveled against attempts to defend the moral import of transparency in automated decision-making. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "The politics of past and future: synthetic media, showing, and telling.",
    "authors": [],
    "year": 2025,
    "journal": "Philosophical Studies",
    "url": "https://philpapers.org/rec/HYSTPO-5",
    "abstract": "Generative artificial intelligence has given us synthetic media that are increasingly easy to create and increasingly hard to distinguish from photographs and videos. Whereas an existing literature has been concerned with how these new media might make a difference for would-be knowers--the viewers of photographs and videos--I advance a thesis about how they will make a difference for would-be communicators--those who embed photos and videos in their speech acts. I claim that the presence of these media in our information environment reduces our ability to show one another things, even as it may increase our resources for telling. And I argue that this has consequences beyond the disruption of knowledge acquisition; showing is a way that we preserve relational equality through superficial asymmetries in political communication, and thereby express respect for our audiences. If synthetic media reduce our options for showing, they then interfere in the way that we manage our relationships in the context of collective political action. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Algorithmic profiling as a source of hermeneutical injustice.",
    "authors": [],
    "year": 2025,
    "journal": "Philosophical Studies",
    "url": "https://philpapers.org/rec/MILAPA-10",
    "abstract": "It is well-established that algorithms can be instruments of injustice. It is less frequently discussed, however, how current modes of AI deployment often make the very discovery of injustice difficult, if not impossible. In this article, we focus on the effects of algorithmic profiling on epistemic agency. We show how algorithmic profiling can give rise to epistemic injustice through the depletion of epistemic resources that are needed to interpret and evaluate certain experiences. By doing so, we not only demonstrate how the philosophical conceptual framework of epistemic injustice can help pinpoint potential, systematic harms from algorithmic profiling, but we also identify a novel source of hermeneutical injustice that to date has received little attention in the relevant literature, what we call epistemic fragmentation. As we detail in this paper, epistemic fragmentation is a structural characteristic of algorithmically-mediated environments that isolate individuals, making it more difficult to develop, uptake and apply new epistemic resources, thus making it more difficult to identify and conceptualise emerging harms in these environments. We thus trace the occurrence of hermeneutical injustice back to the fragmentation of the epistemic experiences of individuals, who are left more vulnerable by the inability to share, compare and learn from shared experiences. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "The AI-design regress.",
    "authors": [],
    "year": 2025,
    "journal": "Philosophical Studies",
    "url": "https://philpapers.org/rec/ROBTAR-12",
    "abstract": "How should we design AI systems that make moral decisions that affect us? When there is disagreement about which moral decisions should be made and which methods would produce them, we should avoid arbitrary design choices. However, I show that this leads to a regress problem similar to the one metanormativists face involving higher orders of uncertainty. I argue that existing strategies for handling this parallel problem give verdicts about where to stop in the regress that are either too arbitrary or too difficult to implement. I propose a new strategy for AI designers that is better than these alternatives. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Attention, Moral Skill, and Algorithmic Recommendation.",
    "authors": [],
    "year": 2025,
    "journal": "Philosophical Studies",
    "url": "https://philpapers.org/rec/SCHAMS-11",
    "abstract": "Recommender systems are artificial intelligence technologies, deployed by online platforms, that model our individual preferences and direct our attention to content we're likely to engage with. As the digital world has become increasingly saturated with information, we've become ever more reliant on these tools to efficiently allocate our attention. And our reliance on algorithmic recommendation may, in turn, reshape us as moral agents. While recommender systems could in principle enhance our moral agency by enabling us to cut through the information saturation of the internet and focus on things that matter, as they're currently designed and implemented they're apt to interfere with our ability to attend appropriately to morally relevant factors. In order to analyze the distinctive moral problems algorithmic recommendation poses, we develop a framework for the ethics of attention and an account of judicious attention allocation as a moral skill. We then discuss empirical evidence suggesting that attentional moral skill can be thwarted and undermined in various ways by algorithmic recommendation and related affordances of online platforms, as well as economic and technical considerations that support this concern. Finally, we consider how emerging technologies might overcome the problems we identify. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Two Types of AI Existential Risk: Decisive and Accumulative.",
    "authors": [],
    "year": 2025,
    "journal": "Philosophical Studies",
    "url": "https://philpapers.org/rec/KASTTO-9",
    "abstract": "The conventional discourse on existential risks (x-risks) from AI typically focuses on abrupt, dire events caused by advanced AI systems, particularly those that might achieve or surpass human-level intelligence. These events have severe consequences that either lead to human extinction or irreversibly cripple human civilization to a point beyond recovery. This decisive view, however, often neglects the serious possibility of AI x-risk manifesting gradually through an incremental series of smaller yet interconnected disruptions, crossing critical thresholds over time. This paper contrasts the conventional decisive AI x-risk hypothesis with what I call an accumulative AI x-risk hypothesis. While the former envisions an overt AI takeover pathway, characterized by scenarios like uncontrollable superintelligence, the latter suggests a different pathway to existential catastrophes. This involves a gradual accumulation of AI-induced threats such as severe vulnerabilities and systemic erosion of critical economic and political structures. The accumulative hypothesis suggests a boiling frog scenario where incremental AI risks slowly undermine systemic and societal resilience until a triggering event results in irreversible collapse. Through complex systems analysis, this paper examines the distinct assumptions differentiating these two hypotheses. It is then argued that the accumulative view can reconcile seemingly incompatible perspectives on AI risks. The implications of differentiating between the two types of pathway--the decisive and the accumulative--for the governance of AI as well as long-term AI safety are discussed. -/- . ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Counter-productivity and suspicion: two arguments against talking about the AGI control problem.",
    "authors": [],
    "year": 2025,
    "journal": "Philosophical Studies",
    "url": "https://philpapers.org/rec/STECAS-15",
    "abstract": "How do you control a superintelligent artificial being given the possibility that its goals or actions might conflict with human interests? Over the past few decades, this concern- the AGI control problem- has remained a central challenge for research in AI safety. This paper develops and defends two arguments that provide pro tanto support for the following policy for those who worry about the AGI control problem: don't talk about it. The first is argument from counter-productivity, which states that unless kept secret, efforts to solve the control problem could be used by a misaligned AGI to counter those very efforts. The second is argument from suspicion, stating that open discussions of the control problem may serve to make humanity appear threatening to an AGI, which increases the risk that the AGI perceives humanity as a threat. I consider objections to the arguments and find them unsuccessful. Yet, I also consider objections to the don't-talk policy itself and find it inconclusive whether it should be adopted. Additionally, the paper examines whether the arguments extend to other areas of AI safety research, such as AGI alignment, and argues that they likely do, albeit not necessarily as directly. I conclude by offering recommendations on what one can safely talk about, regardless of whether the don't-talk policy is ultimately adopted. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Algorithmic profiling as a source of hermeneutical injustice.",
    "authors": [],
    "year": 2024,
    "journal": "Philosophical Studies",
    "url": "https://philpapers.org/rec/MILAPA-8",
    "abstract": "It is well-established that algorithms can be instruments of injustice. It is less frequently discussed, however, how current modes of AI deployment often make the very discovery of injustice difficult, if not impossible. In this article, we focus on the effects of algorithmic profiling on epistemic agency. We show how algorithmic profiling can give rise to epistemic injustice through the depletion of epistemic resources that are needed to interpret and evaluate certain experiences. By doing so, we not only demonstrate how the philosophical conceptual framework of epistemic injustice can help pinpoint potential, systematic harms from algorithmic profiling, but we also identify a novel source of hermeneutical injustice that to date has received little attention in the relevant literature, what we call epistemic fragmentation. As we detail in this paper, epistemic fragmentation is a structural characteristic of algorithmically-mediated environments that isolate individuals, making it more difficult to develop, uptake and apply new epistemic resources, thus making it more difficult to identify and conceptualise emerging harms in these environments. We thus trace the occurrence of hermeneutical injustice back to the fragmentation of the epistemic experiences of individuals, who are left more vulnerable by the inability to share, compare and learn from shared experiences. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Attention, Moral Skill, and Algorithmic Recommendation.",
    "authors": [],
    "year": 2024,
    "journal": "Philosophical Studies",
    "url": "https://philpapers.org/rec/SCHAMS-10",
    "abstract": "Recommender systems are artificial intelligence technologies, deployed by online platforms, that model our individual preferences and direct our attention to content we're likely to engage with. As the digital world has become increasingly saturated with information, we've become ever more reliant on these tools to efficiently allocate our attention. And our reliance on algorithmic recommendation may, in turn, reshape us as moral agents. While recommender systems could in principle enhance our moral agency by enabling us to cut through the information saturation of the internet and focus on things that matter, as they're currently designed and implemented they're apt to interfere with our ability to attend appropriately to morally relevant factors. In order to analyze the distinctive moral problems algorithmic recommendation poses, we develop a framework for the ethics of attention and an account of judicious attention allocation as a moral skill. We then discuss empirical evidence suggesting that attentional moral skill can be thwarted and undermined in various ways by algorithmic recommendation and related affordances of online platforms, as well as economic and technical considerations that support this concern. Finally, we consider how emerging technologies might overcome the problems we identify. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "In defense of virtual veridicalism.",
    "authors": [],
    "year": 2024,
    "journal": "Philosophical Studies",
    "url": "https://philpapers.org/rec/LEEIDO-6",
    "abstract": "This paper defends virtual veridicalism, according to which many perceptual experiences in virtual reality are veridical. My argument centers on perceptual variation, the phenomenon in which perceptual experience appears all the same while being reliably generated by different properties under different circumstances. It consists of three stages. The first stage argues that perceptual variation can occur in color perception without involving misperception. The second stage extends the argument to perceptual variation of space, arguing that it is possible for individuals to perceive distinct physical spaces as having the same experiential space without suffering from systematic misperception. The final stage proceeds to argue that perceptual variation without misperception in color and spatial perception can occur across virtual and ordinary environments. In that sense, given that ordinary experiences are presumably veridical, experiences in virtual reality are also veridical. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Becoming oneself online: narrative self-constitution and the internet.",
    "authors": [],
    "year": 2024,
    "journal": "Philosophical Studies",
    "url": "https://philpapers.org/rec/BORBOO",
    "abstract": "This paper explores how self-identity can be impacted upon by the use of digital and social media. In particular, drawing on a narrative account of selfhood, it argues that some forms of activity and interaction on the internet can support the capacity to be oneself, and foster transformative processes that are self-enhancing. I start by introducing different positions in the philosophical exploration of identity online, critically outlining the arguments of those who hold a \"pessimistic\" and an \"optimistic\" stance respectively. I then expand on the narrative identity framework that has been used to support the optimists' view, arguing that digital and social media use can foster forms of self-understanding that enable us to preserve or develop our identity. More precisely, exploring these dynamics also in relation to the lived experience of mental ill-health, I maintain that internet-enabled technology can support narrative self-constitution in three main ways: (1) by facilitating the processes through which we remember self-defining life-stories; (2) by enabling us to give salience to the stories that we decide should matter the most; and (3) by providing us with opportunities to obtain social uptake for our narratives. I then conclude by dispelling some possible objections to the use of a narrative approach to account for selfhood online. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Facial profiling technology and discrimination: a new threat to civil rights in liberal democracies.",
    "authors": [],
    "year": 2024,
    "journal": "Philosophical Studies",
    "url": "https://philpapers.org/rec/GENFPT-2",
    "abstract": "This paper offers the first philosophical analysis of a form of artificial intelligence (AI) which the author calls facial profiling technology (FPT). FPT is a type of facial analysis technology designed to predict criminal behavior based solely on facial structure. Marketed for use by law enforcement, face classifiers generated by the program can supposedly identify murderers, thieves, pedophiles, and terrorists prior to the commission of crimes. At the time of this writing, an FPT company has a contract with the United States federal government. After recounting how FPT resurrects the same moral problems associated with the pseudoscience of physiognomy, the author of this manuscript develops and defends the 'Liberal Argument Against Facial Discrimination' (LAAFD), which concludes that government use of FPT poses a significant risk of violating the classical liberal value of equality before the law by committing unjust discrimination against groups of people whose faces happen to match FPT classifiers. A key move in the argument suggests how a future scenario that results in widespread discrimination based solely on facial structure could be as unjustified and harmful, mutatis mutandis, as similar discrimination based solely on racial background. In the final section, the author of this paper develops prima facie policy proposals designed to protect classical liberal values if FPT is to be utilized by governments in liberal democratic societies. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Overlapping minds and the hedonic calculus.",
    "authors": [],
    "year": 2024,
    "journal": "Philosophical Studies",
    "url": "https://philpapers.org/rec/ROEOMA",
    "abstract": "It may soon be possible for neurotechnology to connect two subjects' brains such that they share a single token mental state, such as a feeling of pleasure or displeasure. How will our moral frameworks have to adapt to accommodate this prospect? And if this sort of mental-state-sharing might already obtain in some cases, how should this possibility impact our moral thinking? This question turns out to be extremely challenging, because different examples generate different intuitions: If two subjects share very few mental states, then it seems that we should count the value of those states twice, but if they share very many mental states, then it seems that we should count the value of those statesonce. We suggest that these conflicting intuitions can be reconciled if the mental states that matter for welfare have a holistic character, in a way that is independently plausible. We close by drawing tentative conclusions about how we ought to think about the moral significance of shared mental states. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Nativism and empiricism in artificial intelligence.",
    "authors": [],
    "year": 2024,
    "journal": "Philosophical Studies",
    "url": "https://philpapers.org/rec/LONNAE-2",
    "abstract": "Historically, the dispute between empiricists and nativists in philosophy and cognitive science has concerned human and animal minds (Margolis and Laurence in Philos Stud: An Int J Philos Anal Tradit 165(2): 693-718, 2013, Ritchie in Synthese 199(Suppl 1): 159-176, 2021, Colombo in Synthese 195: 4817-4838, 2018). But recent progress has highlighted how empiricist and nativist concerns arise in the construction of artificial systems (Buckner in From deep learning to rational machines: What the history of philosophy can teach us about the future of artificial intelligence. Oxford University Press.). This paper uses nativism and empiricism to address questions about the nature of artificial intelligence and its trajectory. It begins by defining the nativism/empiricism debate in terms of the generality of a system. Nativist systems have initial states with domain-specific features; empiricist systems have initial states with only domain-general features. With the debate framed in this way, it then explores a variety of arguments for nativism and empiricism in AI. These arguments revolve around two different questions which must be distinguished: whether nativism the only possible approach to developing human-level AI (HLAI); and whether nativism is the most practical approach to developing HLAI. On the first question, it argues that nativism is quite clearly not the only possible approach to developing HLAI, as is sometimes suggested. It argues that existing arguments for the necessity of nativism are unconvincing, because they analogize from poverty of the stimulus arguments about humans, while AIs often have access to much more data than humans. Then it argues that the case of evolution gives us a compelling argument against nativism. On the second, practical question, the paper argues that there is a tradeoff between the advantages of encoding innate machinery directly, and the advantages of evolving or learning it. However, as the past decade has shown, empiricism is a much more viable path to greater capability levels, given the 'bitter lesson' (Sutton in _Reinforcement Learning: An Introduction_. MIT press.) that encoding the 'correct' knowledge in AI systems is perennially outperformed by more empiricist methods that leverage large-scale data and computation. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Argumentation-induced rational issue polarisation.",
    "authors": [],
    "year": 2024,
    "journal": "Philosophical Studies",
    "url": "https://philpapers.org/rec/KOPARI-2",
    "abstract": "Computational models have shown how polarisation can rise among deliberating agents as they approximate epistemic rationality. This paper provides further support for the thesis that polarisation can rise under condition of epistemic rationality, but it does not depend on limitations that extant models rely on, such as memory restrictions or biased evaluation of other agents' testimony. Instead, deliberation is modelled through agents' purposeful introduction of arguments and their rational reactions to introductions of others. This process induces polarisation dynamics on its own. A second result is that the effect size of polarisation dynamics correlates with particular types of argumentative behaviour. Polarisation effects can be soothed when agents take into account the opinions of others as premises, and they are amplified as agents fortify their own beliefs. These results underpin the relevance of argumentation as a factor in social-epistemic processes and indicate that rising issue polarisation is not a reliable indicator of epistemic shortcomings. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Artificial consciousness: a perspective from the free energy principle.",
    "authors": [],
    "year": 2024,
    "journal": "Philosophical Studies",
    "url": "https://philpapers.org/rec/WIECLL",
    "abstract": "Does the assumption of a weak form of computational functionalism, according to which the right form of neural computation is sufficient for consciousness, entail that a digital computational simulation of such neural computations is conscious? Or must this computational simulation be implemented in the right way, in order to replicate consciousness? From the perspective of Karl Friston's free energy principle, self-organising systems (such as living organisms) share a set of properties that could be realised in artificial systems, but are not instantiated by computers with a classical (von Neumann) architecture. I argue that at least one of these properties, viz. a certain kind of causal flow, can be used to draw a distinction between systems that merely simulate, and those that actually replicate consciousness. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Group Prioritarianism: Why AI should not replace humanity.",
    "authors": [],
    "year": 2024,
    "journal": "Philosophical Studies",
    "url": "https://philpapers.org/rec/HONGPW",
    "abstract": "If a future AI system can enjoy far more well-being than a human per resource, what would be the best way to allocate resources between these future AI and our future descendants? It is obvious that on total utilitarianism, one should give everything to the AI. However, it turns out that every Welfarist axiology on the market also gives this same recommendation, at least if we assume consequentialism. Without resorting to non-consequentialist normative theories that suggest that we ought not always create the world with the most value, or non-welfarist theories that tell us that the best world may not be the world with the most welfare, I propose a new theory that justifies giving some resources to humanity in the face of overwhelming AI well-being. I call this new theory, \"Group Prioritarianism\". ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "The linguistic dead zone of value-aligned agency, natural and artificial.",
    "authors": [],
    "year": 2024,
    "journal": "Philosophical Studies",
    "url": "https://philpapers.org/rec/LACTLD",
    "abstract": "The value alignment problem for artificial intelligence (AI) asks how we can ensure that the \"values\"--i.e., objective functions--of artificial systems are aligned with the values of humanity. In this paper, I argue that linguistic communication is a necessary condition for robust value alignment. I discuss the consequences that the truth of this claim would have for research programmes that attempt to ensure value alignment for AI systems--or, more loftily, those programmes that seek to design robustly beneficial or ethical artificial agents.",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Instrumental Divergence.",
    "authors": [],
    "year": 2024,
    "journal": "Philosophical Studies",
    "url": "https://philpapers.org/rec/GALIDB",
    "abstract": "The thesis of instrumental convergence holds that a wide range of ends have common means: for instance, self preservation, desire preservation, self improvement, and resource acquisition. Bostrom contends that instrumental convergence gives us reason to think that \"the default outcome of the creation of machine superintelligence is existential catastrophe\". I use the tools of decision theory to investigate whether this thesis is true. I find that, even if intrinsic desires are randomly selected, instrumental rationality induces biases towards certain kinds of choices. Firstly, a bias towards choices which leave less up to chance. Secondly, a bias towards desire preservation, in line with Bostrom's conjecture. And thirdly, a bias towards choices which afford more choices later on. I do not find biases towards any other of the convergent instrumental means on Bostrom's list. I conclude that the biases induced by instrumental rationality at best weakly support Bostrom's conclusion that machine superintelligence is likely to lead to existential catastrophe. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Correction to: Greatest surprise reduction semantics: an information theoretic solution to misrepresentation and disjunction.",
    "authors": [],
    "year": 2022,
    "journal": "Philosophical Studies",
    "url": "https://philpapers.org/rec/WEICTG",
    "abstract": "",
    "source_method": "year_navigation_2022_page_1"
  },
  {
    "title": "Is the brain an organ for free energy minimisation?",
    "authors": [],
    "year": 2022,
    "journal": "Philosophical Studies",
    "url": "https://philpapers.org/rec/WILITB-5",
    "abstract": "Two striking claims are advanced on behalf of the free energy principle in cognitive science and philosophy: that it identifies a condition of the possibility of existence for self-organising systems; and that it has important implications for our understanding of how the brain works, defining a set of process theories--roughly, theories of the structure and functions of neural mechanisms--consistent with the free energy minimising imperative that it derives as a necessary feature of all self-organising systems. I argue that the conjunction of claims and rests on a fallacy of equivocation. The FEP can be interpreted in two ways: as a claim about how it is possible to redescribe the existence of self-organising systems, and as a claim about how such systems maintain their existence. Although the Descriptive FEP plausibly does identify a condition of the possibility of existence for self-organising systems, it has no important implications for our understanding of how the brain works. Although the Explanatory FEP would have such implications if it were true, it does not identify a condition of the possibility of existence for self-organising systems. I consider various ways of responding to this conclusion, and I explore its implications for the role and importance of the FEP in cognitive science and philosophy. ",
    "source_method": "year_navigation_2022_page_1"
  },
  {
    "title": "Greatest surprise reduction semantics: an information theoretic solution to misrepresentation and disjunction.",
    "authors": [],
    "year": 2019,
    "journal": "Philosophical Studies",
    "url": "https://philpapers.org/rec/WEIGSR",
    "abstract": "Causal theories of content, a popular family of approaches to defining the content of mental states, commonly run afoul of two related and serious problems that prevent them from providing an adequate theory of mental content--the misrepresentation problem and the disjunction problem. In this paper, I present a causal theory of content, built on information theoretic tools, that solves these problems and provides a viable model of mental content. This is the greatest surprise reduction theory of content, which identifies the content of a signal as the event the surprisal of which is most reduced by that signal. Conceptually, this amounts to the claim that the content of a signal is the event the probability of which has increased by the largest proportion, or the event that the signal makes the most less surprising to us. I develop the greatest surprise reduction theory of content in four stages. First, I introduce the general project of causal theories of content, and the challenges presented to this project by the misrepresentation and disjunction problems. Next, I review two recent and prominent causal theories of content and demonstrate the serious challenges faced by these approaches, both clarifying the need for a solution to the misrepresentation and disjunction problems and providing a conceptual background for the greatest surprise reduction theory. Then, I develop the greatest surprise reduction theory of content, demonstrate its ability to resolve the misrepresentation and disjunction problems, and explore some additional applications it may have. Finally, I conclude with a discussion of a particularly difficult challenge that remains to be addressed--the partition problem--and sketch a path to a potential solution. ",
    "source_method": "year_navigation_2019_page_1"
  },
  {
    "title": "Understanding as compression.",
    "authors": [],
    "year": 2019,
    "journal": "Philosophical Studies",
    "url": "https://philpapers.org/rec/WILUAC-2",
    "abstract": "What is understanding? My goal in this paper is to lay out a new approach to this question and clarify how that approach deals with certain issues. The claim is that understanding is a matter of compressing information about the understood so that it can be mentally useful. On this account, understanding amounts to having a representational kernel and the ability to use it to generate the information one needs regarding the target phenomenon. I argue that this ambitious new account can accommodate much of the data that has motivated theories of understanding in philosophy of science, and can also be generally applicable in epistemology and daily life as well. ",
    "source_method": "year_navigation_2019_page_1"
  },
  {
    "title": "Are ambient smart environments sufficient for developing good habits?",
    "authors": [],
    "year": 2025,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/BECAAS-3",
    "abstract": "Ambient smart environments (ASEs) have been proposed as technologically enriched spaces capable of supporting adaptive behaviour and healthier habits Aydin (_Philosophy and Technology 32_(2):321-338, 2019); Hipolito et al. (_Synthese 199_(11):4457-4481, 2023); White & Hipolito, (_Cognitive Systems Research, 84,_ 101199, 2024) White & Miller, (_Synthese, 204_(2), 1-24, 2024) White et al. _Synthese, 205_(1), 1-18, 2024); Verbeek (_NanoEthics 3_(3):231-242, 2009). Proponents suggest that ASEs effectively disrupt maladaptive habitual behaviours by introducing optimal levels of uncertainty, implicitly guiding users towards healthier patterns of thinking and acting without requiring deliberate reflection. However, we argue that these interventionist approaches risk overlooking the conditions necessary for the consolidation of more sustainable and beneficial habits. In particular, we argue that such habits require the engagement of motivational states, effortful interaction, and opportunities for further reinforcement. To address this gap, we propose that ASE interventions must be complemented by more stable, deliberately structured, design-centred environments (DEs), which provide consistent affordances that explicitly invite reflective, intrinsically motivated interactions. We emphasise how these hybrid environments scaffold learning and feedback processes, and enable agents to progressively refine their engagement with affordances through experience and reflection. Finally, we discuss the broader philosophical and ethical implications, highlighting how hybrid environments mitigate the risks of techno-paternalism, and actively support adaptive autonomy, reflective control, and meaningful behavioural change. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Rethinking the role of language in arguments for extended cognition.",
    "authors": [],
    "year": 2025,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/DRARTR",
    "abstract": "Clark (_Philosophical Psychology_ 19(3):291-307, 2006) proposes that a standard challenge to the hypothesis of extended cognition can be avoided in the case of linguistically structured cognition, because the role played by our public manipulation of linguistic artifacts is irreducible to the role played by the brain's operations over internal representations. I demonstrate that Clark's argument relies on a view of the brain's cognitive architecture to which he no longer subscribes. I argue that on Clark's later view of the brain as engaged in 'predictive processing', his earlier defense of extended cognition from this challenge is no longer an effective strategy. I explore the implications of this for Clark's attempts to reconcile his previous arguments for extended cognition with his characterization of the predictive-processing brain. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Traces of thinking: a stigmergic approach to 4E cognition.",
    "authors": [],
    "year": 2025,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/SIMTOT-7",
    "abstract": "This paper outlines an approach to analysing minimal cognition that brings out its social and historical dimensions. It proposes a model, the coordinated systems approach (CSA), which understands cognition as a coordinated coalition of loosely autonomous processes responsible for goal-directedness in a system. On this view, even individual cognition has something of a social flavour to it. The central concept of the paper is stigmergy: a process where the material trace of actions of system elements in their environment is a sign that coordinates a group of semi-autonomous processes in future actions - this is the social dimension. The historical dimension refers to longer term processes which establish the coordinative power of the sign and endow it with normative force. According to this proposal, a full explanation of cognitive capabilities should reference both dimensions. In the second half of the paper the CSA is let loose on some puzzles in 4E cognition. Can the model deal with old problems such as that of cognitive bloat, or new problems such as the supposed external memory of the slime mould Physarum polycephalum? Potentially, the approach could be used to analyse minimal cognitive phenomena over a range of scales from bacteria to human beings. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Fairness and randomness in decision-making: the case of decision thresholds.",
    "authors": [],
    "year": 2025,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/VREFAR",
    "abstract": "This paper defends the role of lotteries in fair decision-making. It does so by targeting the use of decision thresholds to convert algorithmic predictions and classifications into decisions. Using an account of fairness from John Broome, the paper argues that decision thresholds are sometimes unfair, and that lotteries would be a fairer allocation method. It closes by dealing with two objections. First, it deals with the objection that lotteries should only be used to break ties in cases where individuals' claims are equally strong. Here, the paper gives a new argument for Broome's view, targeting decision criteria that are arbitrary and highly standardized. It then defends the arguments of the paper against the objection that lotteries are not morally superior to other methods of arbitrary choosing. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "The extracted mind.",
    "authors": [],
    "year": 2025,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/LOOTEM",
    "abstract": "Since Clark and Chalmers advanced \"The Extended Mind\" in 1998, a persistent dispute evolved on how our tool interactions shape the kind of cognition we have. Extended cognition generally views us as cognitively augmented and enhanced by our tool practices, which shall render our cognitive constitution extended to those tools. Bounded and embedded cognition have primarily criticized this metaphysical claim. However, another contender may arise from considering how we use more intelligent tools. We arguably employ advanced technologies that capture, mimic, and then replace our cognitive skills, which we then no longer need to exercise ourselves. This precedes any metaphysical debate, since such practices might stand in a more fundamental conflict with extended cognition. The counter-hypothesis of extracted cognition states that we primarily tend to use tools that initially attain and eventually displace our cognitive responsibilities and involvements. This paper evaluates extended and extracted cognition by comparing theoretical, practical, and ethical arguments respectively. If extracted cognition describes most convincingly how such tool interactions shape our kind of cognition, then we may also endorse \"The Extracted Mind\". ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Believing in default rules: inclusive default reasoning.",
    "authors": [],
    "year": 2025,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/ANDBID",
    "abstract": "This paper argues for the reasonableness of an _inclusive_ conception of default reasoning. The inclusive conception allows untriggered default rules to influence beliefs: Since a default \"from \\varphi , infer \\psi \" is a defeasible inference rule, it by default warrants a belief in the material implication \\varphi \\rightarrow \\psi , even if \\varphi  is not believed. Such inferences are not allowed in standard default logic of the Reiter tradition, but are reasonable by analogy to the Deduction Theorem for classical logic. Our main contribution is a formal framework for inclusive default reasoning. The framework has a solid philosophical foundation, it draws conclusions non-trivially different from non-inclusive frameworks, and it exhibits a host of benchmark properties deemed desirable in the literature--e.g., that extensions always exist and are consistent. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Extended cognition, mutual manipulability, and the relevance of scientific evidence: a reformulation of the matching criterion.",
    "authors": [],
    "year": 2025,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/CORECM",
    "abstract": "The mutual manipulability theory has been proposed as a crucial criterion for studying the relation of constitution in cases of extended cognition. In its most recent formulation (Craver et al., Synthese, 199(3):8807-8828, 2021.), this theory has been integrated with a matching condition imposing some qualitative and quantitative caveat on the activities of the constituent under scrutiny. Although suitable for empirical assessment, this requirement encounters different limitations when used to analyze actual scientific evidence. The current paper addresses this issue, reformulating Craver et al.'s matching condition. The revised criterion focuses on detecting relevant scientific evidence concerning the activities of the targeted constituent. In particular, it proposes to assess some marker of the interaction between the external, non-biological constituent and other biological components involved in the mechanisms underneath the (putatively) extended phenomenon. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Causal ontology and definiteness of consciousness.",
    "authors": [],
    "year": 2025,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/MOOCOA-6",
    "abstract": "Integrated information theory (IIT) holds that the axioms of phenomenal existence specify both the existence and essential properties of consciousness, whereas the postulates of physical existence describe the conditions a physical substrate of consciousness must satisfy to account for these properties. Among the five axioms and their corresponding postulates, the principle of exclusion has received particular attention. This paper examines the reasoning behind the translation of the exclusion axiom into the exclusion postulate. Section 2 clarifies the exclusion axiom and defends it against common objections. Section 3 explores the broader question of how axioms should be translated into postulates within IIT's framework. Section 4 outlines the ontological principles underlying the latest formulation of IIT, whereas Sect. 5 focuses on the justification of the second principle, which clarifies how causal powers should be ascribed to physical entities. Drawing on these discussions, the exclusion postulate is then abductively inferred from the exclusion axiom. Section 6 considers potential objections to this inference and assesses their implications. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "ChatGPT, extended: large language models and the extended mind.",
    "authors": [],
    "year": 2025,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/SMACEL",
    "abstract": "Recent research has relied on the use of fine-tuning techniques to incorporate philosophical knowledge into Large Language Models (LLMs). The present paper outlines an alternative approach to the development of such systems--one that is rooted in a technique known as Retrieval-Augmented Generation (RAG). In contrast to fine-tuning, RAG does not seek to adjust the internal parameters (or internal memory) of an LLM. Instead, RAG relies on the retrieval of information from an externally-situated store, which functions as a form of non-parametric (or external) memory. Applying this technique to the works of the contemporary philosopher Andy Clark yields Digital Andy: an LLM that is able to respond to questions about the extended mind. This serves as a practical demonstration of RAG-based techniques, highlighting how philosophical knowledge can be 'incorporated' into an LLM without the need for additional machine learning. But Digital Andy's reliance on extra-systemic resources also raises questions about the scope of active externalist theorizing, encouraging us to consider Digital Andy's status as an extended cognitive/computational system. Addressing these questions reveals some interesting points of convergence between the philosophical effort to understand the extended mind and the technological effort to build the next generation of LLMs. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "The multiplicity objection against uploading optimism.",
    "authors": [],
    "year": 2025,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/WEBTMO-5",
    "abstract": "Could we transfer you from your biological substrate to an electronic hardware by simulating your brain on a computer? The answer to this question divides optimists and pessimists about mind uploading. Optimists believe that you can genuinely survive the transition; pessimists think that surviving mind uploading is impossible. An influential argument against uploading optimism is the multiplicity objection. In a nutshell, the objection is as follows: If uploading optimism were true, it should be possible to create not only one, but multiple digital versions of you. However, you cannot literally become many. Hence, you cannot survive even a single instance of uploading, and optimism about uploading is misguided. In this paper, I will first spell out the multiplicity objection in detail and then provide a two-pronged defence against the objection. First, uploading pessimists cannot establish that uploading optimism has the contentious implication. Second, it is in fact plausible to think that we could become multiple distinct persons. Optimists' hope for a digital afterlife is therefore not thwarted by the prospect of multiplicity. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Do language models lack communicative intentions?",
    "authors": [],
    "year": 2025,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/ATTDLM",
    "abstract": "In some recent work, some psychologists, linguists, AI researchers, and philosophers (e.g., Shanahan, 2022; Bender & Koller, 2020; Montemayor, 2021; Bender et al., 2021) have argued that, despite producing convincing human-like linguistic output, large language models do not possess linguistic competence on the ground that they lack communicative intention. Among the proponents of this position, the notion of communicative intention is entertained as the liveliest candidate for a distinguishing characteristic of human cognition vis-a-vis LLMs and as such as at least one of the features which accounts for the characteristic mentality of humans. I demonstrate that such a position is largely driven by strong Gricean assumptions about the nature of linguistic competence that I argue is now known to be empirically untenable. Moreover, I show that once we give up these assumptions and fairly assess the architecture of both LLM and human cognitive architectures, we will find no good reasons to suppose large language models lack communicative intentions. I thus reject communicative intentions as a viable ground on which to understand the difference between LLMs and humans and thus as a characteristic mark of human mentality. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Can simulations aid counterfactual reasoning?",
    "authors": [],
    "year": 2025,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/GANCSA-3",
    "abstract": "Some artificial intelligence systems attempt to implement counterfactual reasoning using computer simulations. But differences between the logics of counterfactuals and simulations suggest that the reliability of simulation as a means of counterfactual reasoning may be limited. On the usual understanding of counterfactuals, 'If a vehicle had decelerated, snow would have been white' is true, and the actual world factors into the evaluation of counterfactuals with true antecedents. But we would expect 'In a simulation of a decelerating vehicle, snow would be white' to be false, and for the actual world not to factor into our consideration of what happens in simulations with true stipulations. This paper formulates a logic to model simulation-based reasoning and identifies necessary conditions for the logics of counterfactuals and simulations to align. The investigations imply that simulation-based artificial systems can reliably implement counterfactual reasoning only insofar as it has an adequate notion of similarity between worlds, relevance between propositions, and compatibility between stipulations. Implications for the prospects of simulation-based artificial intelligence are discussed. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Generative midtended cognition and Artificial Intelligence: thinging with thinging things.",
    "authors": [],
    "year": 2025,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/BARGMC-2",
    "abstract": "This paper introduces the concept of \"generative midtended cognition\", that explores the integration of generative AI technologies with human cognitive processes. The term \"generative\" reflects AI's ability to iteratively produce structured outputs, while \"midtended\" captures the potential hybrid (human-AI) nature of the process. It stands between traditional conceptions of _in_tended creation, understood as steered or directed from with_in_, and _ex_tended processes that bring exo-biological processes into the creative process. We examine the working of current generative technologies (based on multimodal transformer architectures typical of large language models like ChatGPT) to explain how they can transform human cognitive agency beyond what the conceptual resources of standard theories of extended cognition can capture. We suggest that the type of cognitive activity typical of the coupling between a human and generative technologies is closer (but not equivalent) to social cognition than to classical extended cognitive paradigms. Yet, it deserves a specific treatment. We provide an explicit definition of _generative midtended cognition_ in which we treat interventions by AI systems as constitutive of the agent's intentional creative processes. Furthermore, we distinguish two dimensions of generative hybrid creativity: 1. Width: captures the sensitivity of the context of the generative process (from the single letter to the whole historical and surrounding data), 2. Depth: captures the granularity of iteration loops involved in the process. Generative midtended cognition stands in the middle depth between _conversational_ forms of cognition in which complete utterances or creative units are exchanged, and micro-cognitive (e.g. neural) subpersonal processes. Finally, the paper discusses the potential risks and benefits of widespread generative AI adoption, including the challenges of authenticity, generative power asymmetry, and creative boost or atrophy. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "A logic for bounded multi-agent reasoning: deductive inference, introspection, and attribution.",
    "authors": [],
    "year": 2025,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/SOLALF-3",
    "abstract": "Logics for belief as spin-offs of normal modal logics are susceptible to criticisms regarding their potential to model human reasoning, especially in light of empirical evidence on people's performance in reasoning tasks. The agents are modelled as unlimited reasoners, who perform deductive inferences, introspect, and reason about others' reasoning, despite bounds of memory or time. To amend this, we propose a hyperintensional doxastic logic for reasoning in a multi-agent setting, which is better aligned with facts on human cognition. We introduce (i) a resource-sensitive impossible-worlds semantics, to account for the fallibility of real reasoners, and (ii) dynamic operators and model updates, inspired by Dynamic Epistemic Logic (DEL), to represent actions that, when affordable, can refine the zero- or higher- order beliefs of agents. The resulting system is applied to case-studies of multi-agent reasoning scenarios. It is thus argued that the framework does justice to non-idealized agents and that it does so in a uniform way, i.e. applicable to both deductive and higher-order reasoning, and in a balanced way, i.e. avoiding criticisms often fired against similar attempts. We finally illustrate a technical connection between this and syntactic approaches against the problem of logical omniscience; this allows us to exploit DEL techniques in obtaining a sound and complete axiomatization. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Beyond the extended mind: new arguments for extensive enactivism.",
    "authors": [],
    "year": 2025,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/KIRBTE",
    "abstract": "Clark and Chalmers (Analysis 58:7-19, 1998) landmark paper, The Extended Mind, launched a thousand ships and changed the contours of the larger sea of theorizing about cognition. Over the past twenty-six years, it has led to intense philosophical debates about of the constitutive bounds of mind and cognition and generated multiple waves of work taking the form of various attempts to clarify and defend its core thesis. The extended mind thesis states that under certain (specialized and particular) conditions cognitive processes may be constituted by resources distributed across the brain, the body, and the environment. The extended mind thesis is part of a larger family of theoretical frameworks such as embodied cognition, distributed cognition, and various versions of enactivism (Gallagher in South J Philos 56: 421-447, 2018; Hutchins in Cognition in the wild, The MIT Press, 1995; Varela et al. in The embodied mind: cognitive science and human experience, The MIT Press, 1991; Di Paolo in Topoi 28:9-21, 2009; Hutto and Myin in Radicalizing enactivism: basic minds without content, The MIT Press, 2013; Hutto and Myin in Evolving enactivism: basic minds meet content, The MIT Press, 2017). In this paper we revive and clarify the commitments of Radical Enactivism's Extensive Enactivism, compare it to alternatives, and provide new arguments and analyses for preferring it over what is on offer from other members of the extended-distributed-enactive family of positions. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Bayes meets Hegel: the dialectics of belief space and the active inference of suffering.",
    "authors": [],
    "year": 2025,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/KRUBMH",
    "abstract": "The Bayesian brain hypothesis conceives of the brain as a generative model (GM) of its environment, where the model improves its accuracy by updating itself via Bayesian inferential statistics. This is accomplished by adjusting the model's prior beliefs into posterior beliefs based on the sensory input from the environment. Thus, the Bayesian brain learns the causal structure of world events and refines its belief space. The Bayesian brain is thought to learn by updating its beliefs and the parameters of its GM. It is less clear whether Bayesian updating alone is sufficient to explain the genesis and evolution of belief space, as it appears challenging to explain the generation of truly novel original priors (structure learning) or the transition to the direct opposite of the initial prior through Bayesian updating. To address this challenge, we suggest integrating Bayesian updating with the principles of the dialectical development of thought as conceived in Hegel's work. We argue that applying dialectics and the freeenergy principle to Bayesian inference makes the evolution of belief space both its emergent and imperative property. We naturalize this idea by illustrating how behaviors of different complexity from the molecular mechanisms of the simplest biological behavior, such as bacterial chemotaxis, through psychopathology can be viewed in the 'dialectical Bayes' framework. This framework is used to explore the cognitive dynamics of physical and emotional pain and to propose a mechanism of chronic suffering. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Extending the extended self: a mediational-constitutional proposal.",
    "authors": [],
    "year": 2025,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/PREETE-2",
    "abstract": "This paper explores the mediational and constitutional role of technical images (both analogue and digital) for the definition and continuity of the sense of self, proposing a new framework within the extended mind theory grounded in recent approaches to memory. Traditional cognitive science often views the mind as confined within the brain, but we argue that cognition is not merely extended but fundamentally constituted through ongoing material engagements with technical images within specific sociocultural contexts. Our interdisciplinary approach integrates cognitive archaeology, media theory, and cognitive sciences, emphasising the dynamic, embodied, and situated nature of cognition. This perspective shifts the understanding of the mind from a static, internal entity to a dynamic, distributed process continually mediated through organismic transactions in the environment. Building on this tradition of studies in the extended mind, we introduce the Mediational-Constitutional Principle, arguing that technical images not only trigger cognitive processes but actively constitute them. In the paper, we illustrate how technical images mediate and constitute the sense of self because they are central in how memory is continually re-enacted in specific sociomaterial environments made of people, things, and practices. These images function as automatic ecological records, blending past and present, influencing personal narratives and memory. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "The AI-mediated communication dilemma: epistemic trust, social media, and the challenge of generative artificial intelligence.",
    "authors": [],
    "year": 2025,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/SAHTAC",
    "abstract": "The rapid adoption of commercial Generative Artificial Intelligence (Gen AI) products raises important questions around the impact this technology will have on our communicative interactions. This paper provides an analysis of some of the potential implications that Artificial Intelligence-Mediated Communication (AI-MC) may have on epistemic trust in online communications, specifically on social media. We argue that AI-MC poses a risk to epistemic trust being diminished in online communications on both normative and descriptive grounds. Descriptively, AI-MC seems to (roughly) lower levels of epistemic trust. Normatively, we argue that this brings about the following dilemma. On the one hand, there are at least some instances where we should epistemically trust AI-MC less, and therefore the reduction in epistemic trust is justified in these instances. On the other hand, there are also instances where we epistemically trust AI-MC less, but this reduction in epistemic trust is not justified, resulting in discrimination and epistemic injustice in these instances. The difficulty in knowing which of these two groups any instance of AI-MC belongs to brings about the AI-MC dilemma: We must choose between maintaining normal levels of epistemic trust and risking epistemic gullibility when reduced trust is justified, or adopting generally reduced epistemic trust and risking epistemic injustice when such reduced trust is unjustified. Navigating this choice between problematic alternatives creates a significant challenge for social media as an epistemic environment. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "We have never been Cartesian.",
    "authors": [],
    "year": 2025,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/TOOWHN",
    "abstract": "Humanism is the traditional approach in many disciplines, including history, philosophy, anthropology, and sociology. Humanists tend to assume that there is an important distinction between the knowing subject and the rest of the world. Posthumanism challenges this assumption, asking us to examine how the distinction between subject and object is constructed in the first place. To do so, posthumanists argue that we must abandon our traditional resources for understanding the knowing subject. In its place, we must adopt a radical new notion of agency that encompasses humans and nonhumans. This paper argues that, although posthumanism teaches us a valuable lesson, the real problem lies not with humanism, but with Cartesianism. To show this, I develop a new version of humanism by drawing on two anti-Cartesian strands within recent philosophy of mind: _the extended mind thesis_ and _mental fictionalism_. The result is a view that allows us to retain our traditional resources for understanding the knowing subject, while recognising that the nature of that subject can change over time. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "How to be a realist about computational neuroscience.",
    "authors": [],
    "year": 2025,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/WILHTB-7",
    "abstract": "Recently, a version of realism has been offered to address the simplification strategies used in computational neuroscience. According to this view, computational models provide us with knowledge about the brain, but they should not be taken literally in _any_ sense, even rejecting the idea that the brain performs computations (computationalism). I acknowledge the need for considerations regarding simplification strategies in neuroscience and how they contribute to our interpretations of computational models; however, I argue that whether we should accept or reject computationalism about the brain is a separate issue that can be addressed independently by a philosophical theory of physical computation. This takes seriously the idea that the brain performs computations while also taking an analogical stance toward computational models in neuroscience. I call this version of realism \"Analogical Computational Realism.\" Analogical Computational Realism is a realist view in virtue of being committed to computationalism while taking certain computational models to pick out real patterns that provide a how-possibly explanation without also thinking that the model is literally implemented in the brain. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Cognitive offloading and the causal structure of human action.",
    "authors": [],
    "year": 2025,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/BRICOA-6",
    "abstract": "The hypothesis of extended cognition (HEC) casts human cognition as constitutively dependent on its bodily and environmental context. Drawing on recent empirical work on 'cognitive offloading', HEC's defenders claim that information processing offloaded onto such brain-external resources is sometimes 'genuinely' cognitive. But while debates about offloading have a high profile in philosophy of cognitive science, surprisingly little attention has been paid to the fact that paradigm cases of offloading are intentional actions. As a result, opposition to HEC is driven in part by unarticulated intuitions about the metaphysics of human agency. Thinking of action as a kind of interface between separately constituted intentions and bodily movements can make HEC look problematic. But while this view of agency has been popularised - most prominently by Donald Davidson - as analytic philosophy of action's 'standard story', it has come under pressure from philosophers influenced by Elizabeth Anscombe's very different account. According to this, actions express an agent's intention only in so far as they fit into the right kind teleologically structured worldly context. In this paper, I'll argue that HEC's supporters should adopt an Anscombean model of action. That is, they should understand cognitive offloading as a manifestation of human agency's general dependence on its bodily and environmental setting. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Home as mind: AI extenders and affective ecologies in dementia care.",
    "authors": [],
    "year": 2025,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/KRUHAM",
    "abstract": "I consider applications of \"AI extenders\" to dementia care. AI extenders are AI-powered technologies that extend minds in ways interestingly different from old-school tech like notebooks, sketch pads, models, and microscopes. I focus on AI extenders as ambiance: so thoroughly embedded into things and spaces that they fade from view and become part of a subject's taken-for-granted background. Using dementia care as a case study, I argue that ambient AI extenders are promising because they afford richer and more durable forms of multidimensional integration than do old-school extenders like Otto's notebook. They can be tailored, in fine-grained ways along multiple timescales, to a user's particular needs, values, and preferences--and crucially, they can do much of this self-optimizing on their own. I discuss why this is so, why it matters, and its potential impact on affect and agency. I conclude with some worries in need of further discussion. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Of opaque oracles: epistemic dependence on AI in science poses no novel problems for social epistemology.",
    "authors": [],
    "year": 2025,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/ORTOOO",
    "abstract": "Deep Neural Networks (DNNs) are epistemically opaque in the sense that their inner functioning is often unintelligible to human investigators. Inkeri Koskinen has recently argued that this poses special problems for a widespread view in social epistemology according to which thick normative trust between researchers is necessary to handle opacity: if DNNs are essentially opaque, there simply exists nobody who could be trusted to understand all the aspects a DNN picks up during training. In this paper, I present a counterexample from scientific practice, AlphaFold2. I argue that for epistemic reliance on an opaque system, trust is not necessary, but reliability is. What matters is whether, for a given context, the reliability of a DNN has been compellingly established by empirical means and whether there exist trustable researchers who have performed such evaluations adequately. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Shifting boundaries, extended minds: ambient technology and extended allostatic control.",
    "authors": [],
    "year": 2025,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/WHISBE",
    "abstract": "This article applies the thesis of the extended mind to ambient smart environments. These systems are characterised by an environment, such as a home or classroom, infused with multiple, highly networked streams of smart technology working in the background, learning about the user and operating without an explicit interface or any intentional sensorimotor engagement from the user. We analyse these systems in the context of work on the \"classical\" extended mind, characterised by conditions such as \"trust and glue\" and phenomenal transparency, and find that these conditions are ill-suited to describing our engagement with ambient smart environments. We then draw from the active inference framework, a theory of brain function which casts cognition as a process of embodied uncertainty minimisation, to develop a version of the extended mind grounded in a process ontology, where the boundaries of mind are understood to be multiple and always shifting. Given this more fluid account of the extended mind, we argue that ambient smart environments should be thought of as extended allostatic control systems, operating more or less invisibly to support an agent's biological capacity for minimising uncertainty over multiple, interlocking timescales. Thus, we account for the functionality of ambient smart environments as extended systems, and in so doing, utilise a markedly different version of the classical thesis of extended mind. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Good classification matters: conceptual engineering in data science.",
    "authors": [],
    "year": 2025,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/KHLGCM",
    "abstract": "Recent years have seen incredible advances in our abilities to gather and store data, as well as in the computational power and methods--most prominently in machine learning--to do things with those data. These advances have given rise to the emerging field \"data science.\" Because of its immense power for providing practically useful information about the world, data science is a field of increasing importance. This paper argues that a core part of what data scientists are doing should be understood as conceptual engineering. At all stages of the data science process, data scientists need to deliberate about, evaluate, and make classificatory choices in a variety of ways, including as part of training and evaluating machine learning models. Viewing these activities as involved in conceptual engineering offers a new way to think about them, one that helps to clarify what is at stake in them, what sorts of considerations are relevant, and how to systematically think about the choices faced. Given the increasing importance of data science, if conceptual engineering is relevant for activities in data science, this also highlights the relevance and impact of conceptual engineering as a method. Furthermore, the paper also suggests that machine learning opens distinctive and novel ways in which data scientists engage in conceptual engineering. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Meaning and understanding in large language models.",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/HAVMAU",
    "abstract": "Can a machine understand the meanings of natural language? Recent developments in the generative large language models (LLMs) of artificial intelligence have led to the belief that traditional philosophical assumptions about machine understanding of language need to be revised. This article critically evaluates the prevailing tendency to regard machine language performance as mere syntactic manipulation and the imitations of understanding, which is only partial and very shallow, without sufficient grounding in the world. The article analyses the views on possible ways of grounding as a condition for successful understanding in LLMs and offers an alternative way in view of the prevailing belief that the success of understanding depends mainly on the referential grounding. An alternative conception seeks to show that semantic fragmentism offers a viable account of natural language understanding and explains how LLMs ground the meanings of linguistic expressions. Uncovering how meanings are grounded allows us to also explain why LLMs' ability to understand is possible and so remarkably successful. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "What is a theory of neural representation for?",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/RICWIA-8",
    "abstract": "This paper asks how representational notions figure into cognitive science, especially neuroscience. Philosophers have a way of skipping over that question and going straight to another: _what is neural representation?_ What is the property or relation that representational notions pick out? I argue that this is a mistake. Our ultimate questions, as philosophers of cognitive science, are about the function and epistemology of cognitive scientific explanations--in this case, explanations that use representational notions. To answer those questions we must understand what representational notions contribute to science: what they enable scientists to do or explain, and how. But I show that we can do this without raising traditional and vexing questions about the definition of neural representation, or the nature of a property or relation that notion picks out. Taking this approach, I defend a realist account of representational explanation that underwrites important connections between philosophy and neuroscience. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Authenticity in algorithm-aided decision-making.",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/KARAIA-5",
    "abstract": "I identify an undertheorized problem with decisions we make with the aid of algorithms: the problem of inauthenticity. When we make decisions with the aid of algorithms, we can make ones that go against our commitments and values in a normatively important way. In this paper, I present a framework for algorithm-aided decision-making that can lead to inauthenticity. I then construct a taxonomy of the features of the decision environment that make such outcomes likely, and I discuss three possible solutions to the problem. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Digitally extended knowledge.",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/KALDEK",
    "abstract": "The hypothesis of extended cognition says that cognitive processes and mental states extend to include extra-organismic parts of the external world, provided certain conditions on cognitive integration are satisfied. Moreover, if knowledge is assumed to be a mental state, knowledge is, by a similar line of reasoning, equally extended. However, extended knowledge presents an additional challenge to do with cognitive bloat, given that knowledge is widely regarded as a distinctive cognitive achievement. This paper explores the idea that different types of software systems, rather than specific external devices, may serve to extend knowledge beyond our bodily boundaries, and if so whether this new hypothesis of digitally extended knowledge leads to any untoward expansion of knowledge. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "The 'NeuroGate': neuromorphic intelligence, extended mind, and neurorights.",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/FARTNN-3",
    "abstract": "This article discusses recent advancements in neurotechnologies and how they seem to support the Extended Mind Thesis (EMT), while also raising concerns about the mental integrity and privacy of individuals. In Sect. 1 we review recent research carried out at the frontiers of Brain Machine Interfaces (BMIs) and neuromorphic computing. Taking inspiration (Sect. 2) from research in these fields we present -with the help of some imagination- a futurist scenario of complete human-computer integration. We discuss a set of practical benefits and risks associated with the implementation of such a scenario, which -despite being fictional at the time of writing- is likely to be realized soon. We frame our discussion (Sect. 3) in the context of research conducted in the cognitive sciences on the extended mind thesis (EMT). We reflect on the epistemic and ontic aspects underlying such a scenario and argue that EMT finds supports in it. We further problematize (Sect. 4) around the significance of this envisaged scenario for research on neuro-rights. We therefore also discuss a series of ethical challenges related to their potential infringement. We conclude the paper (Sect. 5) by showing that the development of specific neurotechnologies can make the extension of the mind an increasingly pervasive and transformative phenomenon, while raising important concerns about the potential consequences for certain characteristics of individuals that may be crucial to preserve through the introduction of neurorights. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "The ontology of videogames.",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/DECTOO-4",
    "abstract": "What are the identity and persistence conditions of videogames? This paper surveys the contemporary philosophical literature on this topic. Specifically, I discuss various views which attempt to ground the identity of videogame works in their rules, in their algorithmic structure, in their source code, or in contextual parameters surrounding gameplay. While these proposals all have merits of their own, I argue that none of them are satisfactory. My conclusion is therefore negative: we still lack an adequate theoretical model to account for the identity and persistence conditions of videogames. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Do opaque algorithms have functions?",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/HURDOA-2",
    "abstract": "The functions of technical artifacts are closely associated with design. Increasingly, however, we depend on technologies that are not designed: algorithms produced using machine learning (ML). Machine learning uses automated optimization processes to produce algorithms that are often opaque even to developers. I argue that these opaque ML models cannot be ascribed functions on the leading design-based account, the ICE theory of Houkes and Vermaas (Technical functions: On the use and design of artefacts, Springer, 2010). Specifically, I argue that the form of rational justification that the ICE theory provides for the uses of artifacts in typical cases cannot be provided for opaque algorithms, since developers lack the necessary \"support beliefs\" about how the algorithm works. This does not mean that opaque ML models have no proper functions at all; rather, it lends support to non-intentional theories of technical functions, modeled after function theories in the sciences. However, these theories are meant to perform different work than the ICE theory. As a result, the kind of justification they provide for artifact use is weaker than that provided by the ICE theory and fails to justify many ongoing uses of algorithms. The paper therefore has two upshots. First, opaque ML models lack intentional technical functions. Second, while they may have functions on alternative theories, in many cases these functional ascriptions still leave the use of algorithms unjustified. Rather than extensions of our agency, opaque algorithms are a kind of found object with unknown properties that may or may not be suited to the purposes to which we put them. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "A dispositional account of technical functions.",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/ROBADA-10",
    "abstract": "It is commonly held that technical artifacts have a \"dual nature.\" On the one hand, technical artifacts are material objects-they are constituted by their physical components and structures. On the other hand, technical artifacts are intentional objects in virtue of their teleological nature--they are _for_ some purpose that is contingent on the intentional states of agents. I argue that this view is false when properly understood. To do this, I develop a theory of technical functions that reduces functions to physical dispositions (Dispositional Account of Technical Functions, DAF). First, I argue that the different notions of technical functions are best understood with reference to dispositions. Then, I argue that technical functions, like dispositions, are modal properties that manifest under appropriate circumstances. Further, I argue that the seemingly teleological nature of technical functions can be accounted for with reference to user expectations. Phrases like \"proper use,\" \"malfunctioning,\" and \"accidental use\" are intentional concepts that agents ascribe to relevant dispositions. Hence, an artifact is a physical object with dispositions, and subcategories of technical functions are ultimately socio-cultural facts about what we expect from the use of artifacts. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "I am no abstract object: a novel challenge to mind uploading.",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/ZHAIAN",
    "abstract": "Mind uploading--the transference of mind from a biological brain to a computer-- offers the alluring possibility of immortality. This paper provides a novel challenge to mind uploading, focusing on the distinction between abstract objects and concrete individuals. Uploads are abstract objects, while currently, persons are concrete indi- viduals. This presents a dilemma: if the mind is concrete, uploading it to a computer is impossible. Alternatively, if mind uploading is feasible, the resulting abstract upload cannot be numerically identical to the original person. Furthermore, by dif- ferentiating survival from persistence, this paper argues that concrete persons might survive as abstract uploads, but only in a highly restricted sense, without preserving their numerical identity. Despite these philosophical hurdles, practical reasons for considering mind uploading as life nears its end still need to be acknowledged. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Large language models and linguistic intentionality.",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/GRILLM-2",
    "abstract": "Do large language models like Chat-GPT or Claude meaningfully use the words they produce? Or are they merely clever prediction machines, simulating language use by producing statistically plausible text? There have already been some initial attempts to answer this question by showing that these models meet the criteria for entering meaningful states according to metasemantic theories of mental content. In this paper, I will argue for a different approach--that we should instead consider whether language models meet the criteria given by our best metasemantic theories of linguistic content. In that vein, I will illustrate how this can be done by applying two such theories to the case of language models: Gareth Evans' ( 1982 ) account of naming practices and Ruth Millikan's ( 1984, 2004, 2005 ) teleosemantics. In doing so, I will argue that it is a mistake to think that the failure of LLMs to meet plausible conditions for mental intentionality thereby renders their outputs meaningless, and that a distinguishing feature of linguistic intentionality--dependency on a pre-existing linguistic system--allows for the plausible result that LLM outputs are meaningful. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "A logic of trust-based beliefs.",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/JIAALO-2",
    "abstract": "Traditionally, knowledge and beliefs are attributed to agents. The article explores an alternative approach where knowledge is informed by data and belief comes from trust in, not necessarily reliable, data. At the core of the article is the modality \"if one dataset is trusted, then another dataset informs a belief\". The main technical result is a sound and complete logical system capturing the properties of this modality and its connection with functional dependency between datasets.",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Why and how to construct an epistemic justification of machine learning?",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/SPEWAH",
    "abstract": "Consider a set of shuffled observations drawn from a fixed probability distribution over some instance domain. What enables learning of inductive generalizations which proceed from such a set of observations? The scenario is worthwhile because it epistemically characterizes most of machine learning. This kind of learning from observations is also inverse and ill-posed. What reduces the non-uniqueness of its result and, thus, its problematic epistemic justification, which stems from a one-to-many relation between the observations and many learnable generalizations? The paper argues that this role belongs to any complexity regularization which satisfies Norton's Material Theory of Induction (MTI) by localizing the inductive risk to facts in the given domain. A prime example of the localization is the Lottery Ticket Hypothesis (LTH) about overparameterized neural networks. The explanation of MTI's role in complexity regularization of neural networks is provided by analyzing the stability of Empirical Risk Minimization (ERM), an inductive rule that controls the learning process and leads to an inductive generalization on the given set of observations. In cases where ERM might become asymptotically unstable, making the justification of the generalization by uniform convergence unavailable, LTH and MTI can be used to define a local stability. A priori, overparameterized neural networks are such cases and the combination of LTH and MTI can block ERM's trivialization caused by equalizing the strengths of its inductive support for risk minimization. We bring closer the investigation of generalization in artificial neural networks and the study of inductive inference and show the division of labor between MTI and the optimality justifications (developed by Gerhard Schurz) in machine learning. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Ambient smart environments: affordances, allostasis, and wellbeing.",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/WHIASE",
    "abstract": "In this paper we assess the functionality and therapeutic potential of ambient smart environments. We argue that the language of affordances alone fails to do justice to the peculiar functionality of this ambient technology, and draw from theoretical approaches based on the free energy principle and active inference. We argue that ambient smart environments should be understood as playing an'upstream' role, shaping an agent's field of affordances in real time, in an adaptive way that supports an optimal grip on a field of affordances. We characterise this optimal grip using precision weighting, and in terms of allostatic control, drawing an analogy with the role of precision weighting in metacognitive processes. One key insight we present is that ambient smart environments may support allostatic control not only by simplifying an agent's problem space, but by increasing uncertainty, in order to destabilise calcified, sub-optimal, psychological and behavioural patterns. In short, we lay an empirically-grounded theoretical foundation for understanding ambient smart environments, and for answering related philosophical questions around agency, trust, and subjective wellbeing. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Carving teleology at its joints.",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/BENCTA-5",
    "abstract": "This paper addresses the conceptualisation and measurement of goal-directedness. Drawing inspiration from Ernst Mayr's demarcation between multiple meanings of teleology, we propose a refined approach that delineates different kinds of teleology/teleonomy based on the temporal depth of generative models of self-organising systems that evince free energy minimisation.",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Deepfakes: a survey and introduction to the topical collection.",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/CAVDAS",
    "abstract": "Deepfakes are extremely realistic audio/video media. They are produced via a complex machine-learning process, one that centrally involves training an algorithm on hundreds or thousands of audio/video recordings of an object or person, S, with the aim of either creating entirely new audio/video media of S or else altering existing audio/video media of S. Deepfakes are widely predicted to have deleterious consequences (principally, moral and epistemic ones) for both individuals and various of our social practices and institutions. In this introduction to the Topical Collection, I first survey existing philosophical research on deepfakes (Sects. 2 and 3). I then give an overview of the papers that comprise the Collection (Sect. 4). Finally, I conclude with remarks on a line of argument made in a number of papers in the Topical Collection: that deepfakes may cause their own demise (Sect. 5). ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Exploring the transition: biology, technology, and epistemic activities.",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/TAMETT",
    "abstract": "By focusing on biorobotics, this article explores the epistemological foundations necessary to support the transition from biological models to technological artifacts. To address this transition, I analyze the position of the German philosopher Thomas Fuchs, who represents one possible approach to the problem of the relationship between bio-inspired technology and biology. While Fuchs defends the idea of a unique ontological space for humans, this article contends that his categorical distinctions face challenges in establishing a robust epistemic foundation necessary to ground the transition from biology to technology. After identifying at least three interwoven reasons for rejecting Fuchs' epistemic foundation, I ask how, through what methods, and by means of which practices the newly bio-inspired object is accessed and shaped. Expanding on philosophy of science and technology in practice, I argue that the plurality of answers to this question provides a possible epistemological foundation _within_ the different frameworks of practices that produce the bio-inspired object. In addressing the potential epistemological foundation for pluralistically grounding the transition from biological models to technological ones, my approach helps us: (i) concretize and examine the relationship between biological and technological models, and (ii) investigate the features and validity of bio-inspired objects, effectively offering a more concrete and pluralistic picture of what bio-inspired sciences and technologies are and what they can (or cannot) do. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "A Triviality Worry for the Internal Model Principle.",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/THOATW-2",
    "abstract": "The Good Regulator Theorem and the Internal Model Principle are sometimes cited as mathematical proofs that an agent needs an internal model of the world in order to have an optimal policy. However, these principles rely on a definition of \"internal model\" that is far too permissive, applying even to cases of systems that do not use an internal model. As a result, these principles do not provide evidence (let alone a proof) that internal models are necessary. The paper also diagnoses what is missing in the GRT and IMP definitions of internal model, which is that models need to make predictions that represent variables in the target system (and these representations need to be usable by an agent so as to guide behavior). ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Does the no miracles argument apply to AI?",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/ROWDTN",
    "abstract": "According to the standard no miracles argument, science's predictive success is best explained by the approximate truth of its theories. In contemporary science, however, machine learning systems, such as AlphaFold2, are also remarkably predictively successful. Thus, we might ask what best explains such successes. Might these AIs accurately represent critical aspects of their targets in the world? And if so, does a variant of the no miracles argument apply to these AIs? We argue for an affirmative answer to these questions. We conclude that if the standard no miracles argument is sound, an AI-specific no miracles argument is also sound. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "A Credence-based Theory-heavy Approach to Non-human Consciousness.",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/DEWACT",
    "abstract": "Many different methodological approaches have been proposed to infer the presence of consciousness in non-human systems. In this paper, a version of the theory-heavy approach is defended. Theory-heavy approaches rely heavily on considerations from theories of consciousness to make inferences about non-human consciousness. Recently, the theory-heavy approach has been critiqued in the form of Birch's (Nous, 56(1): 133-153, 2022) dilemma of demandingness and Shevlin's (Mind & Language, 36(2): 297-314, 2021) specificity problem. However, both challenges implicitly assume an inapt characterization of the theory-heavy approach. I argue that an alternative characterization of the approach, what I call a credence-based theory-heavy approach, avoids these challenges. Theorists can generate interpretations of their theory, at different levels of generality, and operationalize these into theory-informed markers. These theory-informed are assigned a likelihood and are used to assess the probability that a target system is conscious. In providing this characterization, and mapping out the possible ways in which a credence-based theory-heavy approach can be fleshed out, the aim is to situate the theory-heavy approach as a more compelling approach than it is currently being perceived as. Our attention, then, needs to towards remaining challenges such as the consensus problem and the problem of calibrating the likelihoods associated with theory-informed markers. I also explore methodological pluralism and assess how the credence-based theory-heavy approach can benefit from other methodological approaches. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "An idealised account of mechanistic computation.",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/KERAIA-3",
    "abstract": "The mechanistic account of computation offers one promising and influential theory of computational implementation. The aim of this paper is to shore up its conceptual foundations by responding to several recent challenges. After outlining and responding to a recent proposal from Kuokkanen (2022a), I suggest that computational description should be conceptualised as a form of idealisation (selectively attending to modified subsets of model features) rather than abstraction (selectively attending to subsets of features within a target system). I argue that this conceptualisation not only offers the best way of making sense of computational implementation, but also a way of resolving each of the outstanding challenges facing the mechanistic account. The idealisation view allows the mechanistic account to make sense of the omission process found in computational descriptions without leaving the relationship between physical and computational properties mysterious. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Environmental Epistemology.",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/AMIEEV",
    "abstract": "We argue that there is a large class of questions--specifically questions about how to epistemically evaluate environments that currently available epistemic theories are not well-suited for answering, precisely because these questions are not about the epistemic state of particular agents or groups. For example, if we critique Facebook for being conducive to the spread of misinformation, then we are not thereby critiquing Facebook for being irrational, or lacking knowledge, or failing to testify truthfully. Instead, we are saying something about the social media environment. In this paper, we first propose that a new branch of epistemology-Environmental Epistemology-is needed to address these questions. We argue that environments can be proper objects of epistemic evaluation, and that there are genuine epistemic norms that govern environments. We then provide a positive account of these norms and conclude by considering how recognition of these norms may require us to rethink longstanding epistemic debates. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "On Informational Injustice and Epistemic Exclusions.",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/BAGOII",
    "abstract": "Information is a unique resource. Asymmetries that arise out of information access or processing capacities, therefore, enable a distinctive form of injustice. This paper builds a working conception of such injustice and explores it further. Let us call it informational injustice. Informational injustice is a consequence of informational asymmetries between at least two agents, which are deeply exacerbated due to modern information and communication technologies but do not necessarily originate with them. Informational injustice is the injustice of having information from an informational surplus being used to disadvantage the agent with less information. -/- This paper argues that informational injustice exploits an agent as a knower, specifically exploiting the agent's limitation in possessing or processing information--an agent is exploited because she is not informed or lacks in her ability to process information. In the case of lack of information, the agent simply does not know the information under consideration; a person is algorithmically manipulated or nudged to buy a product or vote for someone. In the case of a lack of capacity to process information, the agent simply cannot use the information, despite having access to it, to reach epistemically valuable states such as knowledge; a lawyer dupes you because he knows more about the inner workings of a courtroom and the law. Technically, you have access to the information your lawyer has, but you cannot make use of it due to constraints on time and cognitive effort. Informational injustice excludes the harmed agent from participating in knowledge practices. Thus, informational injustice is also a kind of epistemic exclusion. -/- After fixing the concept of informational injustice, the paper distinguishes between two kinds of informational injustices: interactional informational injustice and structural informational injustice. The former concerns interactions between agents, while the latter concerns social structures that emerge out of interactions between agents. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Learning how to learn by self-tuning reinforcement.",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/TORLHT-2",
    "abstract": "Humans and many animals are capable of learning and learning how to learn better. We are concerned here with one way that reinforcement learners might learn how to learn better. In an experiment described by Harlow in (Psychol Rev 56:51-65, 1949) a group of rhesus monkeys learn a new way of learning in the context of a specific type of problem. We will consider how such agents might coevolve a new learning dynamics and new attendant saliences. To this end, we propose a self-tuning dynamics that illustrates one way that a reinforcement learner might acquire forms of learning that are well-suited to context-specific problems. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Social learning in models and minds.",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/YONSLI",
    "abstract": "After more than a century in which social learning was blackboxed by evolutionary biologists, psychologists and economists, there is now a thriving industry in cognitive neuroscience producing computational models of learning from and about other agents. This is a hugely positive development. The tools of computational cognitive neuroscience are rigorous and precise. They have the potential to prise open the black box. However, we argue that, from the perspective of a scientific realist, these tools are not yet being applied in an optimal way. To fulfil their potential, the shiny new methods of cognitive neuroscience need to be better coordinated with old-fashioned, contrastive experimental designs. Inferences from model complexity to cognitive complexity, of the kind made by those who favour lean interpretations of behaviour ('associationists'), require social learning to be tested in challenging task environments. Inferences from cognitive complexity to social specificity, made by those who favour rich interpretations ('mentalists'), call for non-social control experiments. A parsimonious model that fits current data is a good start, but carefully designed experiments are needed to distinguish models that tell us how social learning _could_ be done from those that tell us how it is _really_ done. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Representationalism and rationality: why mental representation is real.",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/BIERAR",
    "abstract": "This paper presents an argument for the realism about mechanisms, contents, and vehicles of mental representation at both the personal and subpersonal levels, and showcases its role in instrumental rationality and proper cognitive functioning. By demonstrating how misrepresentation is necessary for learning from mistakes and explaining certain failures of action, we argue that fallible rational agents must have mental representations with causally relevant vehicles of content. Our argument contributes to ongoing discussions in philosophy of mind and cognitive science by challenging anti-realist views about the nature of mental representation, and by highlighting the importance of understanding how different agents can misrepresent in pursuit of their goals. While there are potential rebuttals to our claim, our opponents must explain how agents can be rational without having mental representations. This is because mental representation is grounded in rationality. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Scrutinizing the foundations: could large language models be solipsistic?",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/ESASTF",
    "abstract": "In artificial intelligence literature, \"delusions\" are characterized as the generation of unfaithful output from reliable source content. There is an extensive literature on computer-generated delusions, ranging from visual hallucinations, like the production of nonsensical images in Computer Vision, to nonsensical text generated by (natural) language models, but this literature is predominantly taxonomic. In a recent research paper, however, a group of scientists from DeepMind successfully presented a formal treatment of an entire class of delusions in generative AI models (i.e., models based on a transformer architecture, both with and without RLHF--reinforcement learning with human feedback, such as BERT, GPT-3 or the more recent GPT-3.5), referred to as auto-suggestive delusions. Auto-suggestive delusions are not mere unfaithful output, but are self-induced by the transformer models themselves. Typically, these delusions have been subsumed under the concept of exposure bias, but exposure bias alone does not elucidate their nature. In order to address their nature, I will introduce a formal framework that clarifies the probabilistic delusions capable of explaining exposure bias in a broad manner. This will serve as the foundation for exploring auto-suggestive delusions in language models. Next, an examination of self- or auto-suggestive delusions will be undertaken, by drawing an analogy with the rule-following problematic from the philosophy of mind and language. Finally, I will argue that this comprehensive approach leads to the suggestion that transformers, large language models in particular, may develop in a manner that touches upon solipsism and the emergence of a private language, in a weak sense. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "What is a mathematical structure of conscious experience?",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/KLEWIA",
    "abstract": "Several promising approaches have been developed to represent conscious experience in terms of mathematical spaces and structures. What is missing, however, is an explicit definition of what a 'mathematical structure of conscious experience' is. Here, we propose such a definition. This definition provides a link between the abstract formal entities of mathematics and the concreta of conscious experience; it complements recent approaches that study quality spaces, qualia spaces, or phenomenal spaces; and it provides a general method to identify and investigate structures of conscious experience. We hope that ultimately this work provides a basis for developing a common formal language to study consciousness. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Exploring, expounding & ersatzing: a three-level account of deep learning models in cognitive neuroscience.",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/SUBEEB",
    "abstract": "Deep learning (DL) is a statistical technique for pattern classification through which AI researchers train artificial neural networks containing multiple layers that process massive amounts of data. I present a three-level account of explanation that can be reasonably expected from DL models in cognitive neuroscience and that illustrates the explanatory dynamics within a future-biased research program (Feest Philosophy of Science 84:1165-1176, 2017 ; Doerig et al. Nature Reviews: Neuroscience 24:431-450, 2023 ). By relying on the mechanistic framework (Craver Explaining the brain: Mechanisms and the mosaic unity of neuroscience. Clarendon, 2007 ; Stinson Eppuor si muove: Doing history and philosophy of Science with Peter Machamer. Springer, 2017, The Routledge Handbook of the computational mind. Routledge, 2018 ), I develop an account that corresponds to the stages of mechanism discovery, i.e., our shifting epistemic position and epistemic goals, and propose a representative model for each level. Generic, theoretical DL models at _Level 1_ address the general features of a cognitive phenomenon through exploration and provide us with how-possibly explanations. On the other hand, DL models at _Level 2_ either identify the interaction between the features or represent the epistemic stage when the researcher is still unsure if the modeled features are crucial or arbitrary. These models should provide us with how-plausibly explanations. Finally, specific DL models of specific brain areas, i.e., ersatz models filled with relevant cognitive and neuroscientific details, are at _Level 3_. At this level, a researcher can advance how-actually explanations of cognitive phenomena. The main strength of this account is that it elucidates both _global explanatory dynamics_ and _local explanatory dynamics_ (cf. Dresow Synthese 199:10441-10474, 2021 ). The former occurs when the transition between levels happens in accordance with the process of obtaining more details about a particular cognitive phenomenon through multiple DL models. The latter, meanwhile, involves cases in which the transition between levels takes place within a single DL model by elucidating internal mechanisms (e.g., using Explainable AI techniques for rendering models more transparent). ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Computational systems as higher-order mechanisms.",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/FUECSA-3",
    "abstract": "I argue that there are different orders of mechanisms with different constitutive relevance and individuation conditions. In common first-order mechanistic explanations, constitutive relevance norms are captured by the matched-interlevel-experiments condition (Craver et al. (2021) Synthese 199:8807-8828). Regarding individuation, we say that any two mechanisms are of the same type when they have the same concrete components performing the same activities in the same arrangement. By contrast, in higher-order mechanistic explanations, we formulate the decompositions in terms of generalized basic components (GBCs). These GBCs (e.g., logic gates) possess causal properties that are common to a set of physical systems. Mechanistic explanations formulated in terms of GBCs embody the epistemic value of horizontal integration, which aims to explain as many phenomena as possible with a minimal amount of abstract components (Wajnerman Paz (2017a) Philos Psychol 30:213-234). Two higher-order mechanisms are of the same type when they share all the same GBCs, and they are organized as performing the same activities with the same interactions. I use this notion of mechanistic order to enhance the mechanistic account of computation (MAC) and provide an account of the epistemic norms of computational explanation and the nature of medium-independent functional properties and mechanisms. Finally, I use these new conceptual tools to address four criticisms of the MAC. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Functionalism, integrity, and digital consciousness.",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/SHIFIA-2",
    "abstract": "The prospect of consciousness in artificial systems is closely tied to the viability of functionalism about consciousness. Even if consciousness arises from the abstract functional relationships between the parts of a system, it does not follow that any digital system that implements the right functional organization would be conscious. Functionalism requires constraints on what it takes to properly implement an organization. Existing proposals for constraints on implementation relate to the integrity of the parts and states of the realizers of roles in a functional organization. This paper presents and motivates three novel integrity constraints on proper implementation not satisfied by current neural network models. It is proposed that for a system to be conscious, there must be a straightforward relationship between the material entities that compose the system and the realizers of functional roles, that the realizers of the functional roles must play their roles due to internal causal powers, and that they must continue to exist over time. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Paying attention to attention: psychological realism and the attention economy.",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/WHIPAT-31",
    "abstract": "In recent years, philosophers have identified a number of moral and psychological harms associated with the attention economy (Alysworth & Castro, 2021; Castro & Pham, 2020; Williams, 2018). Missing from many of these accounts of the attention economy, however, is what exactly attention is. As a result of this neglect of the cognitive science of attention, many of these accounts are not empirically credible. They rely on oversimplified and unsophisticated accounts of not only attention, but self- control, and addiction as well. Of note are accounts of the attention economy that rely on the 'brain disease' rhetoric of addiction and subsequent control failures (Aylsworth & Castro, 2021; Bhargava & Velasquez, 2021), accounts that rely on a strict dichotomy of top-down vs. bottom-up attention (Williams, 2018; Aylsworth & Castro, 2021), and accounts that construe attention as a limited resource (Williams, 2018). -/- Drawing on recent work from the neuroscience and psychology of attention, I demonstrate the shortcomings of these accounts and sketch a way forward for an empirically grounded account of the attention economy. These accounts tend to uphold strict dichotomies of voluntary control (e.g., compulsion versus choice, dual-process models of self-control, and top-down versus bottom-up) that cannot account for the complexities of attentional control, mental agency, and decision-making. As such, these empirically and conceptually impoverished accounts cannot adequately address the current so-called crisis of attention. To better understand the harms associated with the attention economy, we need an empirically responsible account of the nature and function of attention and mental agency. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Causal reasoning from almost first principles.",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/BOCCRF",
    "abstract": "A formal theory of causal reasoning is presented that encompasses both Pearl's approach to causality and several key formalisms of nonmonotonic reasoning in Artificial Intelligence. This theory will be derived from a single rationality principle of causal acceptance for propositions. However, this principle will also set the theory of causal reasoning apart from common representational approaches to reasoning formalisms.",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Deep convolutional neural networks are not mechanistic explanations of object recognition.",
    "authors": [],
    "year": 2024,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/GRUDCN",
    "abstract": "Given the extent of using deep convolutional neural networks to model the mechanism of object recognition, it becomes important to analyse the evidence of their similarity and the explanatory potential of these models. I focus on one frequent method of their comparison--representational similarity analysis, and I argue, first, that it underdetermines these models as how-actually mechanistic explanations. This happens because different similarity measures in this framework pick out different mechanisms across DCNNs and the brain in order to correspond them, and there is no arbitration between them in terms of relevance for object recognition. Second, the reason similarity measures are underdetermining to a large degree stems from the highly idealised nature of these models, which undermines their status as how-possibly mechanistic explanatory models of object recognition as well. Thus, building models with more theoretical consideration and choosing relevant similarity measures may bring us closer to the goal of mechanistic explanation. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "On the site of predictive justice.",
    "authors": [],
    "year": 2024,
    "journal": "Nous",
    "url": "https://philpapers.org/rec/LAZOTS",
    "abstract": "Optimism about our ability to enhance societal decision-making by leaning on Machine Learning (ML) for cheap, accurate predictions has palled in recent years, as these 'cheap' predictions have come at significant social cost, contributing to systematic harms suffered by already disadvantaged populations. But what precisely goes wrong when ML goes wrong? We argue that, as well as more obvious concerns about the downstream effects of ML-based decision-making, there can be moral grounds for the criticism of these predictions themselves. We introduce and defend a theory of predictive justice, according to which differential model performance for systematically disadvantaged groups can be grounds for moral criticism of the model, independently of its downstream effects. As well as helping resolve some urgent disputes around algorithmic fairness, this theory points the way to a novel dimension of epistemic ethics, related to the recently discussed category of doxastic wrong. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Spatiotemporal functionalism v. the conceivability of zombies.",
    "authors": [],
    "year": 2020,
    "journal": "Nous",
    "url": "https://philpapers.org/rec/CHASFV",
    "abstract": "",
    "source_method": "year_navigation_2020_page_1"
  },
  {
    "title": "Varieties of Cognitive Integration.",
    "authors": [],
    "year": 2019,
    "journal": "Nous",
    "url": "https://philpapers.org/rec/CARVOC-11",
    "abstract": "Extended cognition theorists argue that cognitive processes constitutively depend on resources that are neither organically composed, nor located inside the bodily boundaries of the agent, provided certain conditions on the integration of those processes into the agent's cognitive architecture are met. Epistemologists, however, worry that in so far as such cognitively integrated processes are epistemically relevant, agents could thus come to enjoy an untoward explosion of knowledge. This paper develops and defends an approach to cognitive integration--cluster-model functionalism--which finds application in both domains of inquiry, and which meets the challenge posed by putative cases of cognitive or epistemic bloat. ",
    "source_method": "year_navigation_2019_page_1"
  },
  {
    "title": "Two Paradoxes of Common Knowledge: Coordinated Attack and Electronic Mail.",
    "authors": [],
    "year": 2018,
    "journal": "Nous",
    "url": "https://philpapers.org/rec/LEDTPO-3",
    "abstract": "The coordinated attack scenario and the electronic mail game are two paradoxes of common knowledge. In simple mathematical models of these scenarios, the agents represented by the models can coordinate only if they have common knowledge that they will. As a result, the models predict that the agents will not coordinate in situations where it would be rational to coordinate. I argue that we should resolve this conflict between the models and facts about what it would be rational to do by rejecting common knowledge assumptions implicit in the models. I focus on the assumption that the agents have common knowledge that they are rational, and provide models to show that denying this assumption suffices for a resolution of the paradoxes. I describe how my resolution of the paradoxes fits into a general story about the relationship between rationality in situations involving a single agent and rationality in situations involving many agents. ",
    "source_method": "year_navigation_2018_page_1"
  },
  {
    "title": "Counterpossibles in Science: The Case of Relative Computability.",
    "authors": [],
    "year": 2018,
    "journal": "Nous",
    "url": "https://philpapers.org/rec/JENCIS-2",
    "abstract": "I develop a theory of counterfactuals about relative computability, i.e. counterfactuals such as 'If the validity problem were algorithmically decidable, then the halting problem would also be algorithmically decidable,' which is true, and 'If the validity problem were algorithmically decidable, then arithmetical truth would also be algorithmically decidable,' which is false. These counterfactuals are counterpossibles, i.e. they have metaphysically impossible antecedents. They thus pose a challenge to the orthodoxy about counterfactuals, which would treat them as uniformly true. What's more, I argue that these counterpossibles don't just appear in the periphery of relative computability theory but instead they play an ineliminable role in the development of the theory. Finally, I present and discuss a model theory for these counterfactuals that is a straightforward extension of the familiar comparative similarity models. ",
    "source_method": "year_navigation_2018_page_1"
  },
  {
    "title": "What is it like to be a group agent?",
    "authors": [],
    "year": 2018,
    "journal": "Nous",
    "url": "https://philpapers.org/rec/LISWII-5",
    "abstract": "The existence of group agents is relatively widely accepted. Examples are corporations, courts, NGOs, and even entire states. But should we also accept that there is such a thing as group consciousness? I give an overview of some of the key issues in this debate and sketch a tentative argument for the view that group agents lack phenomenal consciousness (pace Schwitzgebel 2015). In developing my argument, I draw on integrated information theory, a much-discussed theory of consciousness. I conclude by pointing out an implication of my argument for the normative status of group agents. ",
    "source_method": "year_navigation_2018_page_1"
  },
  {
    "title": "Busting Out: Predictive Brains, Embodied Minds, and the Puzzle of the Evidentiary Veil.",
    "authors": [],
    "year": 2017,
    "journal": "Nous",
    "url": "https://philpapers.org/rec/CLABOP",
    "abstract": "Biological brains are increasingly cast as 'prediction machines': evolved organs whose core operating principle is to learn about the world by trying to predict their own patterns of sensory stimulation. This, some argue, should lead us to embrace a brain-bound 'neurocentric' vision of the mind. The mind, such views suggest, consists entirely in the skull-bound activity of the predictive brain. In this paper I reject the inference from predictive brains to skull-bound minds. Predictive brains, I hope to show, can be apt participants in larger cognitive circuits. The path is thus cleared for a new synthesis in which predictive brains act as entry-points for 'extended minds', and embodiment and action contribute constitutively to knowing contact with the world. ",
    "source_method": "year_navigation_2017_page_1"
  },
  {
    "title": "Measurement and Computational Skepticism.",
    "authors": [],
    "year": 2017,
    "journal": "Nous",
    "url": "https://philpapers.org/rec/MATMAC-3",
    "abstract": "Putnam and Searle famously argue against computational theories of mind on the skeptical ground that there is no fact of the matter as to what mathematical function a physical system is computing: both conclude (albeit for somewhat different reasons) that virtually any physical object computes every computable function, implements every program or automaton. There has been considerable discussion of Putnam's and Searle's arguments, though as yet there is little consensus as to what, if anything, is wrong with these arguments. In the present paper we show that an analogous line of reasoning can be raised against the numerical measurement (i.e., numerical representation) of physical magnitudes, and we argue that this result is a reductio ad absurdum of the challenge to computational skepticism. We then use this reductio to get clearer about both (i) what's wrong with Putnam's and Searle's arguments against computationalism, and (ii) what can be learned about both computational implementation and numerical measurement from the shortcomings of both sorts of skeptical argument. ",
    "source_method": "year_navigation_2017_page_1"
  },
  {
    "title": "AI takeover and human disempowerment.",
    "authors": [],
    "year": 2025,
    "journal": "The Philosophical Quarterly",
    "url": "https://philpapers.org/rec/BALATA-6",
    "abstract": "Some take seriously the possibility of artificial intelligence (AI) takeover, where AI systems seize power in a way that leads to human disempowerment. Assessing the likelihood of takeover requires answering empirical questions about the future of AI technologies and the context in which AI will operate. In many cases, philosophers are poorly placed to answer these questions. However, some prior questions are more amenable to philosophical techniques. What does it mean to speak of AI empowerment and human disempowerment? And what empirical claims must hold for the former to lead to the latter? In this paper, I address these questions, providing foundations for further evaluation of the likelihood of takeover. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Nietzsche's Posthumanism.",
    "authors": [],
    "year": 2025,
    "journal": "The Philosophical Quarterly",
    "url": "https://philpapers.org/rec/THUNPI",
    "abstract": "",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Understanding Artificial Agency.",
    "authors": [],
    "year": 2025,
    "journal": "The Philosophical Quarterly",
    "url": "https://philpapers.org/rec/DUNUAA",
    "abstract": "Which artificial intelligence (AI) systems are agents? To answer this question, I propose a multidimensional account of agency. According to this account, a system's agency profile is jointly determined by its level of goal-directedness and autonomy as well as is abilities for directly impacting the surrounding world, long-term planning and acting for reasons. Rooted in extant theories of agency, this account enables fine-grained, nuanced comparative characterizations of artificial agency. I show that this account has multiple important virtues and is more informative than alternatives. More speculatively, it may help to illuminate two important emerging questions in AI ethics: 1. Can agency contribute to the moral status of non-human beings, and how? 2. When and why might AI systems exhibit power-seeking behaviour and does this pose an existential risk to humanity? ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Calling for Explanation.",
    "authors": [],
    "year": 2025,
    "journal": "The Philosophical Quarterly",
    "url": "https://philpapers.org/rec/GORCFE",
    "abstract": "",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Constraining the Compression: Thermodynamic Depth and Composition.",
    "authors": [],
    "year": 2024,
    "journal": "The Philosophical Quarterly",
    "url": "https://philpapers.org/rec/BENCTC-7",
    "abstract": "This paper examines Bird's account of restricted compositionality in terms of compression of information. Additionally, this paper proposes an alternative perspective (to Bird's) that links compositionality to the Free Energy Principle and the minimisation of collective entropy. Emphasising functional integration, this criterion provides a more focused and relatively more objective (patternist) account of composition.",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "The Internet Is Not What You Think It Is: A History, A Philosophy, A Warning.",
    "authors": [],
    "year": 2024,
    "journal": "The Philosophical Quarterly",
    "url": "https://philpapers.org/rec/WESTII-4",
    "abstract": "One of the most fascinating entries in Samuel Pepys diaries, from the 13th May 1665, recounts his experience of having been gifted a new pocket watch:To the 'Change after office, and received my watch from the watchmaker, and a very fine [one] it is, given me by Briggs, the Scrivener... But, Lord! to see how much of my old folly and childishnesse hangs upon me still that I cannot forbear carrying my watch in my hand in the coach all this afternoon, and seeing what o'clock it is one hundred times; and am apt to think with myself, how could I be so long without one; though I remember since, I had one, and found it a trouble, and resolved to carry one no more about me while I lived.Pepys' excitement at being able to 'see what o'clock it is' might strike us as a very antiquated kind of thrill, but his account of the 'trouble' that follows from the distraction of being able to do so will most likely resonate with anyone with a smart phone. Like Pepys, many of us are familiar with the experience of carrying a small object around in our pocket with the capacity to distract and amuse. Many smart phone owners (myself included) may well have, like Pepys, 'resolved to carry it no more about' us--although (at least in my experience) such resolve inevitably breaks. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "The Trolley Problem in the Ethics of Autonomous Vehicles.",
    "authors": [],
    "year": 2023,
    "journal": "The Philosophical Quarterly",
    "url": "https://philpapers.org/rec/PAUTPI-5",
    "abstract": "In 2021, Germany passed the first law worldwide that regulates dilemma situations with autonomous cars. Against this background, this article investigates the permissibility of trade-offs between human lives in the context of self-driving cars. It does so by drawing on the debate about the traditional trolley problem. In contrast to most authors in the relevant literature, it argues that the debate about the trolley problem is both directly and indirectly relevant for the ethics of crashes with self-driving cars. Drawing on its direct normative relevance, the article shows that trade-offs are permissible in situations with self-driving cars that are similar to paradigmatic trolley cases. In scenarios that are unlike paradigmatic trolley cases, the debate about the trolley problem can have indirect normative relevance because it provides reasons against the use of moral theories and principles that cannot account for the trolley problem. ",
    "source_method": "year_navigation_2023_page_1"
  },
  {
    "title": "I, Volkswagen.",
    "authors": [],
    "year": 2022,
    "journal": "The Philosophical Quarterly",
    "url": "https://philpapers.org/rec/COLIV-7",
    "abstract": "Philosophers increasingly argue that collective agents can be blameworthy for wrongdoing. Advocates tend to endorse functionalism, on which collectives are analogous to complicated robots. This is puzzling: we don't hold robots blameworthy. I argue we don't hold robots blameworthy because blameworthiness presupposes the capacity for a mental state I call 'moral self-awareness'. This raises a new problem for collective blameworthiness: collectives seem to lack the capacity for moral self-awareness. I solve the problem by giving an account of how collectives have this capacity. The trick is to take seriously individuals' status as flesh-and-blood material constituents of collectives. The idea will be: under certain conditions that I specify, an individual can be the locus of a collective's moral self-awareness. The account provides general insights concerning collectives' dependence on members, the boundaries of membership, and the locus of collectives' phenomenology. ",
    "source_method": "year_navigation_2022_page_1"
  },
  {
    "title": "No right to an explanation.",
    "authors": [],
    "year": 2025,
    "journal": "Philosophy and Phenomenological Research",
    "url": "https://philpapers.org/rec/KARNRT",
    "abstract": "An increasing number of complex and important decisions are now being made with the aid of opaque algorithms. This has led to calls from both theorists and legislators for the implementation of a right to an explanation for algorithmic decisions. In this paper, we argue that, in most cases and for most kinds of explanations, there is no such right. After differentiating a number of different things that might be meant by a 'right to an explanation,' we argue that, for the most plausible and popular versions (a right to an explanation for a specific bureaucratic decision, a right to an explanation grounded in public reason considerations, or a right to a higher-level explanation of an entire system of automated decision-making), such a right is either superfluous, impossible to obtain, or not the best way to secure relevant normative goods. We also argue that proponents of a right to an explanation carve off certain kinds of automated decisions as requiring more justification than others in similar areas of social (and natural) science policy, a demarcation we argue is unjustified. While there will be some cases where an explanation is the only thing that can secure an important normative good (e.g. when an explanation is the only thing that would catch an unjustified individual decision), we argue these cases are too rare and scattered to ground something as weighty as a right to an explanation. ",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Taking the simulation hypothesis seriously.",
    "authors": [],
    "year": 2024,
    "journal": "Philosophy and Phenomenological Research",
    "url": "https://philpapers.org/rec/CHATTS-6",
    "abstract": "Philosophy and Phenomenological Research, EarlyView.",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Engineering social concepts: Feasibility and causal models.",
    "authors": [],
    "year": 2024,
    "journal": "Philosophy and Phenomenological Research",
    "url": "https://philpapers.org/rec/NEUESC-3",
    "abstract": "How feasible are conceptual engineering projects of social concepts that aim for the engineered concept to be deployed in people's ordinary conceptual practices? Predominant frameworks on the psychology of concepts that shape work on stereotyping, bias, and machine learning have grim implications for the prospects of conceptual engineers: conceptual engineering efforts are ineffective in promoting certain social-conceptual changes. Since conceptual components that give rise to problematic social stereotypes are sensitive to statistical structures of the environment, purely conceptual change won't be possible without corresponding world change. This tradition, however, tends to ignore that concepts don't only encode statistical, but also causal information. Paying attention to this feature of concepts, I argue, shows that conceptual engineering is not only possible. There is an imperative to conceptually-engineer. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Illusory world skepticism.",
    "authors": [],
    "year": 2024,
    "journal": "Philosophy and Phenomenological Research",
    "url": "https://philpapers.org/rec/SCHIWS-4",
    "abstract": "l argue that, contra Chalmers,a skeptical scenario involving deception is a genuine possibility,even if he is correct that simulations are real. I call this new skeptical position \"Illusory World Skepticism.\" Illusory World Skepticism draws from the simulation argument,together with work in physics,astrobiology, and AI,to argue that we may indeed be in an illusory world--a universe scale simulation orchestrated by a deceptive AI--the technophilosopher's ultimate evil demon. In Section One I urge that Illusory World Skepticism is a bone fide skeptical possibility. In Section Two, I explore features of quantum computation. Then, in Sections Three and Four, I draw from the discussion of quantum computation and assume that the simulation argument is correct,applying considerations from the fields of astrobiology and AI safety to illustrate that illusory world skepticism constitutes what I call \"a serious epistemic threat\", a scenario that cannot be dismissed as requiring that knowledge is certainty or which seems to just depict a remote, fictional situation. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Let's hope we're not living in a simulation.",
    "authors": [],
    "year": 2024,
    "journal": "Philosophy and Phenomenological Research",
    "url": "https://philpapers.org/rec/SCHLHW-2",
    "abstract": "In Reality+, David Chalmers suggests that it wouldn't be too bad if we lived in a computer simulation. I argue on the contrary that if we live in a simulation, we ought to attach a significant conditional credence to its being a small or brief simulation. Our existence and the existence of many of the people and things we care about would then unfortunately depend on contingencies difficult to assess and beyond our control. Furthermore, all the badness of the world would appear to reflect the gods' intentional cruelty or callous disregard. A large, stable rock is a more dependable and less axiologically troubling fundamental ground for reality. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Will intelligent machines become moral patients?",
    "authors": [],
    "year": 2023,
    "journal": "Philosophy and Phenomenological Research",
    "url": "https://philpapers.org/rec/MOOWIM-4",
    "abstract": "This paper addresses a question about the moral status of Artificial Intelligence (AI): will AIs ever become moral patients? I argue that, while it is in principle possible for an intelligent machine to be a moral patient, there is no good reason to believe this will in fact happen. I start from the plausible assumption that traditional artifacts do not meet a minimal necessary condition of moral patiency: having a good of one's own. I then argue that intelligent machines are no different from traditional artifacts in this respect. To make this argument, I examine the feature of AIs that enables them to improve their intelligence, i.e., machine learning. I argue that there is no reason to believe that future advances in machine learning will take AIs closer to having a good of their own. I thus argue that concerns about the moral status of future AIs are unwarranted. Nothing about the nature of intelligent machines makes them a better candidate for acquiring moral patiency than the traditional artifacts whose moral status does not concern us. ",
    "source_method": "year_navigation_2023_page_1"
  },
  {
    "title": "Precis of Conscious Experience: A Logical Inquiry#.",
    "authors": [],
    "year": 2022,
    "journal": "Philosophy and Phenomenological Research",
    "url": "https://philpapers.org/rec/GUPPOC",
    "abstract": "Philosophy and Phenomenological Research, Volume 104, Issue 1, Page 232-235, January 2022.",
    "source_method": "year_navigation_2022_page_1"
  },
  {
    "title": "Transparency is Surveillance.",
    "authors": [],
    "year": 2021,
    "journal": "Philosophy and Phenomenological Research",
    "url": "https://philpapers.org/rec/NGUTIS",
    "abstract": "In her BBC Reith Lectures on Trust, Onora O'Neill offers a short, but biting, criticism of transparency. People think that trust and transparency go together but in reality, says O'Neill, they are deeply opposed. Transparency forces people to conceal their actual reasons for action and invent different ones for public consumption. Transparency forces deception. I work out the details of her argument and worsen her conclusion. I focus on public transparency - that is, transparency to the public over expert domains. I offer two versions of the criticism. First, the epistemic intrusion argument: The drive to transparency forces experts to explain their reasoning to non-experts. But expert reasons are, by their nature, often inaccessible to non-experts. So the demand for transparency can pressure experts to act only in those ways for which they can offer public justification. Second, the intimate reasons argument: In many cases of practical deliberation, the relevant reasons are intimate to a community and not easily explicable to those who lack a particular shared background. The demand for transparency, then, pressures community members to abandon the special understanding and sensitivity that arises from their particular experiences. Transparency, it turns out, is a form of surveillance. By forcing reasoning into the explicit and public sphere, transparency roots out corruption -- but it also inhibits the full application of expert skill, sensitivity, and subtle shared understandings. The difficulty here arises from the basic fact that human knowledge vastly outstrips any individual's capacities. We all depend on experts, which makes us vulnerable to their biases and corruption. But if we try to wholly secure our trust -- if we leash groups of experts to pursuing only the goals and taking only the actions that can be justified to the non-expert public -- then we will undermine their expertise. We need both trust and transparency, but they are in essential tension. This is a deep practical dilemma; it admits of no neat resolution, but only painful compromise. ",
    "source_method": "year_navigation_2021_page_1"
  },
  {
    "title": "Too much attention, too little self.",
    "authors": [],
    "year": 2020,
    "journal": "Philosophy and Phenomenological Research",
    "url": "https://philpapers.org/rec/JENTMA-5",
    "abstract": "This is a good time for such a substantial book on Buddhaghosa. His ideas may be more difficult to digest than those of contemporary authors, but Ganeri convincingly argues for their relevance. Together with Ganeri's considerable interpretive and philosophical work, Buddhaghosa's view helps to fill out a perspective that is popular in cognitive science, in which the self is replaced by systems. In this case, the self is replaced by systems of attention, a view that Ganeri calls 'Attentionalism.' In this review I will focus on two aspects of the account that I find especially puzzling, with the hope that this leads to further elucidation, whether by Ganeri or others. Specifically, I will focus on the concepts of ekaggata, or \"placing,\" and anatta, or \"no-self,\" as interpreted by Ganeri. ",
    "source_method": "year_navigation_2020_page_1"
  },
  {
    "title": "Governing the Algorithmic City.",
    "authors": [],
    "year": 2025,
    "journal": "Philosophy & Public Affairs",
    "url": "https://philpapers.org/rec/LAZGTA",
    "abstract": "Philosophy &Public Affairs, Volume 53, Issue 2, Page 102-168, Spring 2025.",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "High Risk, Low Reward: A Challenge to the Astronomical Value of Existential Risk Mitigation.",
    "authors": [],
    "year": 2023,
    "journal": "Philosophy & Public Affairs",
    "url": "https://philpapers.org/rec/THOHRL",
    "abstract": "Philosophy &Public Affairs, Volume 51, Issue 4, Page 373-412, Fall 2023.",
    "source_method": "year_navigation_2023_page_1"
  },
  {
    "title": "Reconciling Algorithmic Fairness Criteria.",
    "authors": [],
    "year": 2023,
    "journal": "Philosophy & Public Affairs",
    "url": "https://philpapers.org/rec/BEIRAF",
    "abstract": "Philosophy &Public Affairs, Volume 51, Issue 2, Page 166-190, Spring 2023.",
    "source_method": "year_navigation_2023_page_1"
  },
  {
    "title": "Algorithmic Fairness and Base Rate Tracking.",
    "authors": [],
    "year": 2022,
    "journal": "Philosophy & Public Affairs",
    "url": "https://philpapers.org/rec/EVAAFA-2",
    "abstract": "Philosophy & Public Affairs, Volume 50, Issue 2, Page 239-266, Spring 2022.",
    "source_method": "year_navigation_2022_page_1"
  },
  {
    "title": "On statistical criteria of algorithmic fairness.",
    "authors": [],
    "year": 2021,
    "journal": "Philosophy & Public Affairs",
    "url": "https://philpapers.org/rec/HEDOSC",
    "abstract": "Predictive algorithms are playing an increasingly prominent role in society, being used to predict recidivism, loan repayment, job performance, and so on. With this increasing influence has come an increasing concern with the ways in which they might be unfair or biased against individuals in virtue of their race, gender, or, more generally, their group membership. Many purported criteria of algorithmic fairness concern statistical relationships between the algorithm's predictions and the actual outcomes, for instance requiring that the rate of false positives be equal across the relevant groups. We might seek to ensure that algorithms satisfy all of these purported fairness criteria. But a series of impossibility results shows that this is impossible, unless base rates are equal across the relevant groups. What are we to make of these pessimistic results? I argue that none of the purported criteria, except for a calibration criterion, are necessary conditions for fairness, on the grounds that they can all be simultaneously violated by a manifestly fair and uniquely optimal predictive algorithm, even when base rates are equal. I conclude with some general reflections on algorithmic fairness. ",
    "source_method": "year_navigation_2021_page_1"
  },
  {
    "title": "[Book review] David S. Wendler, Life without Degrees of Moral Status: Implications for Rabbits, Robots, and the Rest of Us. [REVIEW]",
    "authors": [],
    "year": 2025,
    "journal": "Ethics",
    "url": "https://philpapers.org/rec/RYLLWQ",
    "abstract": "",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "The Right to Be Forgotten and the Value of an Open Future.",
    "authors": [],
    "year": 2024,
    "journal": "Ethics",
    "url": "https://philpapers.org/rec/PRETRT-3",
    "abstract": "This article seeks to shed light on debates about the right to be forgotten by offering a new account of the right as grounded in the confidence that the direction of one's life is up to one and worth the trouble that it takes to direct it. I show how this confidence is supported by what the right actually provides: the possibility of new social interactions unconditioned by information about one's past. This view avoids pitfalls facing other accounts of the right's moral basis, clarifies its relation to rights of privacy, and resolves several puzzles thought to face its practical application. ",
    "source_method": "year_navigation_2024_page_1"
  },
  {
    "title": "Vallor, Shannon. Technology and the Virtues: A Philosophical Guide to a World Worth Wanting. New York: Oxford University Press, 2016. Pp. ix+309. $39.95. [REVIEW]",
    "authors": [],
    "year": 2017,
    "journal": "Ethics",
    "url": "https://philpapers.org/rec/KAWVST",
    "abstract": "A review of Shannon Vallor's Technology and the Virtues: A Philosophical Guide to a Future Worth Wanting.",
    "source_method": "year_navigation_2017_page_1"
  },
  {
    "title": "Relational equality for extended minds.",
    "authors": [
      "Matilda Carter"
    ],
    "year": 2025,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/CARREF-3",
    "abstract": "This paper deals with the impact of the extended mind thesis on relational egalitarianism: the now-dominant view on (the politically relevant form of) equality within contemporary political philosophy. If proponents of the extended mind thesis are right, I argue, persons have two core interests that arise from their relationships with elements of the external environment: an interest in an environment supportive of cognition and an interest in extended mental authenticity. Acknowledging this requires relational egalitarians to be spatially-conscious, giving these interests (...) due weight wherever they are engaged. This will not always change their conclusions, but there are a range of cases in which this form of relational egalitarianism yields unique insights. In this paper, I examine three: the relationship between landlords and tenants, cloud software and tech ecosystems, and forced transfers for dementia care. (shrink)",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Value inheritance: the transmission of values through cognitive extenders.",
    "authors": [
      "Helena Gagnier"
    ],
    "year": 2025,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/GAGVIT",
    "abstract": "This paper explores the cognitive and ethical implications of cognitive extenders--tools that become integral to human cognition by functioning as part of the mind itself. Building on the Extended Mind Thesis, which posits that cognition extends beyond the brain to incorporate external aids, I argue that cognitive extenders, particularly artificial intelligence (AI) tools, can embed and transmit values to users through a process I call \"value inheritance.\" By examining the relationship between cognition and value-laden technology, I propose that if a (...) technology becomes a cognitive extender, its embedded values may influence the user's mental content. I show that technological tools are not neutral; they carry values that can affect users' beliefs and behaviours. Specifically, I examine AI extenders--such as decision-making assistants and language models--that seamlessly integrate with users, potentially embedding biases and societal norms in subtle, unconscious ways. This raises ethical concerns, particularly regarding the role of AI developers and policymakers in designing tools that prioritize ethical considerations. The paper concludes with a discussion of the ethical stakes in AI design, emphasizing the need for transparency, responsible development, and alignment with societal values to mitigate the risks of unintentional value inheritance. Understanding this potential for value-laden cognitive influence is essential for navigating the ethical landscape of AI technology. (shrink)",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Why causal inference is necessary for algorithmic fairness.",
    "authors": [
      "Alexander Williams Tolbert"
    ],
    "year": 2025,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/WILWCI-6",
    "abstract": "This paper argues that causal inference is a necessary condition for achieving fairness in algorithmic decision-making. Dominant machine learning models are typically limited to associative methods. However, we often need to modify the very probability distributions that produce social injustice, not merely identify predictive patterns from them, an undertaking standard machine learning neglects. Fairness often requires identifying who or what is responsible for a particular outcome of interest, uncovering the source of disparities, and determining which policies are effective interventions to (...) address these issues. Normative questions such as these require causal modeling. I then confront a key objection: certain social variables, notably race, appear to violate modularity in causal models. To address this, I propose several strategies, including bracketing subsystems into coarser causal abstractions/macro-variables, refining the study design, and relaxing local invariances to handle these non-local influences. In sum, I claim that causal modeling is indispensable for responsible algorithmic decision-making. (shrink)",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Calling for Explanation.",
    "authors": [
      "David Gordon"
    ],
    "year": 2025,
    "journal": "The Philosophical Quarterly",
    "url": "https://philpapers.org/rec/GORCFE",
    "abstract": "",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Relational equality for extended minds.",
    "authors": [
      "Matilda Carter"
    ],
    "year": 2025,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/CARREF-3",
    "abstract": "This paper deals with the impact of the extended mind thesis on relational egalitarianism: the now-dominant view on (the politically relevant form of) equality within contemporary political philosophy. If proponents of the extended mind thesis are right, I argue, persons have two core interests that arise from their relationships with elements of the external environment: an interest in an environment supportive of cognition and an interest in extended mental authenticity. Acknowledging this requires relational egalitarians to be spatially-conscious, giving these interests (...) due weight wherever they are engaged. This will not always change their conclusions, but there are a range of cases in which this form of relational egalitarianism yields unique insights. In this paper, I examine three: the relationship between landlords and tenants, cloud software and tech ecosystems, and forced transfers for dementia care. (shrink)",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Value inheritance: the transmission of values through cognitive extenders.",
    "authors": [
      "Helena Gagnier"
    ],
    "year": 2025,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/GAGVIT",
    "abstract": "This paper explores the cognitive and ethical implications of cognitive extenders--tools that become integral to human cognition by functioning as part of the mind itself. Building on the Extended Mind Thesis, which posits that cognition extends beyond the brain to incorporate external aids, I argue that cognitive extenders, particularly artificial intelligence (AI) tools, can embed and transmit values to users through a process I call \"value inheritance.\" By examining the relationship between cognition and value-laden technology, I propose that if a (...) technology becomes a cognitive extender, its embedded values may influence the user's mental content. I show that technological tools are not neutral; they carry values that can affect users' beliefs and behaviours. Specifically, I examine AI extenders--such as decision-making assistants and language models--that seamlessly integrate with users, potentially embedding biases and societal norms in subtle, unconscious ways. This raises ethical concerns, particularly regarding the role of AI developers and policymakers in designing tools that prioritize ethical considerations. The paper concludes with a discussion of the ethical stakes in AI design, emphasizing the need for transparency, responsible development, and alignment with societal values to mitigate the risks of unintentional value inheritance. Understanding this potential for value-laden cognitive influence is essential for navigating the ethical landscape of AI technology. (shrink)",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Why causal inference is necessary for algorithmic fairness.",
    "authors": [
      "Alexander Williams Tolbert"
    ],
    "year": 2025,
    "journal": "Synthese",
    "url": "https://philpapers.org/rec/WILWCI-6",
    "abstract": "This paper argues that causal inference is a necessary condition for achieving fairness in algorithmic decision-making. Dominant machine learning models are typically limited to associative methods. However, we often need to modify the very probability distributions that produce social injustice, not merely identify predictive patterns from them, an undertaking standard machine learning neglects. Fairness often requires identifying who or what is responsible for a particular outcome of interest, uncovering the source of disparities, and determining which policies are effective interventions to (...) address these issues. Normative questions such as these require causal modeling. I then confront a key objection: certain social variables, notably race, appear to violate modularity in causal models. To address this, I propose several strategies, including bracketing subsystems into coarser causal abstractions/macro-variables, refining the study design, and relaxing local invariances to handle these non-local influences. In sum, I claim that causal modeling is indispensable for responsible algorithmic decision-making. (shrink)",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "Calling for Explanation.",
    "authors": [
      "David Gordon"
    ],
    "year": 2025,
    "journal": "The Philosophical Quarterly",
    "url": "https://philpapers.org/rec/GORCFE",
    "abstract": "",
    "source_method": "year_navigation_2025_page_1"
  },
  {
    "title": "What is AI safety? What do we want it to be?",
    "authors": ["Jacqueline Harding", "Cameron Domenico Kirk-Giannini"],
    "year": 2025,
    "journal": "Philosophical Studies",
    "url": "https://doi.org/10.1007/s11098-025-02367-z",
    "abstract": "Challenges prevailing narratives that frame AI safety narrowly around catastrophic risk and engineering analogies. Instead, they argue for the 'Safety Conception'a broader view that defines AI safety as any effort to prevent harm from AI systems.",
    "source_method": "newsletter_july_2025"
  },
  {
    "title": "Resource Rational Contractualism Should Guide AI Alignment",
    "authors": ["Sydney Levine", "Matija Franklin", "Tan Zhi-Xuan"],
    "year": 2025,
    "journal": "arXiv",
    "url": "http://arxiv.org/abs/2506.17434",
    "abstract": "Proposes Resource-Rational Contractualism (RRC), a framework where AI systems approximate agreements rational parties would form using normatively-grounded, cognitively-inspired heuristics that balance effort and accuracy.",
    "source_method": "newsletter_july_2025"
  },
  {
    "title": "A timing problem for instrumental convergence",
    "authors": ["Rhys Southan", "Helena Ward", "Jen Semler"],
    "year": 2025,
    "journal": "Philosophical Studies",
    "url": "https://doi.org/10.1007/s11098-025-02370-4",
    "abstract": "Critiques the assumption that rational agents will preserve their goals, arguing that this instrumental goal preservation thesis fails due to a 'timing problem.'",
    "source_method": "newsletter_july_2025"
  },
  {
    "title": "Is there a tension between AI safety and AI welfare?",
    "authors": ["Robert Long", "Jeff Sebo", "Toni Sims"],
    "year": 2025,
    "journal": "Philosophical Studies",
    "url": "https://doi.org/10.1007/s11098-025-02302-2",
    "abstract": "Examines whether AI safety measures create ethical tensions with AI welfare, arguing there is indeed a moderately strong tension that deserves more examination and thoughtful frameworks for navigation.",
    "source_method": "newsletter_june_2025"
  },
  {
    "title": "Against willing servitude: Autonomy in the ethics of advanced artificial intelligence",
    "authors": ["Adam Bales"],
    "year": 2025,
    "journal": "The Philosophical Quarterly",
    "url": "https://academic.oup.com/pq/advance-article/doi/10.1093/pq/pqaf031/8100849",
    "abstract": "Argues that designing AI systems with moral status to be willing servants would problematically violate their autonomy, raising fundamental questions about the ethics of AI servitude.",
    "source_method": "newsletter_june_2025"
  },
  {
    "title": "The Potential and Limitations of Artificial Colleagues",
    "authors": ["Friedemann Bieber", "Charlotte Franziska Unruh"],
    "year": 2025,
    "journal": "Philosophy & Technology",
    "url": "https://doi.org/10.1007/s13347-025-00890-9",
    "abstract": "Critically assesses whether AI agents in the workplace can fulfill the social and moral goods associated with collegial relationships, arguing they fall short at the collective level.",
    "source_method": "newsletter_may_2025"
  },
  {
    "title": "A Framework to Assess the Persuasion Risks Large Language Model Chatbots Pose to Democratic Societies",
    "authors": ["Friedemann Bieber", "Charlotte Franziska Unruh"],
    "year": 2025,
    "journal": "arXiv",
    "url": "http://arxiv.org/abs/2505.00036",
    "abstract": "Develops a framework for assessing how LLM chatbots might pose persuasion risks to democratic processes through their communicative capabilities.",
    "source_method": "newsletter_may_2025"
  },
  {
    "title": "Neither Direct, Nor Indirect: Understanding Proxy-Based Algorithmic Discrimination",
    "authors": [],
    "year": 2025,
    "journal": "Journal of Ethics and Social Philosophy",
    "url": "https://doi.org/10.1007/s10892-025-09520-0",
    "abstract": "Examines proxy-based algorithmic discrimination that operates through neither direct nor traditionally understood indirect pathways.",
    "source_method": "newsletter_may_2025"
  },
  {
    "title": "A Matter of Principle? AI Alignment as the Fair Treatment of Claims",
    "authors": ["Iason Gabriel", "Geoff Keeling"],
    "year": 2025,
    "journal": "Philosophical Studies",
    "url": "https://link.springer.com/10.1007/s11098-025-02300-4",
    "abstract": "Proposes a new approach to AI alignment rooted in fairness and public justification, arguing for principles derived from fair processes that can be justified to all stakeholders.",
    "source_method": "newsletter_april_2025"
  },
  {
    "title": "Two Types of AI Existential Risk: Decisive and Accumulative",
    "authors": ["Atoosa Kasirzadeh"],
    "year": 2025,
    "journal": "Philosophical Studies",
    "url": "https://doi.org/10.1007/s11098-025-02301-3",
    "abstract": "Distinguishes between two pathways to AI-induced existential catastrophe: decisive (abrupt) and accumulative (gradual erosion through interconnected social risks).",
    "source_method": "newsletter_april_2025"
  },
  {
    "title": "Political Neutrality in AI is ImpossibleBut Here is How to Approximate It",
    "authors": ["Jillian Fisher"],
    "year": 2025,
    "journal": "arXiv",
    "url": "http://arxiv.org/abs/2503.05728",
    "abstract": "Challenges the possibility of true political neutrality in AI systems and proposes practical approximationstools, metrics, and frameworks to balance perspectives.",
    "source_method": "newsletter_april_2025"
  },
  {
    "title": "The Epistemic Cost of Opacity: How the Use of Artificial Intelligence Undermines the Knowledge of Medical Doctors in High-Stakes Contexts",
    "authors": ["Eva Schmidt", "Paul Martin Putora", "Rianne Fijten"],
    "year": 2025,
    "journal": "Philosophy & Technology",
    "url": "https://link.springer.com/article/10.1007/s13347-024-00834-9",
    "abstract": "Examines how the inherent opacity of AI systems can undermine doctors' knowledge in high-stakes medical contexts, even when the systems are statistically reliable.",
    "source_method": "newsletter_march_2025"
  },
  {
    "title": "Are Biological Systems More Intelligent Than Artificial Intelligence?",
    "authors": ["Michael Timothy Bennett"],
    "year": 2025,
    "journal": "OSF Preprints",
    "url": "https://osf.io/e6fky_v2",
    "abstract": "Develops a mathematical framework showing that dynamic, decentralized control in biology enables more efficient adaptation than rigid structures in artificial systems.",
    "source_method": "newsletter_march_2025"
  },
  {
    "title": "Authorship and ChatGPT: a Conservative View",
    "authors": ["Ren van Woudenberg", "Chris Ranalli", "Daniel Bracker"],
    "year": 2025,
    "journal": "Philosophy & Technology",
    "url": "https://doi.org/10.1007/s13347-024-00715-1",
    "abstract": "Argues that despite its human-like text generation, ChatGPT lacks the intentionality, responsibility, and mental states required for true authorship.",
    "source_method": "newsletter_march_2025"
  },
  {
    "title": "Governing the Algorithmic City",
    "authors": ["Seth Lazar"],
    "year": 2025,
    "journal": "Philosophy & Public Affairs",
    "url": "https://onlinelibrary.wiley.com/doi/abs/10.1111/papa.12279",
    "abstract": "Examines how algorithmic systems that mediate our social relationships raise novel questions for political philosophy, introducing the concept of the 'Algorithmic City' and developing frameworks for procedural legitimacy and justification.",
    "source_method": "newsletter_february_2025"
  },
  {
    "title": "Dating Apps and the Digital Sexual Sphere",
    "authors": ["Elsa Kugelberg"],
    "year": 2025,
    "journal": "American Political Science Review",
    "url": "https://www.cambridge.org/core/journals/american-political-science-review/article/dating-apps-and-the-digital-sexual-sphere/2F83AAEFB7DEA94FA4179369A004CEEC",
    "abstract": "Examines dating apps as powerful intermediaries in the 'digital sexual sphere,' arguing that while they offer opportunities for justice, their design often reinforces existing inequalities.",
    "source_method": "newsletter_february_2025"
  },
  {
    "title": "Actions Speak Louder than Words: Agent Decisions Reveal Implicit Biases in Language Models",
    "authors": ["Yuxuan Li", "Hirokazu Shirado", "Sauvik Das"],
    "year": 2025,
    "journal": "arXiv",
    "url": "http://arxiv.org/abs/2501.17420",
    "abstract": "Reveals how language models may harbor implicit biases even when explicit bias has been reduced through alignment, using a novel technique examining LLM-generated agent decisions.",
    "source_method": "newsletter_february_2025"
  },
  {
    "title": "Who Does the Giant Number Pile Like Best: Analyzing Fairness in Hiring Contexts",
    "authors": ["Preethi Seshadri", "Seraphina Goldfarb-Tarrant"],
    "year": 2025,
    "journal": "arXiv",
    "url": "http://arxiv.org/abs/2501.04316",
    "abstract": "Explores fairness in LLM-based hiring systems through resume summarization and retrieval tasks, revealing concerning biases and brittleness.",
    "source_method": "newsletter_january_2025"
  },
  {
    "title": "Desire-Fulfilment and Consciousness",
    "authors": ["Andreas Mogensen"],
    "year": 2025,
    "journal": "Global Priorities Institute",
    "url": "https://globalprioritiesinstitute.org/desire-fulfilment-and-consciousness-andreas-mogensen/",
    "abstract": "Argues that individuals without consciousness can still accrue welfare goods under a nuanced understanding of desire-fulfilment theory.",
    "source_method": "newsletter_january_2025"
  },
  {
    "title": "A Theory of Appropriateness with Applications to Generative AI",
    "authors": ["Joel Z. Leibo"],
    "year": 2024,
    "journal": "arXiv",
    "url": "http://arxiv.org/abs/2412.19010",
    "abstract": "Presents a theory of appropriateness for AI systems based on human social and cognitive foundations.",
    "source_method": "newsletter_january_2025"
  },
  {
    "title": "On the Ethical Considerations of Generative Agents",
    "authors": ["N'yoma Diamond", "Soumya Banerjee"],
    "year": 2024,
    "journal": "arXiv",
    "url": "http://arxiv.org/abs/2411.19211",
    "abstract": "Discusses ethical challenges and concerns posed by generative agents, suggesting guidelines for mitigating systemic risks.",
    "source_method": "newsletter_december_2024"
  },
  {
    "title": "The linguistic dead zone of value-aligned agency",
    "authors": ["Travis LaCroix"],
    "year": 2024,
    "journal": "Philosophical Studies",
    "url": "https://doi.org/10.1007/s11098-024-02257-w",
    "abstract": "Argues that linguistic communication is a necessary condition for robust value alignment in AI systems.",
    "source_method": "newsletter_december_2024"
  },
  {
    "title": "Are Large Language Models Consistent over Value-laden Questions?",
    "authors": ["Jared Moore", "Tanvi Deshpande", "Diyi Yang"],
    "year": 2024,
    "journal": "arXiv",
    "url": "http://arxiv.org/abs/2407.02996",
    "abstract": "Investigates LLM consistency across paraphrases, related questions, and multilingual translations.",
    "source_method": "newsletter_december_2024"
  },
  {
    "title": "Conscious artificial intelligence and biological naturalism",
    "authors": ["Anil Seth"],
    "year": 2024,
    "journal": "OSF Preprints",
    "url": "https://osf.io/tz6an",
    "abstract": "Explores the possibility of AI consciousness, challenging common assumptions about computational consciousness.",
    "source_method": "newsletter_november_2024"
  },
  {
    "title": "Biased AI can Influence Political Decision-Making",
    "authors": ["Jillian Fisher"],
    "year": 2024,
    "journal": "arXiv",
    "url": "http://arxiv.org/abs/2410.06415",
    "abstract": "Presents experiments showing how partisan bias in AI models can influence political decision-making.",
    "source_method": "newsletter_november_2024"
  },
  {
    "title": "Take Caution in Using LLMs as Human Surrogates",
    "authors": ["Yuan Gao", "Dokyun Lee", "Gordon Burtch", "Sina Fazelpour"],
    "year": 2024,
    "journal": "arXiv",
    "url": "http://arxiv.org/abs/2410.19599",
    "abstract": "Examines limitations of using LLMs as human surrogates in social science research.",
    "source_method": "newsletter_november_2024"
  }
]