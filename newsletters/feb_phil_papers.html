<!DOCTYPE html>
<html><head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<meta http-equiv="Content-Security-Policy" content="default-src 'none'; img-src data:; style-src 'unsafe-inline' data:">
		<title>Zotero Report</title>
		<link rel="stylesheet" type="text/css" href="data:text/css;base64,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">
		<link rel="stylesheet" type="text/css" media="screen,projection" href="data:text/css;base64,LyogR2VuZXJpYyBzdHlsZXMgKi8KYm9keSB7Cglmb250OiA2Mi41JSBHZW9yZ2lhLCBUaW1lcywgc2VyaWY7Cgl3aWR0aDogNzgwcHg7CgltYXJnaW46IDAgYXV0bzsKfQoKaDIgewoJZm9udC1zaXplOiAxLjVlbTsKCWxpbmUtaGVpZ2h0OiAxLjVlbTsKCWZvbnQtZmFtaWx5OiBHZW9yZ2lhLCBUaW1lcywgc2VyaWY7Cn0KCnAgewoJbGluZS1oZWlnaHQ6IDEuNWVtOwp9CgphOmFueS1saW5rIHsKCWNvbG9yOiAjOTAwOwp9CgphOmhvdmVyLCBhOmFjdGl2ZSB7Cgljb2xvcjogIzc3NzsKfQoKQG1lZGlhIChwcmVmZXJzLWNvbG9yLXNjaGVtZTogZGFyaykgewoJYTphbnktbGluayB7CgkJY29sb3I6ICNmMDA7Cgl9CgoJYTpob3ZlciwgYTphY3RpdmUgewoJCWNvbG9yOiAjOTk5OwoJfQp9CgoKdWwucmVwb3J0IHsKCWZvbnQtc2l6ZTogMS40ZW07Cgl3aWR0aDogNjgwcHg7CgltYXJnaW46IDAgYXV0bzsKCXBhZGRpbmc6IDIwcHggMjBweDsKfQoKLyogTWV0YWRhdGEgdGFibGUgKi8KdGFibGUgewoJYm9yZGVyOiAxcHggI2NjYyBzb2xpZDsKCW92ZXJmbG93OiBhdXRvOwoJd2lkdGg6IDEwMCU7CgltYXJnaW46IC4xZW0gYXV0byAuNzVlbTsKCXBhZGRpbmc6IDAuNWVtOwp9Cg==">
		<link rel="stylesheet" type="text/css" media="print" href="data:text/css;base64,Ym9keSB7Cglmb250OiAxMnB0ICJUaW1lcyBOZXcgUm9tYW4iLCBUaW1lcywgR2VvcmdpYSwgc2VyaWY7CgltYXJnaW46IDA7Cgl3aWR0aDogYXV0bzsKfQoKLyogUGFnZSBCcmVha3MgKHBhZ2UtYnJlYWstaW5zaWRlIG9ubHkgcmVjb2duaXplZCBieSBPcGVyYSkgKi8KaDEsIGgyLCBoMywgaDQsIGg1LCBoNiB7CglwYWdlLWJyZWFrLWFmdGVyOiBhdm9pZDsKCXBhZ2UtYnJlYWstaW5zaWRlOiBhdm9pZDsKfQoKdWwsIG9sLCBkbCB7CglwYWdlLWJyZWFrLWluc2lkZTogYXZvaWQ7Cgljb2xvci1hZGp1c3Q6IGV4YWN0Owp9CgpoMiB7Cglmb250LXNpemU6IDEuM2VtOwoJbGluZS1oZWlnaHQ6IDEuM2VtOwp9CgphIHsKCWNvbG9yOiBpbmhlcml0OwoJdGV4dC1kZWNvcmF0aW9uOiBub25lOwp9Cg==">
	</head>
	<body>
		<ul class="report combineChildItems">
			<li id="item_K9JKGR76" class="item preprint">
			<h2>Actions Speak Louder than Words: Agent Decisions Reveal Implicit Biases in Language Models</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yuxuan Li</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hirokazu Shirado</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sauvik Das</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>While advances in fairness and alignment have helped mitigate 
overt biases exhibited by large language models (LLMs) when explicitly 
prompted, we hypothesize that these models may still exhibit implicit 
biases when simulating human behavior. To test this hypothesis, we 
propose a technique to systematically uncover such biases across a broad
 range of sociodemographic categories by assessing decision-making 
disparities among agents with LLM-generated, 
sociodemographically-informed personas. Using our technique, we tested 
six LLMs across three sociodemographic groups and four decision-making 
scenarios. Our results show that state-of-the-art LLMs exhibit 
significant sociodemographic disparities in nearly all simulations, with
 more advanced models exhibiting greater implicit biases despite 
reducing explicit biases. Furthermore, when comparing our findings to 
real-world disparities reported in empirical studies, we find that the 
biases we uncovered are directionally aligned but markedly amplified. 
This directional alignment highlights the utility of our technique in 
uncovering systematic biases in LLMs rather than random variations; 
moreover, the presence and amplification of implicit biases emphasizes 
the need for novel strategies to address these biases.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-01-29</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Actions Speak Louder than Words</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2501.17420">http://arxiv.org/abs/2501.17420</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/31/2025, 1:11:37 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2501.17420 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2501.17420">10.48550/arXiv.2501.17420</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2501.17420</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/31/2025, 1:11:37 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/31/2025, 1:11:40 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Human-Computer Interaction</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_9U6SV2TN">Preprint PDF					</li>
					<li id="item_L72K2K7T">Snapshot					</li>
				</ul>
			</li>


			<li id="item_MDE43IU6" class="item journalArticle">
			<h2>AI language model rivals expert ethicist in perceived moral expertise</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Danica Dillion</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Debanjan Mondal</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Niket Tandon</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kurt Gray</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>People view AI as possessing expertise across various fields, 
but the perceived quality of AI-generated moral expertise remains 
uncertain. Recent work suggests that large language models (LLMs) 
perform well on tasks designed to assess moral alignment, reflecting 
moral judgments with relatively high accuracy. As LLMs are increasingly 
employed in decision-making roles, there is a growing expectation for 
them to offer not just aligned judgments but also demonstrate sound 
moral reasoning. Here, we advance work on the Moral Turing Test and find
 that Americans rate ethical advice from GPT-4o as slightly more moral, 
trustworthy, thoughtful, and correct than that of the popular New York 
Times advice column, The Ethicist. Participants perceived GPT models as 
surpassing both a representative sample of Americans and a renowned 
ethicist in delivering moral justifications and advice, suggesting that 
people may increasingly view LLM outputs as viable sources of moral 
expertise. This work suggests that people might see LLMs as valuable 
complements to human expertise in moral guidance and decision-making. It
 also underscores the importance of carefully programming ethical 
guidelines in LLMs, considering their potential to influence users’ 
moral reasoning.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-02-03</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>www.nature.com</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.nature.com/articles/s41598-025-86510-0">https://www.nature.com/articles/s41598-025-86510-0</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>2/13/2025, 11:15:52 AM</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>2025 The Author(s)</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: Nature Publishing Group</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>15</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>4084</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Scientific Reports</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1038/s41598-025-86510-0">10.1038/s41598-025-86510-0</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Sci Rep</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2045-2322</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>2/13/2025, 11:15:52 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>2/13/2025, 11:15:54 AM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer science</li>
					<li>Psychology</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_SYPB9WCQ">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_HNVQUYUZ" class="item attachment">
			<h2>AI-Action-Summit-Tool-AI-Explainer-V5.pdf</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Attachment</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://futureoflife.org/wp-content/uploads/2025/02/AI-Action-Summit-Tool-AI-Explainer-V5.pdf">https://futureoflife.org/wp-content/uploads/2025/02/AI-Action-Summit-Tool-AI-Explainer-V5.pdf</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>2/13/2025, 11:27:23 AM</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>2/13/2025, 11:27:23 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>2/13/2025, 11:27:23 AM</td>
					</tr>
				</tbody></table>
			</li>


			<li id="item_ZLI3UZVZ" class="item newspaperArticle">
			<h2>Ban on D.E.I. Language Sweeps Through the Sciences</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Newspaper Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Katrina Miller</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Roni Caryn Rabin</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>President Trump’s executive order is altering scientific 
exploration across a broad swath of fields, even beyond government 
agencies, researchers say.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-02-09</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-US</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>NYTimes.com</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.nytimes.com/2025/02/09/science/trump-dei-science.html">https://www.nytimes.com/2025/02/09/science/trump-dei-science.html</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>2/13/2025, 11:28:02 AM</td>
					</tr>
					<tr>
					<th>Section</th>
						<td>Science</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>The New York Times</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0362-4331</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>2/13/2025, 11:28:02 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>2/13/2025, 11:28:02 AM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Physics</li>
					<li>Research</li>
					<li>Brookhaven (NY)</li>
					<li>Brookhaven National Laboratory</li>
					<li>Colleges and Universities</li>
					<li>Discrimination</li>
					<li>Diversity Initiatives</li>
					<li>Engineering and Engineers</li>
					<li>Executive Orders and Memorandums</li>
					<li>Fermi National Accelerator Laboratory</li>
					<li>Flags, Emblems and Insignia</li>
					<li>Homosexuality and Bisexuality</li>
					<li>Hughes, Howard Medical Institute</li>
					<li>Laboratories and Scientific Equipment</li>
					<li>Minorities</li>
					<li>National Academies of the United States</li>
					<li>National Aeronautics and Space Administration</li>
					<li>National Institutes of Health</li>
					<li>National Science Foundation</li>
					<li>Science and Technology</li>
					<li>Space and Astronomy</li>
					<li>Transgender</li>
					<li>Trump, Donald J</li>
					<li>United States Politics and Government</li>
					<li>your-feed-science</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_DLRRQWYD">Snapshot					</li>
				</ul>
			</li>


			<li id="item_NHG9DJVE" class="item journalArticle">
			<h2>Construct Validity in Automated Counterterrorism Analysis</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Adrian K. Yee</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Governments and social scientists are increasingly developing 
machine learning methods to automate the process of identifying 
terrorists in real time and predict future attacks. However, current 
operationalizations of “terrorist”’ in artificial intelligence are 
difficult to justify given three issues that remain neglected: 
insufficient construct legitimacy, insufficient criterion validity, and 
insufficient construct validity. I conclude that machine learning 
methods should be at most used for the identification of singular 
individuals deemed terrorists and not for identifying possible 
terrorists from some more general class, nor to predict terrorist 
attacks more broadly, given intolerably high risks that result from such
 approaches.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-11-27</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.cambridge.org/core/product/identifier/S0031824824000655/type/journal_article">https://www.cambridge.org/core/product/identifier/S0031824824000655/type/journal_article</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>2/13/2025, 11:45:34 AM</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>https://creativecommons.org/licenses/by/4.0/</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1-18</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Philosophy of Science</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1017/psa.2024.65">10.1017/psa.2024.65</a></td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Philos. sci.</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0031-8248, 1539-767X</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>2/13/2025, 11:45:34 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>2/13/2025, 11:45:34 AM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_UI3XGZHQ">PDF					</li>
				</ul>
			</li>


			<li id="item_VEH6BTZ8" class="item journalArticle">
			<h2>Dating Apps and the Digital Sexual Sphere</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Elsa Kugelberg</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The online dating application has in recent years become a 
major avenue for meeting potential partners. However, while the digital 
public sphere has gained the attention of political philosophers, a 
systematic normative evaluation of issues arising in the “digital sexual
 sphere” is lacking. I provide a philosophical framework for assessing 
the conduct of dating app corporations, capturing both the motivations 
of users, and the reason why they find usage unsatisfying. Identifying 
dating apps as agents intervening in a social institution necessary for 
the reproduction of society, with immense power over people’s lives, I 
ask if they exercise their power in line with individuals’ interests. 
Acknowledging that people have claims to noninterference, equal 
standing, and choice improvement relating to intimacy, I find that the 
traditional, nondigital, sexual sphere poses problems to their 
realisation, especially for sexual minorities. In this context, apps’ 
potential for justice in the sexual sphere is immense but unfulfilled.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025/01/30</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Cambridge University Press</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.cambridge.org/core/journals/american-political-science-review/article/dating-apps-and-the-digital-sexual-sphere/2F83AAEFB7DEA94FA4179369A004CEEC">https://www.cambridge.org/core/journals/american-political-science-review/article/dating-apps-and-the-digital-sexual-sphere/2F83AAEFB7DEA94FA4179369A004CEEC</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/30/2025, 9:03:47 AM</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1-16</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>American Political Science Review</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1017/S000305542400128X">10.1017/S000305542400128X</a></td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0003-0554, 1537-5943</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/30/2025, 9:03:47 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/30/2025, 9:03:50 AM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_QPR4W3U8">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_BM8IKKUF" class="item journalArticle">
			<h2>Deception and manipulation in generative AI</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Christian Tarsney</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large language models now possess human-level linguistic 
abilities in many contexts. This raises the concern that they can be 
used to deceive and manipulate on unprecedented scales, for instance 
spreading political misinformation on social media. In future, agentic 
AI systems might also deceive and manipulate humans for their own 
purposes. In this paper, first, I argue that AI-generated content should
 be subject to stricter standards against deception and manipulation 
than we ordinarily apply to humans. Second, I offer new 
characterizations of AI deception and manipulation meant to support such
 standards, according to which a statement is deceptive (resp. 
manipulative) if it leads human addressees away from the beliefs (resp. 
choices) they would endorse under “semi-ideal” conditions. Third, I 
propose two measures to guard against AI deception and manipulation, 
inspired by this characterization: “extreme transparency” requirements 
for AI-generated content and “defensive systems” that, among other 
things, annotate AI-generated statements with contextualizing 
information. Finally, I consider to what extent these measures can 
protect against deceptive behavior in future, agentic AI systems.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-01-18</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Springer Link</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1007/s11098-024-02259-8">https://doi.org/10.1007/s11098-024-02259-8</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/27/2025, 8:30:09 PM</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Philosophical Studies</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/s11098-024-02259-8">10.1007/s11098-024-02259-8</a></td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Philos Stud</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1573-0883</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/27/2025, 8:30:09 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/27/2025, 8:30:56 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Artificial intelligence</li>
					<li>Artificial Intelligence</li>
					<li>AI safety</li>
					<li>AI ethics</li>
					<li>Deception</li>
					<li>Manipulation</li>
					<li>Trustworthy AI</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_X45MN87U">PDF					</li>
				</ul>
			</li>


			<li id="item_96FUWCC3" class="item preprint">
			<h2>Fully Autonomous AI Agents Should Not be Developed</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Margaret Mitchell</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Avijit Ghosh</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alexandra Sasha Luccioni</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Giada Pistilli</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This paper argues that fully autonomous AI agents should not 
be developed. In support of this position, we build from prior 
scientific literature and current product marketing to delineate 
different AI agent levels and detail the ethical values at play in each,
 documenting trade-offs in potential benefits and risks. Our analysis 
reveals that risks to people increase with the autonomy of a system: The
 more control a user cedes to an AI agent, the more risks to people 
arise. Particularly concerning are safety risks, which affect human life
 and impact further values.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-02-04</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2502.02649">http://arxiv.org/abs/2502.02649</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>2/6/2025, 9:51:01 AM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2502.02649 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2502.02649">10.48550/arXiv.2502.02649</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2502.02649</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>2/6/2025, 9:51:01 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>2/6/2025, 9:51:04 AM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_CM7XLKMW">Preprint PDF					</li>
					<li id="item_A7CK2P8X">Snapshot					</li>
				</ul>
			</li>


			<li id="item_2SFTNEGC" class="item journalArticle">
			<h2>Governing the Algorithmic City</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Seth Lazar</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Wiley Online Library</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/papa.12279">https://onlinelibrary.wiley.com/doi/abs/10.1111/papa.12279</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>2/1/2025, 3:10:39 PM</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>© 2025 The Author(s). Philosophy &amp; Public Affairs published by Wiley Periodicals LLC.</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/papa.12279</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>n/a</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Philosophy &amp; Public Affairs</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1111/papa.12279">10.1111/papa.12279</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>n/a</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1088-4963</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>2/1/2025, 3:10:39 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>2/1/2025, 3:10:44 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_UHRMYTTP">Full Text PDF					</li>
					<li id="item_UFUML8AT">Snapshot					</li>
				</ul>
			</li>


			<li id="item_QTAEIXM7" class="item preprint">
			<h2>Gradual Disempowerment: Systemic Existential Risks from Incremental AI Development</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jan Kulveit</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Raymond Douglas</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nora Ammann</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Deger Turan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David Krueger</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David Duvenaud</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This paper examines the systemic risks posed by incremental 
advancements in artificial intelligence, developing the concept of 
`gradual disempowerment', in contrast to the abrupt takeover scenarios 
commonly discussed in AI safety. We analyze how even incremental 
improvements in AI capabilities can undermine human influence over 
large-scale systems that society depends on, including the economy, 
culture, and nation-states. As AI increasingly replaces human labor and 
cognition in these domains, it can weaken both explicit human control 
mechanisms (like voting and consumer choice) and the implicit alignments
 with human interests that often arise from societal systems' reliance 
on human participation to function. Furthermore, to the extent that 
these systems incentivise outcomes that do not line up with human 
preferences, AIs may optimize for those outcomes more aggressively. 
These effects may be mutually reinforcing across different domains: 
economic power shapes cultural narratives and political decisions, while
 cultural shifts alter economic and political behavior. We argue that 
this dynamic could lead to an effectively irreversible loss of human 
influence over crucial societal systems, precipitating an existential 
catastrophe through the permanent disempowerment of humanity. This 
suggests the need for both technical research and governance approaches 
that specifically address the risk of incremental erosion of human 
influence across interconnected societal systems.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-01-29</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Gradual Disempowerment</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2501.16946">http://arxiv.org/abs/2501.16946</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/31/2025, 1:07:31 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2501.16946 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2501.16946">10.48550/arXiv.2501.16946</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2501.16946</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/31/2025, 1:07:31 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/31/2025, 1:07:36 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_2IF6R3LT">
<p class="plaintext">Comment: 19 pages, 2 figures</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_4UA48YGR">Preprint PDF					</li>
					<li id="item_29YPXEZT">Snapshot					</li>
				</ul>
			</li>


			<li id="item_9VX6U4D8" class="item preprint">
			<h2>IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in LLM Writing Assistance</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Paul Röttger</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Musashi Hinck</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Valentin Hofmann</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kobi Hackenburg</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Valentina Pyatkin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Faeze Brahman</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dirk Hovy</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large language models (LLMs) are helping millions of users 
write texts about diverse issues, and in doing so expose users to 
different ideas and perspectives. This creates concerns about issue 
bias, where an LLM tends to present just one perspective on a given 
issue, which in turn may influence how users think about this issue. So 
far, it has not been possible to measure which issue biases LLMs 
actually manifest in real user interactions, making it difficult to 
address the risks from biased LLMs. Therefore, we create IssueBench: a 
set of 2.49m realistic prompts for measuring issue bias in LLM writing 
assistance, which we construct based on 3.9k templates (e.g. "write a 
blog about") and 212 political issues (e.g. "AI regulation") from real 
user interactions. Using IssueBench, we show that issue biases are 
common and persistent in state-of-the-art LLMs. We also show that biases
 are remarkably similar across models, and that all models align more 
with US Democrat than Republican voter opinion on a subset of issues. 
IssueBench can easily be adapted to include other issues, templates, or 
tasks. By enabling robust and realistic measurement, we hope that 
IssueBench can bring a new quality of evidence to ongoing discussions 
about LLM biases and how to address them.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-02-12</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>IssueBench</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2502.08395">http://arxiv.org/abs/2502.08395</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>2/13/2025, 11:48:19 AM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2502.08395 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2502.08395">10.48550/arXiv.2502.08395</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2502.08395</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>2/13/2025, 11:48:19 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>2/13/2025, 11:48:19 AM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_4JKZWCSJ">
<p class="plaintext">Comment: under review</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_UYU49KBM">Preprint PDF					</li>
					<li id="item_KLGT9PG9">Snapshot					</li>
				</ul>
			</li>


			<li id="item_3IZNH782" class="item preprint">
			<h2>Key concepts and current beliefs about AI moral patienthood</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Robert Long</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Prior to the launch of Eleos AI Research, Robert Long wrote a 
document in order to communicate his views about AI welfare to his 
collaborators—to Kyle Fish, who was working closely with Rob at the time
 and provided extensive input on this document; and more broadly, to 
others interested in working on AI welfare.

This document outlines the current thinking of Eleos AI Research on the 
potential moral patienthood, welfare, and rights of artificial 
intelligence (AI) systems. It lays out some the relevant terminology and
 concepts that we use to think and communicate about these issues, and 
reviews existing approaches to evaluating AI systems for three features 
potentially relevant to moral patienthood: consciousness, sentience, and
 agency. Throughout, we emphasize the need for more thorough research 
and more precise evaluations, and conclude by identifying some promising
 research directions.</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://localhost:4321/post/key-concepts-and-current-beliefs-about-ai-moral-patienthood/">http://localhost:4321/post/key-concepts-and-current-beliefs-about-ai-moral-patienthood/</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/29/2025, 2:03:30 PM</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/29/2025, 2:03:30 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/29/2025, 2:04:31 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_5JYU26QM">20250127-Eleos-background-thinking-upload.pdf					</li>
					<li id="item_TV5N9XD3">Snapshot					</li>
				</ul>
			</li>


			<li id="item_ELK2ZJZW" class="item journalArticle">
			<h2>Out-group animosity drives engagement on social media</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Steve Rathje</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jay J. Van Bavel</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sander van der Linden</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>There has been growing concern about the role social media 
plays in political polarization. We investigated whether out-group 
animosity was particularly successful at generating engagement on two of
 the largest social media platforms: Facebook and Twitter. Analyzing 
posts from news media accounts and US congressional members (n = 
2,730,215), we found that posts about the political out-group were 
shared or retweeted about twice as often as posts about the in-group. 
Each individual term referring to the political out-group increased the 
odds of a social media post being shared by 67%. Out-group language 
consistently emerged as the strongest predictor of shares and retweets: 
the average effect size of out-group language was about 4.8 times as 
strong as that of negative affect language and about 6.7 times as strong
 as that of moral-emotional language—both established predictors of 
social media engagement. Language about the out-group was a very strong 
predictor of “angry” reactions (the most popular reactions across all 
datasets), and language about the in-group was a strong predictor of 
“love” reactions, reflecting in-group favoritism and out-group 
derogation. This out-group effect was not moderated by political 
orientation or social media platform, but stronger effects were found 
among political leaders than among news media accounts. In sum, 
out-group language is the strongest predictor of social media engagement
 across all relevant predictors measured, suggesting that social media 
may be creating perverse incentives for content expressing out-group 
animosity.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2021-06-29</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>pnas.org (Atypon)</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.pnas.org/doi/10.1073/pnas.2024292118">https://www.pnas.org/doi/10.1073/pnas.2024292118</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>2/13/2025, 11:27:40 AM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: Proceedings of the National Academy of Sciences</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>118</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>e2024292118</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Proceedings of the National Academy of Sciences</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1073/pnas.2024292118">10.1073/pnas.2024292118</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>26</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>2/13/2025, 11:27:40 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>2/13/2025, 11:27:40 AM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_ES3PTPSP">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_AFMFQVF4" class="item manuscript">
			<h2>Propositional Interpretability in Artificial Intelligence</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Manuscript</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David J. Chalmers</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Mechanistic interpretability in artificial intelligence aims 
to explain AI behavior in human-understandable terms, with a particular 
focus on internal mechanisms. This paper introduces and defends 
propositional interpretability, which interprets an AI system’s internal
 states in terms of propositional attitudes—such as beliefs, desires, 
and probabilities—akin to those in human cognition. Propositional 
interpretability is crucial for AI safety, ethics, and cognitive 
science, offering insight into an AI system’s goals, decision-making 
processes, and world models. The paper outlines thought logging as a 
central challenge: systematically tracking an AI system’s propositional 
attitudes over time. Several existing interpretability methods—including
 causal tracing, probing, sparse auto-encoders, and chain-of-thought 
techniques—are assessed for their potential to contribute to thought 
logging. The discussion also engages with philosophical questions about 
AI psychology, psychosemantics, and externalism, ultimately arguing that
 propositional interpretability provides a powerful explanatory 
framework for understanding and evaluating AI systems.</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>PhilPapers</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/27/2025, 8:31:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/29/2025, 2:22:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_HC5CWKA8">PDF					</li>
					<li id="item_SMMVW4R4">Snapshot					</li>
				</ul>
			</li>


			<li id="item_8YLKEXDF" class="item preprint">
			<h2>User-Driven Value Alignment: Understanding Users' Perceptions and
 Strategies for Addressing Biased and Discriminatory Statements in AI 
Companions</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xianzhe Fan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Qing Xiao</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xuhui Zhou</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jiaxin Pei</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Maarten Sap</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zhicong Lu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hong Shen</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large language model-based AI companions are increasingly 
viewed by users as friends or romantic partners, leading to deep 
emotional bonds. However, they can generate biased, discriminatory, and 
harmful outputs. Recently, users are taking the initiative to address 
these harms and re-align AI companions. We introduce the concept of 
user-driven value alignment, where users actively identify, challenge, 
and attempt to correct AI outputs they perceive as harmful, aiming to 
guide the AI to better align with their values. We analyzed 77 social 
media posts about discriminatory AI statements and conducted 
semi-structured interviews with 20 experienced users. Our analysis 
revealed six common types of discriminatory statements perceived by 
users, how users make sense of those AI behaviors, and seven user-driven
 alignment strategies, such as gentle persuasion and anger expression. 
We discuss implications for supporting user-driven value alignment in 
future AI systems, where users and their communities have greater 
agency.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-09-01</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>User-Driven Value Alignment</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2409.00862">http://arxiv.org/abs/2409.00862</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>2/13/2025, 11:27:55 AM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2409.00862 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2409.00862">10.48550/arXiv.2409.00862</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2409.00862</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>2/13/2025, 11:27:55 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>2/13/2025, 11:27:55 AM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Human-Computer Interaction</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_93NXKDHB">
<p class="plaintext">Comment: 17 pages, 1 figure</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_EWFYPAJM">Preprint PDF					</li>
					<li id="item_MAIM4LQE">Snapshot					</li>
				</ul>
			</li>


			<li id="item_NL3J7HQ6" class="item journalArticle">
			<h2>What Is It for a Machine Learning Model to Have a Capability?</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jacqueline Harding</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nathaniel Sharadin</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>What can contemporary machine learning (ML) models do? Given 
the proliferation of ML models in society, answering this question 
matters to a variety of stakeholders, both public and private. The 
evaluation of models’ capabilities is rapidly emerging as a key subfield
 of modern ML, buoyed by regulatory attention and government grants. 
Despite this, the notion of an ML model possessing a capability has not 
been interrogated: what are we saying when we say that a model is able 
to do something? And what sorts of evidence bear upon this question? In 
this paper, we aim to answer these questions, using the capabilities of 
large language models (LLMs) as a running example. Drawing on the large 
philosophical literature on abilities, we develop an account of ML 
models’ capabilities which can be usefully applied to the nascent 
science of model evaluation. Our core proposal is a conditional analysis
 of model abilities (CAMA): crudely, a machine learning model has a 
capability to X just when it would reliably succeed at doing X if it 
‘tried’. The main contribution of the paper is making this proposal 
precise in the context of ML, resulting in an operationalisation of CAMA
 applicable to LLMs. We then put CAMA to work, showing that it can help 
make sense of various features of ML model evaluation practice, as well 
as suggest procedures for performing fair inter-model comparisons.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-07-09</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.journals.uchicago.edu/doi/10.1086/732153">https://www.journals.uchicago.edu/doi/10.1086/732153</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>2/13/2025, 11:49:55 AM</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>732153</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>The British Journal for the Philosophy of Science</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1086/732153">10.1086/732153</a></td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>The British Journal for the Philosophy of Science</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0007-0882, 1464-3537</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>2/13/2025, 11:49:55 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>2/13/2025, 11:49:55 AM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_GLB3WIFN">PDF					</li>
				</ul>
			</li>

		</ul>
	
</body></html>