<!DOCTYPE html>
<html><head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<meta http-equiv="Content-Security-Policy" content="default-src 'none'; img-src data:; style-src 'unsafe-inline' data:">
		<title>Zotero Report</title>
		<link rel="stylesheet" type="text/css" href="data:text/css;base64,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">
		<link rel="stylesheet" type="text/css" media="screen,projection" href="data:text/css;base64,LyogR2VuZXJpYyBzdHlsZXMgKi8KYm9keSB7Cglmb250OiA2Mi41JSBHZW9yZ2lhLCBUaW1lcywgc2VyaWY7Cgl3aWR0aDogNzgwcHg7CgltYXJnaW46IDAgYXV0bzsKfQoKaDIgewoJZm9udC1zaXplOiAxLjVlbTsKCWxpbmUtaGVpZ2h0OiAxLjVlbTsKCWZvbnQtZmFtaWx5OiBHZW9yZ2lhLCBUaW1lcywgc2VyaWY7Cn0KCnAgewoJbGluZS1oZWlnaHQ6IDEuNWVtOwp9CgphOmFueS1saW5rIHsKCWNvbG9yOiAjOTAwOwp9CgphOmhvdmVyLCBhOmFjdGl2ZSB7Cgljb2xvcjogIzc3NzsKfQoKQG1lZGlhIChwcmVmZXJzLWNvbG9yLXNjaGVtZTogZGFyaykgewoJYTphbnktbGluayB7CgkJY29sb3I6ICNmMDA7Cgl9CgoJYTpob3ZlciwgYTphY3RpdmUgewoJCWNvbG9yOiAjOTk5OwoJfQp9CgoKdWwucmVwb3J0IHsKCWZvbnQtc2l6ZTogMS40ZW07Cgl3aWR0aDogNjgwcHg7CgltYXJnaW46IDAgYXV0bzsKCXBhZGRpbmc6IDIwcHggMjBweDsKfQoKLyogTWV0YWRhdGEgdGFibGUgKi8KdGFibGUgewoJYm9yZGVyOiAxcHggI2NjYyBzb2xpZDsKCW92ZXJmbG93OiBhdXRvOwoJd2lkdGg6IDEwMCU7CgltYXJnaW46IC4xZW0gYXV0byAuNzVlbTsKCXBhZGRpbmc6IDAuNWVtOwp9Cg==">
		<link rel="stylesheet" type="text/css" media="print" href="data:text/css;base64,Ym9keSB7Cglmb250OiAxMnB0ICJUaW1lcyBOZXcgUm9tYW4iLCBUaW1lcywgR2VvcmdpYSwgc2VyaWY7CgltYXJnaW46IDA7Cgl3aWR0aDogYXV0bzsKfQoKLyogUGFnZSBCcmVha3MgKHBhZ2UtYnJlYWstaW5zaWRlIG9ubHkgcmVjb2duaXplZCBieSBPcGVyYSkgKi8KaDEsIGgyLCBoMywgaDQsIGg1LCBoNiB7CglwYWdlLWJyZWFrLWFmdGVyOiBhdm9pZDsKCXBhZ2UtYnJlYWstaW5zaWRlOiBhdm9pZDsKfQoKdWwsIG9sLCBkbCB7CglwYWdlLWJyZWFrLWluc2lkZTogYXZvaWQ7Cgljb2xvci1hZGp1c3Q6IGV4YWN0Owp9CgpoMiB7Cglmb250LXNpemU6IDEuM2VtOwoJbGluZS1oZWlnaHQ6IDEuM2VtOwp9CgphIHsKCWNvbG9yOiBpbmhlcml0OwoJdGV4dC1kZWNvcmF0aW9uOiBub25lOwp9Cg==">
	</head>
	<body>
		<ul class="report combineChildItems">
			<li id="item_9SYLPPPZ" class="item journalArticle">
			<h2>Against willing servitude: Autonomy in the ethics of advanced artificial intelligence</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Adam Bales</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Abstract
            Some people believe that advanced artificial intelligence 
systems (AIs) might, in the future, come to have moral status. Further, 
humans might be tempted to design such AIs that they serve us, carrying 
out tasks that make our lives better. This raises the question of 
whether designing AIs with moral status to be willing servants would 
problematically violate their autonomy. In this paper, I argue that it 
would in fact do so.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-03-31</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Against willing servitude</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://academic.oup.com/pq/advance-article/doi/10.1093/pq/pqaf031/8100849">https://academic.oup.com/pq/advance-article/doi/10.1093/pq/pqaf031/8100849</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>6/11/2025, 1:12:59 PM</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>https://creativecommons.org/licenses/by/4.0/</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>pqaf031</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>The Philosophical Quarterly</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1093/pq/pqaf031">10.1093/pq/pqaf031</a></td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0031-8094, 1467-9213</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>6/11/2025, 1:12:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>6/11/2025, 1:12:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_ZPDYPQ8F">PDF					</li>
				</ul>
			</li>


			<li id="item_ZSWUJE55" class="item journalArticle">
			<h2>Smartphones: Parts of Our Minds? Or Parasites?</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rachael L Brown</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Robert C Brooks</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-05-26</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Smartphones</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.tandfonline.com/doi/full/10.1080/00048402.2025.2504070">https://www.tandfonline.com/doi/full/10.1080/00048402.2025.2504070</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>6/10/2025, 12:58:18 PM</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1-16</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Australasian Journal of Philosophy</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1080/00048402.2025.2504070">10.1080/00048402.2025.2504070</a></td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Australasian Journal of Philosophy</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0004-8402, 1471-6828</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>6/10/2025, 12:58:18 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>6/10/2025, 12:58:18 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_PWLEURM6">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_Y3DFUGJK" class="item journalArticle">
			<h2>AI rule and a fundamental objection to epistocracy</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sean Donahue</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Epistocracy is rule by whoever is more likely to make correct 
decisions. AI epistocracy is rule by an artificial intelligence that is 
more likely to make correct decisions than any humans, individually or 
collectively. I argue that although various objections have been raised 
against epistocracy, the most popular do not apply to epistocracy 
organized around AI rule. I use this result to show that epistocracy is 
fundamentally flawed because none of its forms provide adequate 
opportunity for people (as opposed to individuals) to develop a record 
of meaningful moral achievement. This Collective Moral Achievement 
Objection provides a novel reason to value democracy. It also provides 
guidance on how we ought to incorporate digital technologies into 
politics, regardless of how proficient these technologies may become at 
identifying correct decisions.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-06-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Springer Link</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1007/s00146-024-02175-9">https://doi.org/10.1007/s00146-024-02175-9</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>6/13/2025, 9:07:27 AM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>40</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>4105-4117</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>AI &amp; SOCIETY</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/s00146-024-02175-9">10.1007/s00146-024-02175-9</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>5</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>AI &amp; Soc</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1435-5655</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>6/13/2025, 9:07:27 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>6/13/2025, 9:07:30 AM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Artificial Intelligence</li>
					<li>Democracy</li>
					<li>Epistemology</li>
					<li>Philosophy of Artificial Intelligence</li>
					<li>Collective self-determination</li>
					<li>Critical Thinking</li>
					<li>Epistocracy</li>
					<li>Humanitiy and Technology</li>
					<li>Moral achievement</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_3JTH3R8U">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_A5ZA3VQI" class="item journalArticle">
			<h2>AI assisted ethics</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Amitai Etzioni</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Oren Etzioni</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The growing number of ‘smart’ instruments, those equipped with
 AI, has raised concerns because these instruments make autonomous 
decisions; that is, they act beyond the guidelines provided them by 
programmers. Hence, the question the makers and users of smart 
instrument (e.g., driver-less cars) face is how to ensure that these 
instruments will not engage in unethical conduct (not to be conflated 
with illegal conduct). The article suggests that to proceed we need a 
new kind of AI program—oversight programs—that will monitor, audit, and 
hold operational AI programs accountable.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2016-06-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Springer Link</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1007/s10676-016-9400-6">https://doi.org/10.1007/s10676-016-9400-6</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>6/13/2025, 10:43:46 AM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>18</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>149-156</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Ethics and Information Technology</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/s10676-016-9400-6">10.1007/s10676-016-9400-6</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Ethics Inf Technol</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1572-8439</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>6/13/2025, 10:43:46 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>6/13/2025, 10:43:48 AM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Artificial Intelligence</li>
					<li>Computer Ethics</li>
					<li>Logic in AI</li>
					<li>Engineering Ethics</li>
					<li>Ethics of Technology</li>
					<li>Philosophy of Artificial Intelligence</li>
					<li>Communiterianism</li>
					<li>Driverless cars</li>
					<li>Ethics bot</li>
					<li>Second-layer AI</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_7PX4JJ96">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_8RH6NDH9" class="item preprint">
			<h2>The Philosophic Turn for AI Agents: Replacing centralized digital rhetoric with decentralized truth-seeking</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Philipp Koralus</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In the face of rapidly advancing AI technology, individuals 
will increasingly rely on AI agents to navigate life's growing 
complexities, raising critical concerns about maintaining both human 
agency and autonomy. This paper addresses a fundamental dilemma posed by
 AI decision-support systems: the risk of either becoming overwhelmed by
 complex decisions, thus losing agency, or having autonomy compromised 
by externally controlled choice architectures reminiscent of ``nudging''
 practices. While the ``nudge'' framework, based on the use of 
choice-framing to guide individuals toward presumed beneficial outcomes,
 initially appeared to preserve liberty, at AI-driven scale, it 
threatens to erode autonomy. To counteract this risk, the paper proposes
 a philosophic turn in AI design. AI should be constructed to facilitate
 decentralized truth-seeking and open-ended inquiry, mirroring the 
Socratic method of philosophical dialogue. By promoting individual and 
collective adaptive learning, such AI systems would empower users to 
maintain control over their judgments, augmenting their agency without 
undermining autonomy. The paper concludes by outlining essential 
features for autonomy-preserving AI systems, sketching a path toward AI 
systems that enhance human judgment rather than undermine it.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-04-24</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>The Philosophic Turn for AI Agents</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2504.18601">http://arxiv.org/abs/2504.18601</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>6/10/2025, 1:00:32 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2504.18601 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2504.18601">10.48550/arXiv.2504.18601</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2504.18601</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>6/10/2025, 1:00:32 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>6/10/2025, 1:00:33 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_TPQNWSIZ">Full Text PDF					</li>
					<li id="item_8F4ZJVH2">Snapshot					</li>
				</ul>
			</li>


			<li id="item_BGYRG7UZ" class="item book">
			<h2>Artificial Intelligence and the Value Alignment Problem: A Philosophical Introduction</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Book</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Travis LaCroix</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>eng</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Artificial Intelligence and the Value Alignment Problem</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>K10plus ISBN</td>
					</tr>
					<tr>
					<th>Place</th>
						<td>Peterborough</td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>Broadview Press Ltd</td>
					</tr>
					<tr>
					<th>ISBN</th>
						<td>978-1-55481-629-3</td>
					</tr>
					<tr>
					<th># of Pages</th>
						<td>340</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>6/11/2025, 1:13:58 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>6/11/2025, 1:13:58 PM</td>
					</tr>
				</tbody></table>
			</li>


			<li id="item_VAI9REZN" class="item journalArticle">
			<h2>Metaethical perspectives on ‘benchmarking’ AI ethics</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Travis LaCroix</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alexandra Sasha Luccioni</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Benchmarks are seen as the cornerstone for measuring technical
 progress in artificial intelligence (AI) research and have been 
developed for a variety of tasks ranging from question answering to 
emotion recognition. An increasingly prominent research area in AI is 
ethics, which currently has no set of benchmarks nor commonly accepted 
way for measuring the ‘ethicality’ of an AI system. In this paper, 
drawing upon research in moral philosophy and metaethics, we argue that 
it is impossible to develop such a benchmark. As such, alternative 
mechanisms are necessary for evaluating whether an AI system is 
‘ethical’. This is especially pressing in light of the prevalence of 
applied, industrial AI research. We argue that it makes more sense to 
talk about ‘values’ (and ‘value alignment’) rather than ‘ethics’ when 
considering the possible actions of present and future AI systems. We 
further highlight that, because values are unambiguously relative, 
focusing on values forces us to consider explicitly what the values are 
and whose values they are. Shifting the emphasis from ethics to values 
therefore gives rise to several new ways of understanding how 
researchers might advance research programmes for robustly safe or 
beneficial AI.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-03-19</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Springer Link</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1007/s43681-025-00703-x">https://doi.org/10.1007/s43681-025-00703-x</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>6/11/2025, 1:14:49 PM</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>AI and Ethics</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/s43681-025-00703-x">10.1007/s43681-025-00703-x</a></td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>AI Ethics</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2730-5961</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>6/11/2025, 1:14:49 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>6/11/2025, 1:14:49 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Value alignment</li>
					<li>AI ethics</li>
					<li>Computer Ethics</li>
					<li>Meta-Ethics</li>
					<li>Normative Ethics</li>
					<li>Engineering Ethics</li>
					<li>Philosophy of Artificial Intelligence</li>
					<li>Benchmarking</li>
					<li>Metaethics</li>
					<li>Moral dilemmas</li>
					<li>Research Ethics</li>
					<li>Unit testing</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_G4UVYEP7">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_2GX997MT" class="item bookSection">
			<h2>Honest Ai</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Book Section</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>N. G. Laskowski</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>Henry Shevlin</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>PhilPapers</td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>Oxford University Press</td>
					</tr>
					<tr>
					<th>Book Title</th>
						<td>AI in Society: Relationships (Oxford Intersections)</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>6/11/2025, 9:27:40 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>6/11/2025, 9:27:40 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_683V9IQV">Snapshot					</li>
				</ul>
			</li>


			<li id="item_IQXVRHU2" class="item journalArticle">
			<h2>Is there a tension between AI safety and AI welfare?</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Robert Long</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jeff Sebo</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Toni Sims</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The field of AI safety considers whether and how AI 
development can be safe and beneficial for humans and other animals, and
 the field of AI welfare considers whether and how AI development can be
 safe and beneficial for AI systems. There is a prima facie tension 
between these projects, since some measures in AI safety, if deployed 
against humans and other animals, would raise questions about the ethics
 of constraint, deception, surveillance, alteration, suffering, death, 
disenfranchisement, and more. Is there in fact a tension between these 
projects? We argue that, considering all relevant factors, there is 
indeed a moderately strong tension—and it deserves more 
examination.&nbsp;In particular, we should devise interventions that can
 promote both safety and welfare where possible, and prepare frameworks 
for navigating any remaining tensions thoughtfully.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-05-23</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Springer Link</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1007/s11098-025-02302-2">https://doi.org/10.1007/s11098-025-02302-2</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>6/11/2025, 1:11:30 PM</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Philosophical Studies</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/s11098-025-02302-2">10.1007/s11098-025-02302-2</a></td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Philos Stud</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1573-0883</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>6/11/2025, 1:11:30 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>6/11/2025, 1:11:30 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Artificial Intelligence</li>
					<li>Machine ethics</li>
					<li>AI safety</li>
					<li>Computer Ethics</li>
					<li>AI consciousness</li>
					<li>AI welfare</li>
					<li>Animal Ethics</li>
					<li>Catastrophic risk</li>
					<li>Engineering Ethics</li>
					<li>Ethics of Technology</li>
					<li>Philosophy of Artificial Intelligence</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_5S9GRHMP">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_G7E5A2LD" class="item journalArticle">
			<h2>Normative conflicts and shallow AI alignment</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Raphaël Millière</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The progress of AI systems such as large language models 
(LLMs) raises increasingly pressing concerns about their safe 
deployment. This paper examines the value alignment problem for LLMs, 
arguing that current alignment strategies are fundamentally inadequate 
to prevent misuse. Despite ongoing efforts to instill norms such as 
helpfulness, honesty, and harmlessness in LLMs through fine-tuning based
 on human preferences, they remain vulnerable to adversarial attacks 
that exploit conflicts between these norms. I argue that this 
vulnerability reflects a fundamental limitation of existing alignment 
methods: they reinforce shallow behavioral dispositions rather than 
endowing LLMs with a genuine capacity for normative deliberation. 
Drawing from on research in moral psychology, I show how humans’ ability
 to engage in deliberative reasoning enhances their resilience against 
similar adversarial tactics. LLMs, by contrast, lack a robust capacity 
to detect and rationally resolve normative conflicts, leaving them 
susceptible to manipulation; even recent advances in reasoning-focused 
LLMs have not addressed this vulnerability. This “shallow alignment” 
problem carries significant implications for AI safety and regulation, 
suggesting that current approaches are insufficient for mitigating 
potential harms posed by increasingly capable AI systems.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-05-27</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Springer Link</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1007/s11098-025-02347-3">https://doi.org/10.1007/s11098-025-02347-3</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>6/11/2025, 1:11:25 PM</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Philosophical Studies</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/s11098-025-02347-3">10.1007/s11098-025-02347-3</a></td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Philos Stud</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1573-0883</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>6/11/2025, 1:11:28 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>6/11/2025, 1:11:28 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Artificial Intelligence</li>
					<li>AI safety</li>
					<li>Large language models</li>
					<li>Adversarial attacks</li>
					<li>Alignment problem</li>
					<li>Complexity</li>
					<li>Computer Ethics</li>
					<li>Logic in AI</li>
					<li>Meta-Ethics</li>
					<li>Normative Ethics</li>
					<li>Normative reasoning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_I3GAR3NK">Full Text PDF					</li>
				</ul>
			</li>

		</ul>
	
</body></html>