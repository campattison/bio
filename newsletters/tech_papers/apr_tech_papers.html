<!DOCTYPE html>
<html><head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<meta http-equiv="Content-Security-Policy" content="default-src 'none'; img-src data:; style-src 'unsafe-inline' data:">
		<title>Zotero Report</title>
		<link rel="stylesheet" type="text/css" href="data:text/css;base64,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">
		<link rel="stylesheet" type="text/css" media="screen,projection" href="data:text/css;base64,LyogR2VuZXJpYyBzdHlsZXMgKi8KYm9keSB7Cglmb250OiA2Mi41JSBHZW9yZ2lhLCBUaW1lcywgc2VyaWY7Cgl3aWR0aDogNzgwcHg7CgltYXJnaW46IDAgYXV0bzsKfQoKaDIgewoJZm9udC1zaXplOiAxLjVlbTsKCWxpbmUtaGVpZ2h0OiAxLjVlbTsKCWZvbnQtZmFtaWx5OiBHZW9yZ2lhLCBUaW1lcywgc2VyaWY7Cn0KCnAgewoJbGluZS1oZWlnaHQ6IDEuNWVtOwp9CgphOmFueS1saW5rIHsKCWNvbG9yOiAjOTAwOwp9CgphOmhvdmVyLCBhOmFjdGl2ZSB7Cgljb2xvcjogIzc3NzsKfQoKQG1lZGlhIChwcmVmZXJzLWNvbG9yLXNjaGVtZTogZGFyaykgewoJYTphbnktbGluayB7CgkJY29sb3I6ICNmMDA7Cgl9CgoJYTpob3ZlciwgYTphY3RpdmUgewoJCWNvbG9yOiAjOTk5OwoJfQp9CgoKdWwucmVwb3J0IHsKCWZvbnQtc2l6ZTogMS40ZW07Cgl3aWR0aDogNjgwcHg7CgltYXJnaW46IDAgYXV0bzsKCXBhZGRpbmc6IDIwcHggMjBweDsKfQoKLyogTWV0YWRhdGEgdGFibGUgKi8KdGFibGUgewoJYm9yZGVyOiAxcHggI2NjYyBzb2xpZDsKCW92ZXJmbG93OiBhdXRvOwoJd2lkdGg6IDEwMCU7CgltYXJnaW46IC4xZW0gYXV0byAuNzVlbTsKCXBhZGRpbmc6IDAuNWVtOwp9Cg==">
		<link rel="stylesheet" type="text/css" media="print" href="data:text/css;base64,Ym9keSB7Cglmb250OiAxMnB0ICJUaW1lcyBOZXcgUm9tYW4iLCBUaW1lcywgR2VvcmdpYSwgc2VyaWY7CgltYXJnaW46IDA7Cgl3aWR0aDogYXV0bzsKfQoKLyogUGFnZSBCcmVha3MgKHBhZ2UtYnJlYWstaW5zaWRlIG9ubHkgcmVjb2duaXplZCBieSBPcGVyYSkgKi8KaDEsIGgyLCBoMywgaDQsIGg1LCBoNiB7CglwYWdlLWJyZWFrLWFmdGVyOiBhdm9pZDsKCXBhZ2UtYnJlYWstaW5zaWRlOiBhdm9pZDsKfQoKdWwsIG9sLCBkbCB7CglwYWdlLWJyZWFrLWluc2lkZTogYXZvaWQ7Cgljb2xvci1hZGp1c3Q6IGV4YWN0Owp9CgpoMiB7Cglmb250LXNpemU6IDEuM2VtOwoJbGluZS1oZWlnaHQ6IDEuM2VtOwp9CgphIHsKCWNvbG9yOiBpbmhlcml0OwoJdGV4dC1kZWNvcmF0aW9uOiBub25lOwp9Cg==">
	</head>
	<body>
		<ul class="report combineChildItems">
			<li id="item_T6TU4Z4L" class="item preprint">
			<h2>Vision Language Models as Values Detectors</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Giulio Antonio Abbo</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tony Belpaeme</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large Language Models integrating textual and visual inputs 
have introduced new possibilities for interpreting complex data. Despite
 their remarkable ability to generate coherent and contextually relevant
 text based on visual stimuli, the alignment of these models with human 
perception in identifying relevant elements in images requires further 
exploration. This paper investigates the alignment between 
state-of-the-art LLMs and human annotators in detecting elements of 
relevance within home environment scenarios. We created a set of twelve 
images depicting various domestic scenarios and enlisted fourteen 
annotators to identify the key element in each image. We then compared 
these human responses with outputs from five different LLMs, including 
GPT-4o and four LLaVA variants. Our findings reveal a varied degree of 
alignment, with LLaVA 34B showing the highest performance but still 
scoring low. However, an analysis of the results highlights the models' 
potential to detect value-laden elements in images, suggesting that with
 improved training and refined prompts, LLMs could enhance applications 
in social robotics, assistive technologies, and human-computer 
interaction by providing deeper insights and more contextually relevant 
responses.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-01-07</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2501.03957">http://arxiv.org/abs/2501.03957</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/25/2025, 5:45:31 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2501.03957 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2501.03957">10.48550/arXiv.2501.03957</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2501.03957</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/2/2025, 2:13:04 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/2/2025, 2:13:04 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computer Vision and Pattern Recognition</li>
					<li>Computer Science - Human-Computer Interaction</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_6WB5BAWH">
<p class="plaintext">Comment: 13 pages, 2 figures</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_M3IRSKJH">Preprint PDF					</li>
					<li id="item_W6GUC2PI">Snapshot					</li>
				</ul>
			</li>


			<li id="item_C43IUKG4" class="item journalArticle">
			<h2>(Successful) Democracies Breed Their Own Support</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Daron Acemoglu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nicolás Ajzenman</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Cevat Giray Aksoy</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Martin Fiszbein</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Carlos Molina</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Using large-scale survey data covering more than 110 countries
 and exploiting within-country variation across cohorts and surveys, we 
show that individuals with longer exposure to democracy display stronger
 support for democratic institutions, and that this effect is almost 
entirely driven by exposure to democracies with successful performance 
in terms of economic growth, control of corruption, peace and political 
stability, and public goods provision. Across a variety of 
speciﬁcations, estimation methods, and samples, the results are robust, 
and the timing and nature of the effects are consistent with our 
interpretation. We also present suggestive evidence that democratic 
institutions that receive support from their citizens perform better in 
the face of negative shocks.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-03-06</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://academic.oup.com/restud/article/92/2/621/7675443">https://academic.oup.com/restud/article/92/2/621/7675443</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/11/2025, 11:47:00 AM</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>https://academic.oup.com/pages/standard-publication-reuse-rights</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>92</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>621-655</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Review of Economic Studies</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1093/restud/rdae051">10.1093/restud/rdae051</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0034-6527, 1467-937X</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_P5QNR6UZ">PDF					</li>
				</ul>
			</li>


			<li id="item_LU63C6ZT" class="item preprint">
			<h2>Rethinking Reflection in Pre-Training</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Essential AI</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Darsh J. Shah</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Peter Rushton</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Somanshu Singla</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mohit Parmar</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kurt Smith</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yash Vanjani</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ashish Vaswani</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Adarsh Chaluvaraju</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Andrew Hojel</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Andrew Ma</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Anil Thomas</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Anthony Polloreno</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ashish Tanwer</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Burhan Drak Sibai</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Divya S. Mansingka</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Divya Shivaprasad</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ishaan Shah</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Karl Stratos</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Khoi Nguyen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Michael Callahan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Michael Pust</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mrinal Iyer</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Philip Monk</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Platon Mazarakis</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ritvik Kapila</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Saurabh Srivastava</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tim Romanski</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>A language model's ability to reflect on its own reasoning 
provides a key advantage for solving complex problems. While most recent
 research has focused on how this ability develops during reinforcement 
learning, we show that it actually begins to emerge much earlier - 
during the model's pre-training. To study this, we introduce deliberate 
errors into chains-of-thought and test whether the model can still 
arrive at the correct answer by recognizing and correcting these 
mistakes. By tracking performance across different stages of 
pre-training, we observe that this self-correcting ability appears early
 and improves steadily over time. For instance, an OLMo2-7B model 
pre-trained on 4 trillion tokens displays self-correction on our six 
self-reflection tasks.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-04-05</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2504.04022">http://arxiv.org/abs/2504.04022</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/9/2025, 11:11:36 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2504.04022 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2504.04022">10.48550/arXiv.2504.04022</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2504.04022</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Artificial Intelligence</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_DP7VHD9M">Preprint PDF					</li>
					<li id="item_9KDUWSYR">Snapshot					</li>
				</ul>
			</li>


			<li id="item_VA56K7Z7" class="item preprint">
			<h2>LLM Social Simulations Are a Promising Research Method</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jacy Reese Anthis</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ryan Liu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sean M. Richardson</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Austin C. Kozlowski</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bernard Koch</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>James Evans</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Erik Brynjolfsson</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Michael Bernstein</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Accurate and verifiable large language model (LLM) simulations
 of human research subjects promise an accessible data source for 
understanding human behavior and training new AI systems. However, 
results to date have been limited, and few social scientists have 
adopted these methods. In this position paper, we argue that the promise
 of LLM social simulations can be achieved by addressing five tractable 
challenges. We ground our argument in a literature survey of empirical 
comparisons between LLMs and human research subjects, commentaries on 
the topic, and related work. We identify promising directions with 
prompting, fine-tuning, and complementary methods. We believe that LLM 
social simulations can already be used for exploratory research, such as
 pilot experiments for psychology, economics, sociology, and marketing. 
More widespread use may soon be possible with rapidly advancing LLM 
capabilities, and researchers should prioritize developing conceptual 
models and evaluations that can be iteratively deployed and refined at 
pace with ongoing AI advances.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-04-03</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2504.02234">http://arxiv.org/abs/2504.02234</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/9/2025, 11:29:43 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2504.02234 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2504.02234">10.48550/arXiv.2504.02234</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2504.02234</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computers and Society</li>
					<li>Computer Science - Human-Computer Interaction</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_B3M4D2KV">Preprint PDF					</li>
					<li id="item_TCU2SDZ4">Snapshot					</li>
				</ul>
			</li>


			<li id="item_J5HGL9IK" class="item webpage">
			<h2>Claude Sonnet 3.7 (often) knows when it’s in alignment evaluations</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Apollo Research</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We evaluate whether Claude Sonnet 3.7 and other frontier models know that they are being evaluated.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>4/7/2025</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-GB</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.apolloresearch.ai/blog/claude-sonnet-37-often-knows-when-its-in-alignment-evaluations">https://www.apolloresearch.ai/blog/claude-sonnet-37-often-knows-when-its-in-alignment-evaluations</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/25/2025, 9:30:24 PM</td>
					</tr>
					<tr>
					<th>Website Title</th>
						<td>Apollo Research</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/2/2025, 2:13:04 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>5/1/2025, 2:05:23 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_HKBGK2PU">Snapshot					</li>
				</ul>
			</li>


			<li id="item_4JP9YKBU" class="item webpage">
			<h2>https://www.goodfire.ai/blog/under-the-hood-of-a-reasoning-model</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>AUTHORS Dron Hazra † Max Loeffler † Murat Cubuktepe Levon 
Avagyan ‡ Liv Gorton Mark Bissell Owen Lewis Thomas McGrath Daniel 
Balsam * PUBLISHED Apr. 15</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>2025</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>https</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.goodfire.ai/blog/under-the-hood-of-a-reasoning-model">https://www.goodfire.ai/blog/under-the-hood-of-a-reasoning-model</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/17/2025, 11:35:50 AM</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/17/2025, 11:35:50 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/17/2025, 11:35:50 AM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_526LAIXN">Snapshot					</li>
				</ul>
			</li>


			<li id="item_U5RWDZMU" class="item preprint">
			<h2>Inference-Time Scaling for Complex Tasks: Where We Stand and What Lies Ahead</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Vidhisha Balachandran</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jingya Chen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lingjiao Chen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Shivam Garg</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Neel Joshi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yash Lara</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>John Langford</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Besmira Nushi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Vibhav Vineet</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yue Wu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Safoora Yousefi</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Inference-time scaling can enhance the reasoning capabilities 
of large language models (LLMs) on complex problems that benefit from 
step-by-step problem solving. Although lengthening generated scratchpads
 has proven effective for mathematical tasks, the broader impact of this
 approach on other tasks remains less clear. In this work, we 
investigate the benefits and limitations of scaling methods across nine 
state-of-the-art models and eight challenging tasks, including math and 
STEM reasoning, calendar planning, NP-hard problems, navigation, and 
spatial reasoning. We compare conventional models (e.g., GPT-4o) with 
models fine-tuned for inference-time scaling (e.g., o1) through 
evaluation protocols that involve repeated model calls, either 
independently or sequentially with feedback. These evaluations 
approximate lower and upper performance bounds and potential for future 
performance improvements for each model, whether through enhanced 
training or multi-model inference systems. Our extensive empirical 
analysis reveals that the advantages of inference-time scaling vary 
across tasks and diminish as problem complexity increases. In addition, 
simply using more tokens does not necessarily translate to higher 
accuracy in these challenging regimes. Results from multiple independent
 runs with conventional models using perfect verifiers show that, for 
some tasks, these models can achieve performance close to the average 
performance of today's most advanced reasoning models. However, for 
other tasks, a significant performance gap remains, even in very high 
scaling regimes. Encouragingly, all models demonstrate significant gains
 when inference is further scaled with perfect verifiers or strong 
feedback, suggesting ample potential for future improvements.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-03-31</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Inference-Time Scaling for Complex Tasks</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2504.00294">http://arxiv.org/abs/2504.00294</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/11/2025, 12:33:43 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2504.00294 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2504.00294">10.48550/arXiv.2504.00294</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2504.00294</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_BY6TZAEQ">Preprint PDF					</li>
					<li id="item_SKSYXN69">Snapshot					</li>
				</ul>
			</li>


			<li id="item_S4AJNNCA" class="item preprint">
			<h2>Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Krisztian Balog</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Donald Metzler</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zhen Qin</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large language models (LLMs) are increasingly integral to 
information retrieval (IR), powering ranking, evaluation, and 
AI-assisted content creation. This widespread adoption necessitates a 
critical examination of potential biases arising from the interplay 
between these LLM-based components. This paper synthesizes existing 
research and presents novel experiment designs that explore how 
LLM-based rankers and assistants influence LLM-based judges. We provide 
the first empirical evidence of LLM judges exhibiting significant bias 
towards LLM-based rankers. Furthermore, we observe limitations in LLM 
judges' ability to discern subtle system performance differences. 
Contrary to some previous findings, our preliminary study does not find 
evidence of bias against AI-generated content. These results highlight 
the need for a more holistic view of the LLM-driven information 
ecosystem. To this end, we offer initial guidelines and a research 
agenda to ensure the reliable use of LLMs in IR evaluation.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-03-24</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Rankers, Judges, and Assistants</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2503.19092">http://arxiv.org/abs/2503.19092</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/14/2025, 9:56:20 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2503.19092 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2503.19092">10.48550/arXiv.2503.19092</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2503.19092</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:56:33 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:56:35 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Information Retrieval</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_V5ZJNPCD">Preprint PDF					</li>
				</ul>
			</li>


			<li id="item_RLRBHNFI" class="item preprint">
			<h2>Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Krisztian Balog</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Donald Metzler</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zhen Qin</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large language models (LLMs) are increasingly integral to 
information retrieval (IR), powering ranking, evaluation, and 
AI-assisted content creation. This widespread adoption necessitates a 
critical examination of potential biases arising from the interplay 
between these LLM-based components. This paper synthesizes existing 
research and presents novel experiment designs that explore how 
LLM-based rankers and assistants influence LLM-based judges. We provide 
the first empirical evidence of LLM judges exhibiting significant bias 
towards LLM-based rankers. Furthermore, we observe limitations in LLM 
judges' ability to discern subtle system performance differences. 
Contrary to some previous findings, our preliminary study does not find 
evidence of bias against AI-generated content. These results highlight 
the need for a more holistic view of the LLM-driven information 
ecosystem. To this end, we offer initial guidelines and a research 
agenda to ensure the reliable use of LLMs in IR evaluation.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-03-24</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Rankers, Judges, and Assistants</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2503.19092">http://arxiv.org/abs/2503.19092</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/14/2025, 9:56:20 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2503.19092 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2503.19092">10.48550/arXiv.2503.19092</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2503.19092</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:56:33 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:56:35 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Information Retrieval</li>
				</ul>
			</li>


			<li id="item_DU7KAS3N" class="item journalArticle">
			<h2>Creating a Resilient AI Ecosystem with NDIF and NNsight</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David Bau</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Zotero</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_9PHQC8HN">PDF					</li>
				</ul>
			</li>


			<li id="item_44IRRFNW" class="item journalArticle">
			<h2>AI Dominance Requires Interpretability and Standards for Transparency and Security</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David Bau</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tom McGrath</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sarah Schwettmann</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dylan Hadfield-Menell</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Zotero</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_5YBVA7Q9">PDF					</li>
				</ul>
			</li>


			<li id="item_3XBCFU48" class="item preprint">
			<h2>Ctrl-Z: Controlling AI Agents via Resampling</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Aryan Bhatt</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Cody Rushing</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Adam Kaufman</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tyler Tracy</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Vasil Georgiev</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David Matolcsi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Akbir Khan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Buck Shlegeris</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Control evaluations measure whether monitoring and security 
protocols for AI systems prevent intentionally subversive AI models from
 causing harm. Our work presents the first control evaluation performed 
in an agent environment. We construct BashBench, a dataset of 257 
challenging multi-step system administration tasks, and evaluate whether
 various safety measures can prevent an adversarially constructed AI 
agent from covertly downloading and executing malicious code in this 
environment. This multi-step setting introduces new attack and defense 
dynamics, which we investigate in order to design novel control 
protocols that prevent safety failures without hindering the ability of 
non-malicious agents to perform useful work. We introduce a class of 
control protocols called resample protocols that dynamically take 
additional samples of certain actions. We find these protocols 
significantly improve on existing techniques by selectively blocking the
 AI agent from executing suspicious code and incriminating the agent by 
generating additional examples of dangerous behavior. We measure the 
tradeoff between attack prevention and usefulness; our best protocol 
combines resampling with analysis of previous steps, reducing the 
success rate of attacks from 58% to 7% at a 5% cost to the performance 
of a non-malicious agent.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-04-14</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Ctrl-Z</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2504.10374">http://arxiv.org/abs/2504.10374</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/17/2025, 11:40:05 AM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2504.10374 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2504.10374">10.48550/arXiv.2504.10374</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2504.10374</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/17/2025, 11:40:05 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/17/2025, 11:40:05 AM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_GQCR6Z8X">
<p class="plaintext">Comment: bashcontrol.com</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_9Q9IZ7FV">Preprint PDF					</li>
					<li id="item_9XIMK5RK">Snapshot					</li>
				</ul>
			</li>


			<li id="item_MTMTJSJM" class="item journalArticle">
			<h2>Reasoning Models Don’t Always Say What They Think</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yanda Chen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Joe Benton</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ansh Radhakrishnan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jonathan Uesato</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Carson Denison</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>John Schulman</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Arushi Somani</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Peter Hase</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Misha Wagner</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Fabien Roger</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Vlad Mikulik</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sam Bowman</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jan Leike</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jared Kaplan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ethan Perez</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Chain-of-thought (CoT) offers a potential boon for AI safety 
as it allows monitoring a model’s CoT to try to understand its 
intentions and reasoning processes. However, the effectiveness of such 
monitoring hinges on CoTs faithfully representing models’ actual 
reasoning processes. We evaluate CoT faithfulness of state-of-the-art 
reasoning models across 6 reasoning hints presented in the prompts and 
find: (1) for most settings and models tested, CoTs reveal their usage 
of hints in at least 1% of examples where they use the hint, but the 
reveal rate is often below 20%, (2) outcome-based reinforcement learning
 initially improves faithfulness but plateaus without saturating, and 
(3) when reinforcement learning increases how frequently hints are used 
(reward hacking), the propensity to verbalize them does not increase, 
even without training against a CoT monitor. These results suggest that 
CoT monitoring is a promising way of noticing undesired behaviors during
 training and evaluations, but that it is not sufficient to rule them 
out. They also suggest that in settings like ours where CoT reasoning is
 not necessary, test-time monitoring of CoTs is unlikely to reliably 
catch rare and catastrophic unexpected behaviors.</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Zotero</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_FHPIT6TR">PDF					</li>
				</ul>
			</li>


			<li id="item_26AX8GIA" class="item preprint">
			<h2>JudgeLRM: Large Reasoning Models as a Judge</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nuo Chen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zhiyuan Hu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Qingyun Zou</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jiaying Wu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Qian Wang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bryan Hooi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bingsheng He</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The rise of Large Language Models (LLMs) as evaluators offers a
 scalable alternative to human annotation, yet existing Supervised 
Fine-Tuning (SFT) for judges approaches often fall short in domains 
requiring complex reasoning. In this work, we investigate whether LLM 
judges truly benefit from enhanced reasoning capabilities. Through a 
detailed analysis of reasoning requirements across evaluation tasks, we 
reveal a negative correlation between SFT performance gains and the 
proportion of reasoning-demanding samples - highlighting the limitations
 of SFT in such scenarios. To address this, we introduce JudgeLRM, a 
family of judgment-oriented LLMs trained using reinforcement learning 
(RL) with judge-wise, outcome-driven rewards. JudgeLRM models 
consistently outperform both SFT-tuned and state-of-the-art reasoning 
models. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B 
outperforms DeepSeek-R1 by 2.79% in F1 score, particularly excelling in 
judge tasks requiring deep reasoning.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-03-31</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>JudgeLRM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2504.00050">http://arxiv.org/abs/2504.00050</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/14/2025, 9:59:44 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2504.00050 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2504.00050">10.48550/arXiv.2504.00050</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2504.00050</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:59:44 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:59:44 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Artificial Intelligence</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_QQY8A8F9">
<p class="plaintext">Comment: preprint</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_IJTQAX37">Preprint PDF					</li>
					<li id="item_8HTPY65X">Snapshot					</li>
				</ul>
			</li>


			<li id="item_E2N8TN4A" class="item preprint">
			<h2>Reinforcement Learning for Reasoning in Small LLMs: What Works and What Doesn't</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Quy-Anh Dang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Chris Ngo</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Enhancing the reasoning capabilities of large language models 
(LLMs) typically relies on massive computational resources and extensive
 datasets, limiting accessibility for resource-constrained settings. Our
 study investigates the potential of reinforcement learning (RL) to 
improve reasoning in small LLMs, focusing on a 1.5-billion-parameter 
model, DeepSeek-R1-Distill-Qwen-1.5B, under strict constraints: training
 on 4 NVIDIA A40 GPUs (48 GB VRAM each) within 24 hours. Adapting the 
Group Relative Policy Optimization (GRPO) algorithm and curating a 
compact, high-quality mathematical reasoning dataset, we conducted three
 experiments to explore model behavior and performance. Our results 
demonstrate rapid reasoning gains - e.g., AMC23 accuracy rising from 63%
 to 80% and AIME24 reaching 46.7%, surpassing o1-preview - using only 
7,000 samples and a $42 training cost, compared to thousands of dollars 
for baseline models. However, challenges such as optimization 
instability and length constraints emerged with prolonged training. 
These findings highlight the efficacy of RL-based fine-tuning for small 
LLMs, offering a cost-effective alternative to large-scale approaches. 
We release our code and datasets as open-source resources, providing 
insights into trade-offs and laying a foundation for scalable, 
reasoning-capable LLMs in resource-limited environments. All are 
available at https://github.com/knoveleng/open-rs.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-03-20</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Reinforcement Learning for Reasoning in Small LLMs</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2503.16219">http://arxiv.org/abs/2503.16219</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/11/2025, 1:09:29 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2503.16219 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2503.16219">10.48550/arXiv.2503.16219</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2503.16219</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_L6UZVT8Z">Preprint PDF					</li>
					<li id="item_SPNLZYCS">Snapshot					</li>
				</ul>
			</li>


			<li id="item_52ZWP8E2" class="item journalArticle">
			<h2>AI-Enabled Coups: How a Small Group Could Use AI to Seize Power</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tom Davidson</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lukas Finnveden</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rose Hadshar</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This report assesses the risk that a small group—or even just 
one person—could use advanced AI to stage a coup. An AI-enabled coup is 
most likely to be staged by leaders of frontier AI projects, heads of 
state, and military officials; and could occur even in established 
democracies.</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Zotero</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/17/2025, 11:42:37 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/17/2025, 11:42:37 AM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_5KHIQIJS">PDF					</li>
				</ul>
			</li>


			<li id="item_FISQWQRU" class="item journalArticle">
			<h2>HOW AI AND HUMAN BEHAVIORS SHAPE PSYCHOSOCIAL EFFECTS OF CHATBOT USE: A LONGITUDINAL RANDOMIZED CONTROLLED STUDY</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Cathy Mengying Fang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Auren R Liu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Valdemar Danry</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Eunhae Lee</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Samantha W T Chan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Pat Pataranutaporn</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Pattie Maes</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jason Phang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Michael Lampe</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lama Ahmad</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sandhini Agarwal</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>AI chatbots, especially those with voice capabilities, have 
become increasingly human-like, with more users seeking emotional 
support and companionship from them. Concerns are rising about how such 
interactions might impact users’ loneliness and socialization with real 
people. We conducted a four-week randomized, controlled, IRB-approved 
experiment (n=981, &gt;300K messages) to investigate how AI chatbot 
interaction modes (text, neutral voice, and engaging voice) and 
conversation types (open-ended, non-personal, and personal) influence 
psychosocial outcomes such as loneliness, social interaction with real 
people, emotional dependence on AI and problematic AI usage. Results 
showed that while voice-based chatbots initially appeared beneficial in 
mitigating loneliness and dependence compared with text-based chatbots, 
these advantages diminished at high usage levels, especially with a 
neutral-voice chatbot. Conversation type also shaped outcomes: personal 
topics slightly increased loneliness but tended to lower emotional 
dependence compared with open-ended conversations, whereas non-personal 
topics were associated with greater dependence among heavy users. 
Overall, higher daily usage—across all modalities and conversation 
types—correlated with higher loneliness, dependence, and problematic 
use, and lower socialization. Exploratory analyses revealed that those 
with stronger emotional attachment tendencies and higher trust in the AI
 chatbot tended to experience greater loneliness and emotional 
dependence, respectively. These findings underscore the complex 
interplay between chatbot design choices (e.g., voice expressiveness) 
and user behaviors (e.g., conversation content, usage frequency). We 
highlight the need for further research on whether chatbots’ ability to 
manage emotional content without fostering dependence or replacing human
 relationships benefits overall well-being.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Zotero</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/2/2025, 2:13:04 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/2/2025, 2:13:04 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_62Y4T8HY">PDF					</li>
				</ul>
			</li>


			<li id="item_2KPHJM2Q" class="item preprint">
			<h2>Concise Reasoning via Reinforcement Learning</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mehdi Fatemi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Banafsheh Rafiee</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mingjie Tang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kartik Talamadupula</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Despite significant advancements in large language models 
(LLMs), a major drawback of reasoning models is their enormous token 
usage, which increases computational cost, resource requirements, and 
response time. In this work, we revisit the core principles of 
reinforcement learning (RL) and, through mathematical analysis, 
demonstrate that the tendency to generate lengthy responses arises 
inherently from RL-based optimization during training. This finding 
questions the prevailing assumption that longer responses inherently 
improve reasoning accuracy. Instead, we uncover a natural correlation 
between conciseness and accuracy that has been largely overlooked. 
Moreover, we show that introducing a secondary phase of RL 
post-training, using a small set of problems and limited resources, can 
significantly reduce a model's chain of thought while maintaining or 
even enhancing accuracy. Finally, we validate our conclusions through 
extensive experimental results.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-04-07</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2504.05185">http://arxiv.org/abs/2504.05185</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/14/2025, 10:01:11 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2504.05185 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2504.05185">10.48550/arXiv.2504.05185</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2504.05185</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 10:01:11 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 10:01:11 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_ZW4J9EVA">Preprint PDF					</li>
					<li id="item_UMGMYFZ4">Snapshot					</li>
				</ul>
			</li>


			<li id="item_5V9Z8IGF" class="item preprint">
			<h2>Political Neutrality in AI is Impossible- But Here is How to Approximate it</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jillian Fisher</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ruth E. Appel</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Chan Young Park</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yujin Potter</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Liwei Jiang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Taylor Sorensen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Shangbin Feng</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yulia Tsvetkov</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Margaret E. Roberts</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jennifer Pan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dawn Song</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yejin Choi</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>AI systems often exhibit political bias, influencing users' 
opinions and decision-making. While political neutrality-defined as the 
absence of bias-is often seen as an ideal solution for fairness and 
safety, this position paper argues that true political neutrality is 
neither feasible nor universally desirable due to its subjective nature 
and the biases inherent in AI training data, algorithms, and user 
interactions. However, inspired by Joseph Raz's philosophical insight 
that "neutrality [...] can be a matter of degree" (Raz, 1986), we argue 
that striving for some neutrality remains essential for promoting 
balanced AI interactions and mitigating user manipulation. Therefore, we
 use the term "approximation" of political neutrality to shift the focus
 from unattainable absolutes to achievable, practical proxies. We 
propose eight techniques for approximating neutrality across three 
levels of conceptualizing AI, examining their trade-offs and 
implementation strategies. In addition, we explore two concrete 
applications of these approximations to illustrate their practicality. 
Finally, we assess our framework on current large language models (LLMs)
 at the output level, providing a demonstration of how it can be 
evaluated. This work seeks to advance nuanced discussions of political 
neutrality in AI and promote the development of responsible, aligned 
language models.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-02-18</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2503.05728">http://arxiv.org/abs/2503.05728</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/11/2025, 10:31:20 AM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2503.05728 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2503.05728">10.48550/arXiv.2503.05728</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2503.05728</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_BGET3T3E">
<p class="plaintext">Comment: Code: https://github.com/jfisher52/Approximation_Political_Neutrality</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_99UKKT7B">Preprint PDF					</li>
					<li id="item_2JUN5TUI">Snapshot					</li>
				</ul>
			</li>


			<li id="item_W7FUVUS3" class="item preprint">
			<h2>Is a Good Foundation Necessary for Efficient Reinforcement Learning? The Computational Role of the Base Model in Exploration</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dylan J. Foster</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zakaria Mhammedi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dhruv Rohatgi</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Language model alignment (or, reinforcement learning) 
techniques that leverage active exploration -- deliberately encouraging 
the model to produce diverse, informative responses -- offer the promise
 of super-human capabilities. However, current understanding of 
algorithm design primitives for computationally efficient exploration 
with language models is limited. To better understand how to leverage 
access to powerful pre-trained generative models to improve the 
efficiency of exploration, we introduce a new computational framework 
for RL with language models, in which the learner interacts with the 
model through a sampling oracle. Focusing on the linear softmax model 
parameterization, we provide new results that reveal the 
computational-statistical tradeoffs of efficient exploration: 1. 
Necessity of coverage: Coverage refers to the extent to which the 
pre-trained model covers near-optimal responses -- a form of hidden 
knowledge. We show that coverage, while not necessary for data 
efficiency, lower bounds the runtime of any algorithm in our framework. 
2. Inference-time exploration: We introduce a new algorithm, 
SpannerSampling, which obtains optimal data efficiency and is 
computationally efficient whenever the pre-trained model enjoys 
sufficient coverage, matching our lower bound. SpannerSampling leverages
 inference-time computation with the pre-trained model to reduce the 
effective search space for exploration. 3. Insufficiency of 
training-time interventions: We contrast the result above by showing 
that training-time interventions that produce proper policies cannot 
achieve similar guarantees in polynomial time. 4. Computational benefits
 of multi-turn exploration: Finally, we show that under additional 
representational assumptions, one can achieve improved runtime 
(replacing sequence-level coverage with token-level coverage) through 
multi-turn exploration.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-03-13</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Is a Good Foundation Necessary for Efficient Reinforcement Learning?</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2503.07453">http://arxiv.org/abs/2503.07453</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/11/2025, 3:16:04 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2503.07453 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2503.07453">10.48550/arXiv.2503.07453</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2503.07453</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Machine Learning</li>
					<li>Mathematics - Statistics Theory</li>
					<li>Statistics - Statistics Theory</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_XY9N7HKD">
<p class="plaintext">Comment: V2: Improved number of prompts used by Algorithm 1</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_DLNHCJC8">Preprint PDF					</li>
					<li id="item_M3P6ZKG7">Snapshot					</li>
				</ul>
			</li>


			<li id="item_V7WBR9JC" class="item preprint">
			<h2>Neural Interactive Proofs</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lewis Hammond</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sam Adam-Day</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We consider the problem of how a trusted, but computationally 
bounded agent (a 'verifier') can learn to interact with one or more 
powerful but untrusted agents ('provers') in order to solve a given 
task. More specifically, we study the case in which agents are 
represented using neural networks and refer to solutions of this problem
 as neural interactive proofs. First we introduce a unifying framework 
based on prover-verifier games, which generalises previously proposed 
interaction protocols. We then describe several new protocols for 
generating neural interactive proofs, and provide a theoretical 
comparison of both new and existing approaches. Finally, we support this
 theory with experiments in two domains: a toy graph isomorphism problem
 that illustrates the key ideas, and a code validation task using large 
language models. In so doing, we aim to create a foundation for future 
work on neural interactive proofs and their application in building 
safer AI systems.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-03-17</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2412.08897">http://arxiv.org/abs/2412.08897</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/25/2025, 9:19:45 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2412.08897 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2412.08897">10.48550/arXiv.2412.08897</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2412.08897</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/2/2025, 2:13:04 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/2/2025, 2:13:04 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_3I6M78XX">
<p class="plaintext">Comment: ICLR'25 camera-ready version; 51 pages, 17 figures</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_T96UM6MF">Preprint PDF					</li>
					<li id="item_ZUAJJN3J">Snapshot					</li>
				</ul>
			</li>


			<li id="item_WZEIWFKG" class="item preprint">
			<h2>Safety Cases: A Scalable Approach to Frontier AI Safety</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Benjamin Hilton</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Marie Davidsen Buhl</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tomek Korbak</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Geoffrey Irving</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Safety cases - clear, assessable arguments for the safety of a
 system in a given context - are a widely-used technique across various 
industries for showing a decision-maker (e.g. boards, customers, third 
parties) that a system is safe. In this paper, we cover how and why 
frontier AI developers might also want to use safety cases. We then 
argue that writing and reviewing safety cases would substantially assist
 in the fulfilment of many of the Frontier AI Safety Commitments. 
Finally, we outline open research questions on the methodology, 
implementation, and technical details of safety cases.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-02-05</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Safety Cases</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2503.04744">http://arxiv.org/abs/2503.04744</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/25/2025, 9:34:29 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2503.04744 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2503.04744">10.48550/arXiv.2503.04744</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2503.04744</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/2/2025, 2:13:04 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/2/2025, 2:13:04 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_6I6SE6MU">
<p class="plaintext">Comment: 18 pages, 2 figures, 3 tables</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_PU59XAGT">Preprint PDF					</li>
					<li id="item_KMV4DVJW">Snapshot					</li>
				</ul>
			</li>


			<li id="item_3ZMNKGUF" class="item preprint">
			<h2>A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Andreas Hochlehnert</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hardik Bhatnagar</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Vishaal Udandarao</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Samuel Albanie</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ameya Prabhu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Matthias Bethge</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Reasoning has emerged as the next major frontier for language 
models (LMs), with rapid advances from both academic and industrial 
labs. However, this progress often outpaces methodological rigor, with 
many evaluations relying on benchmarking practices that lack 
transparency, robustness, or statistical grounding. In this work, we 
conduct a comprehensive empirical study and find that current 
mathematical reasoning benchmarks are highly sensitive to subtle 
implementation choices - including decoding parameters, random seeds, 
prompt formatting, and even hardware and software-framework 
configurations. Performance gains reported in recent studies frequently 
hinge on unclear comparisons or unreported sources of variance. To 
address these issues, we propose a standardized evaluation framework 
with clearly defined best practices and reporting standards. Using this 
framework, we reassess recent methods and find that reinforcement 
learning (RL) approaches yield only modest improvements - far below 
prior claims - and are prone to overfitting, especially on small-scale 
benchmarks like AIME24. In contrast, supervised finetuning (SFT) methods
 show consistently stronger generalization. To foster reproducibility, 
we release all code, prompts, and model outputs, for reasoning 
benchmarks, establishing more rigorous foundations for future work.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-04-09</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>A Sober Look at Progress in Language Model Reasoning</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2504.07086">http://arxiv.org/abs/2504.07086</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/13/2025, 6:01:29 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2504.07086 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2504.07086">10.48550/arXiv.2504.07086</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2504.07086</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_S8UYXYJA">
<p class="plaintext">Comment: Technical Report</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_8FEGHAEW">Preprint PDF					</li>
					<li id="item_ZGKPV9ZM">Snapshot					</li>
				</ul>
			</li>


			<li id="item_GGHBWY8T" class="item preprint">
			<h2>Can A Society of Generative Agents Simulate Human Behavior and Inform Public Health Policy? A Case Study on Vaccine Hesitancy</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Abe Bohan Hou</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hongru Du</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yichen Wang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jingyu Zhang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zixiao Wang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Paul Pu Liang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Daniel Khashabi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lauren Gardner</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tianxing He</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Can we simulate a sandbox society with generative agents to 
model human behavior, thereby reducing the over-reliance on real human 
trials for assessing public policies? In this work, we investigate the 
feasibility of simulating health-related decision-making, using vaccine 
hesitancy, defined as the delay in acceptance or refusal of vaccines 
despite the availability of vaccination services (MacDonald, 2015), as a
 case study. To this end, we introduce the VacSim framework with 100 
generative agents powered by Large Language Models (LLMs). VacSim 
simulates vaccine policy outcomes with the following steps: 1) 
instantiate a population of agents with demographics based on census 
data; 2) connect the agents via a social network and model vaccine 
attitudes as a function of social dynamics and disease-related 
information; 3) design and evaluate various public health interventions 
aimed at mitigating vaccine hesitancy. To align with real-world results,
 we also introduce simulation warmup and attitude modulation to adjust 
agents' attitudes. We propose a series of evaluations to assess the 
reliability of various LLM simulations. Experiments indicate that models
 like Llama and Qwen can simulate aspects of human behavior but also 
highlight real-world alignment challenges, such as inconsistent 
responses with demographic profiles. This early exploration of 
LLM-driven simulations is not meant to serve as definitive policy 
guidance; instead, it serves as a call for action to examine social 
simulation for policy development.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-03-16</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Can A Society of Generative Agents Simulate Human Behavior and Inform Public Health Policy?</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2503.09639">http://arxiv.org/abs/2503.09639</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/25/2025, 9:35:41 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2503.09639 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2503.09639">10.48550/arXiv.2503.09639</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2503.09639</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/2/2025, 2:13:04 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/2/2025, 2:13:04 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computers and Society</li>
					<li>Computer Science - Human-Computer Interaction</li>
					<li>Computer Science - Multiagent Systems</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_E7G2M9AU">Preprint PDF					</li>
					<li id="item_5Q7MTCZ3">Snapshot					</li>
				</ul>
			</li>


			<li id="item_I5B42LC3" class="item preprint">
			<h2>Large Language Models Are Better Logical Fallacy Reasoners with Counterargument, Explanation, and Goal-Aware Prompt Formulation</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jiwon Jeong</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hyeju Jang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hogun Park</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The advancement of Large Language Models (LLMs) has greatly 
improved our ability to process complex language. However, accurately 
detecting logical fallacies remains a significant challenge. This study 
presents a novel and effective prompt formulation approach for logical 
fallacy detection, applicable in both supervised (fine-tuned) and 
unsupervised (zero-shot) settings. Our method enriches input text 
incorporating implicit contextual information -- counterarguments, 
explanations, and goals -- which we query for validity within the 
context of the argument. We then rank these queries based on confidence 
scores to inform classification. We evaluate our approach across 
multiple datasets from 5 domains, covering 29 distinct fallacy types, 
using models from the GPT and LLaMA series. The results show substantial
 improvements over state-of-the-art models, with F1 score increases of 
up to 0.60 in zero-shot settings and up to 0.45 in fine-tuned models. 
Extensive analyses further illustrate why and how our method excels.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-03-30</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2503.23363">http://arxiv.org/abs/2503.23363</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/14/2025, 10:00:28 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2503.23363 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2503.23363">10.48550/arXiv.2503.23363</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2503.23363</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 10:00:28 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 10:00:28 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_7H9FFPTE">
<p class="plaintext">Comment: Accepted to NAACL 2025 Findings</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_DTGWU2AQ">Preprint PDF					</li>
					<li id="item_7JJQBH3Q">Snapshot					</li>
				</ul>
			</li>


			<li id="item_274KYEXK" class="item preprint">
			<h2>Language Model Alignment in Multilingual Trolley Problems</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zhijing Jin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Max Kleiman-Weiner</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Giorgio Piatti</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sydney Levine</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jiarui Liu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Fernando Gonzalez</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Francesco Ortu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>András Strausz</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mrinmaya Sachan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rada Mihalcea</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yejin Choi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bernhard Schölkopf</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We evaluate the moral alignment of large language models 
(LLMs) with human preferences in multilingual trolley problems. Building
 on the Moral Machine experiment, which captures over 40 million human 
judgments across 200+ countries, we develop a cross-lingual corpus of 
moral dilemma vignettes in over 100 languages called MultiTP. This 
dataset enables the assessment of LLMs' decision-making processes in 
diverse linguistic contexts. Our analysis explores the alignment of 19 
different LLMs with human judgments, capturing preferences across six 
moral dimensions: species, gender, fitness, status, age, and the number 
of lives involved. By correlating these preferences with the demographic
 distribution of language speakers and examining the consistency of LLM 
responses to various prompt paraphrasings, our findings provide insights
 into cross-lingual and ethical biases of LLMs and their intersection. 
We discover significant variance in alignment across languages, 
challenging the assumption of uniform moral reasoning in AI systems and 
highlighting the importance of incorporating diverse perspectives in AI 
ethics. The results underscore the need for further research on the 
integration of multilingual dimensions in responsible AI research to 
ensure fair and equitable AI interactions worldwide. Our code and data 
are at https://github.com/causalNLP/moralmachine</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-12-14</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2407.02273">http://arxiv.org/abs/2407.02273</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/14/2025, 10:06:43 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2407.02273 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2407.02273">10.48550/arXiv.2407.02273</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2407.02273</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 10:06:43 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 10:06:43 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_XW9S5JGR">
<p class="plaintext">Comment: Best Paper @ NeurIPS 2024 Workshop on Pluralistic Alignment</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_ANY2VITE">Preprint PDF					</li>
					<li id="item_ZHHXIAM8">Snapshot					</li>
				</ul>
			</li>


			<li id="item_NI2WNTPG" class="item preprint">
			<h2>Large Language Models Pass the Turing Test</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Cameron R. Jones</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Benjamin K. Bergen</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We evaluated 4 systems (ELIZA, GPT-4o, LLaMa-3.1-405B, and 
GPT-4.5) in two randomised, controlled, and pre-registered Turing tests 
on independent populations. Participants had 5 minute conversations 
simultaneously with another human participant and one of these systems 
before judging which conversational partner they thought was human. When
 prompted to adopt a humanlike persona, GPT-4.5 was judged to be the 
human 73% of the time: significantly more often than interrogators 
selected the real human participant. LLaMa-3.1, with the same prompt, 
was judged to be the human 56% of the time -- not significantly more or 
less often than the humans they were being compared to -- while baseline
 models (ELIZA and GPT-4o) achieved win rates significantly below chance
 (23% and 21% respectively). The results constitute the first empirical 
evidence that any artificial system passes a standard three-party Turing
 test. The results have implications for debates about what kind of 
intelligence is exhibited by Large Language Models (LLMs), and the 
social and economic impacts these systems are likely to have.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-03-31</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2503.23674">http://arxiv.org/abs/2503.23674</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/11/2025, 1:10:04 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2503.23674 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2503.23674">10.48550/arXiv.2503.23674</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2503.23674</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Human-Computer Interaction</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_R9TPJQPW">Preprint PDF					</li>
					<li id="item_9A6X3Y4V">Snapshot					</li>
				</ul>
			</li>


			<li id="item_3D3VJKQP" class="item preprint">
			<h2>Collaborating with AI Agents: Field Experiments on Teamwork, Productivity, and Performance</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Harang Ju</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sinan Aral</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>To uncover how AI agents change productivity, performance, and
 work processes, we introduce MindMeld: an experimentation platform 
enabling humans and AI agents to collaborate in integrative workspaces. 
In a large-scale marketing experiment on the platform, 2310 participants
 were randomly assigned to human-human and human-AI teams, with 
randomized AI personality traits. The teams exchanged 183,691 messages, 
and created 63,656 image edits, 1,960,095 ad copy edits, and 10,375 
AI-generated images while producing 11,138 ads for a large think tank. 
Analysis of fine-grained communication, collaboration, and workflow logs
 revealed that collaborating with AI agents increased communication by 
137% and allowed humans to focus 23% more on text and image content 
generation messaging and 20% less on direct text editing. Humans on 
Human-AI teams sent 23% fewer social messages, creating 60% greater 
productivity per worker and higher-quality ad copy. In contrast, 
human-human teams produced higher-quality images, suggesting that AI 
agents require fine-tuning for multimodal workflows. AI personality 
prompt randomization revealed that AI traits can complement human 
personalities to enhance collaboration. For example, conscientious 
humans paired with open AI agents improved image quality, while 
extroverted humans paired with conscientious AI agents reduced the 
quality of text, images, and clicks. In field tests of ad campaigns with
 ~5M impressions, ads with higher image quality produced by human 
collaborations and higher text quality produced by AI collaborations 
performed significantly better on click-through rate and cost per click 
metrics. Overall, ads created by human-AI teams performed similarly to 
those created by human-human teams. Together, these results suggest AI 
agents can improve teamwork and productivity, especially when tuned to 
complement human traits.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-03-23</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Collaborating with AI Agents</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2503.18238">http://arxiv.org/abs/2503.18238</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/26/2025, 2:58:26 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2503.18238 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2503.18238">10.48550/arXiv.2503.18238</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2503.18238</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/2/2025, 2:13:04 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/2/2025, 2:13:04 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_DP8FQ2KU">
<p class="plaintext">Comment: 56 pages, 8 figures</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_RL8NQGKS">Preprint PDF					</li>
					<li id="item_92WAU22A">Snapshot					</li>
				</ul>
			</li>


			<li id="item_KUDUVEJF" class="item preprint">
			<h2>Randomness, Not Representation: The Unreliability of Evaluating Cultural Alignment in LLMs</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ariba Khan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Stephen Casper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dylan Hadfield-Menell</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Research on the 'cultural alignment' of Large Language Models 
(LLMs) has emerged in response to growing interest in understanding 
representation across diverse stakeholders. Current approaches to 
evaluating cultural alignment through survey-based assessments that 
borrow from social science methodologies often overlook systematic 
robustness checks. Here, we identify and test three assumptions behind 
current survey-based evaluation methods: (1) Stability: that cultural 
alignment is a property of LLMs rather than an artifact of evaluation 
design, (2) Extrapolability: that alignment with one culture on a narrow
 set of issues predicts alignment with that culture on others, and (3) 
Steerability: that LLMs can be reliably prompted to represent specific 
cultural perspectives. Through experiments examining both explicit and 
implicit preferences of leading LLMs, we find a high level of 
instability across presentation formats, incoherence between evaluated 
versus held-out cultural dimensions, and erratic behavior under prompt 
steering. We show that these inconsistencies can cause the results of an
 evaluation to be very sensitive to minor variations in methodology. 
Finally, we demonstrate in a case study on evaluation design that narrow
 experiments and a selective assessment of evidence can be used to paint
 an incomplete picture of LLMs' cultural alignment properties. Overall, 
these results highlight significant limitations of current survey-based 
approaches to evaluating the cultural alignment of LLMs and highlight a 
need for systematic robustness checks and red-teaming for evaluation 
results. Data and code are available at 
https://huggingface.co/datasets/akhan02/cultural-dimension-cover-letters
 and https://github.com/ariba-k/llm-cultural-alignment-evaluation, 
respectively.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-04-08</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Randomness, Not Representation</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2503.08688">http://arxiv.org/abs/2503.08688</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/14/2025, 10:06:57 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2503.08688 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2503.08688">10.48550/arXiv.2503.08688</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2503.08688</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 10:06:57 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 10:06:57 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_CQFHRWGR">Preprint PDF					</li>
					<li id="item_NTIGBCCM">Snapshot					</li>
				</ul>
			</li>


			<li id="item_F73YXTQ4" class="item preprint">
			<h2>How to evaluate control measures for LLM agents? A trajectory from today to superintelligence</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tomek Korbak</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mikita Balesni</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Buck Shlegeris</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Geoffrey Irving</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>As LLM agents grow more capable of causing harm autonomously, 
AI developers will rely on increasingly sophisticated control measures 
to prevent possibly misaligned agents from causing harm. AI developers 
could demonstrate that their control measures are sufficient by running 
control evaluations: testing exercises in which a red team produces 
agents that try to subvert control measures. To ensure control 
evaluations accurately capture misalignment risks, the affordances 
granted to this red team should be adapted to the capability profiles of
 the agents to be deployed under control measures. In this paper we 
propose a systematic framework for adapting affordances of red teams to 
advancing AI capabilities. Rather than assuming that agents will always 
execute the best attack strategies known to humans, we demonstrate how 
knowledge of an agents's actual capability profile can inform 
proportional control evaluations, resulting in more practical and 
cost-effective control measures. We illustrate our framework by 
considering a sequence of five fictional models (M1-M5) with 
progressively advanced capabilities, defining five distinct AI control 
levels (ACLs). For each ACL, we provide example rules for control 
evaluation, control measures, and safety cases that could be 
appropriate. Finally, we show why constructing a compelling AI control 
safety case for superintelligent LLM agents will require research 
breakthroughs, highlighting that we might eventually need alternative 
approaches to mitigating misalignment risk.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-04-07</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>How to evaluate control measures for LLM agents?</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2504.05259">http://arxiv.org/abs/2504.05259</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/14/2025, 10:04:50 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2504.05259 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2504.05259">10.48550/arXiv.2504.05259</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2504.05259</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 10:04:50 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 10:04:50 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Cryptography and Security</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_UU6FAWZR">Preprint PDF					</li>
					<li id="item_7YQPPP29">Snapshot					</li>
				</ul>
			</li>


			<li id="item_7WMXGYSE" class="item preprint">
			<h2>Measuring AI Ability to Complete Long Tasks</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Thomas Kwa</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ben West</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Joel Becker</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Amy Deng</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Katharyn Garcia</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Max Hasin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sami Jawhar</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Megan Kinniment</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nate Rush</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sydney Von Arx</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ryan Bloom</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Thomas Broadley</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Haoxing Du</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Brian Goodrich</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nikola Jurkovic</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Luke Harold Miles</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Seraphina Nix</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tao Lin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Neev Parikh</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David Rein</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lucas Jun Koba Sato</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hjalmar Wijk</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Daniel M. Ziegler</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Elizabeth Barnes</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lawrence Chan</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Despite rapid progress on AI benchmarks, the real-world 
meaning of benchmark performance remains unclear. To quantify the 
capabilities of AI systems in terms of human capabilities, we propose a 
new metric: 50%-task-completion time horizon. This is the time humans 
typically take to complete tasks that AI models can complete with 50% 
success rate. We first timed humans with relevant domain expertise on a 
combination of RE-Bench, HCAST, and 66 novel shorter tasks. On these 
tasks, current frontier AI models such as Claude 3.7 Sonnet have a 50% 
time horizon of around 50 minutes. Furthermore, frontier AI time horizon
 has been doubling approximately every seven months since 2019, though 
the trend may have accelerated in 2024. The increase in AI models' time 
horizons seems to be primarily driven by greater reliability and ability
 to adapt to mistakes, combined with better logical reasoning and tool 
use capabilities. We discuss the limitations of our results -- including
 their degree of external validity -- and the implications of increased 
autonomy for dangerous capabilities. If these results generalize to 
real-world software tasks, extrapolation of this trend predicts that 
within 5 years, AI systems will be capable of automating many software 
tasks that currently take humans a month.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-03-18</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2503.14499">http://arxiv.org/abs/2503.14499</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/25/2025, 6:00:08 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2503.14499 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2503.14499">10.48550/arXiv.2503.14499</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2503.14499</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/2/2025, 2:13:04 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/2/2025, 2:13:04 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_ISABVLGA">Preprint PDF					</li>
					<li id="item_L3CF8CLT">Snapshot					</li>
				</ul>
			</li>


			<li id="item_G5TFNN8A" class="item preprint">
			<h2>Actions Speak Louder than Words: Agent Decisions Reveal Implicit Biases in Language Models</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yuxuan Li</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hirokazu Shirado</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sauvik Das</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>While advances in fairness and alignment have helped mitigate 
overt biases exhibited by large language models (LLMs) when explicitly 
prompted, we hypothesize that these models may still exhibit implicit 
biases when simulating human behavior. To test this hypothesis, we 
propose a technique to systematically uncover such biases across a broad
 range of sociodemographic categories by assessing decision-making 
disparities among agents with LLM-generated, 
sociodemographically-informed personas. Using our technique, we tested 
six LLMs across three sociodemographic groups and four decision-making 
scenarios. Our results show that state-of-the-art LLMs exhibit 
significant sociodemographic disparities in nearly all simulations, with
 more advanced models exhibiting greater implicit biases despite 
reducing explicit biases. Furthermore, when comparing our findings to 
real-world disparities reported in empirical studies, we find that the 
biases we uncovered are directionally aligned but markedly amplified. 
This directional alignment highlights the utility of our technique in 
uncovering systematic biases in LLMs rather than random variations; 
moreover, the presence and amplification of implicit biases emphasizes 
the need for novel strategies to address these biases.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-01-29</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Actions Speak Louder than Words</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2501.17420">http://arxiv.org/abs/2501.17420</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/14/2025, 10:00:42 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2501.17420 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2501.17420">10.48550/arXiv.2501.17420</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2501.17420</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 10:00:42 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 10:00:42 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Human-Computer Interaction</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_X2TV72NG">Preprint PDF					</li>
					<li id="item_45XP7RSM">Snapshot					</li>
				</ul>
			</li>


			<li id="item_ICZTCD77" class="item preprint">
			<h2>Inference-Time Scaling for Generalist Reward Modeling</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zijun Liu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Peiyi Wang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Runxin Xu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Shirong Ma</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Chong Ruan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Peng Li</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yang Liu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yu Wu</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Reinforcement learning (RL) has been widely adopted in 
post-training for large language models (LLMs) at scale. Recently, the 
incentivization of reasoning capabilities in LLMs from RL indicates that
 $\textit{proper learning methods could enable effective inference-time 
scalability}$. A key challenge of RL is to obtain accurate reward 
signals for LLMs in various domains beyond verifiable questions or 
artificial rules. In this work, we investigate how to improve reward 
modeling (RM) with more inference compute for general queries, i.e. the 
$\textbf{inference-time scalability of generalist RM}$, and further, how
 to improve the effectiveness of performance-compute scaling with proper
 learning methods. For the RM approach, we adopt pointwise generative 
reward modeling (GRM) to enable flexibility for different input types 
and potential for inference-time scaling. For the learning method, we 
propose Self-Principled Critique Tuning (SPCT) to foster scalable reward
 generation behaviors in GRMs through online RL, to generate principles 
adaptively and critiques accurately, resulting in 
$\textbf{DeepSeek-GRM}$ models. Furthermore, for effective 
inference-time scaling, we use parallel sampling to expand compute 
usage, and introduce a meta RM to guide voting process for better 
scaling performance. Empirically, we show that SPCT significantly 
improves the quality and scalability of GRMs, outperforming existing 
methods and models in various RM benchmarks without severe biases, and 
could achieve better performance compared to training-time scaling. 
DeepSeek-GRM still meets challenges in some tasks, which we believe can 
be addressed by future efforts in generalist reward systems. The models 
will be released and open-sourced.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-04-05</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2504.02495">http://arxiv.org/abs/2504.02495</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/10/2025, 11:19:19 AM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2504.02495 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2504.02495">10.48550/arXiv.2504.02495</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2504.02495</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_RMBM4R5R">
<p class="plaintext">Comment: Preprint, under review. 42 pages</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_P79CZCUH">Preprint PDF					</li>
					<li id="item_BCAH3PVI">Snapshot					</li>
				</ul>
			</li>


			<li id="item_ZB7IH2W6" class="item preprint">
			<h2>OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jiacheng Liu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Taylor Blanton</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yanai Elazar</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sewon Min</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>YenSung Chen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Arnavi Chheda-Kothary</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Huy Tran</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Byron Bischoff</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Eric Marsh</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Michael Schmitz</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Cassidy Trier</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Aaron Sarnat</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jenna James</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jon Borchardt</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bailey Kuehl</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Evie Cheng</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Karen Farley</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sruthi Sreeram</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Taira Anderson</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David Albright</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Carissa Schoenick</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Luca Soldaini</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dirk Groeneveld</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rock Yuren Pang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Pang Wei Koh</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Noah A. Smith</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sophie Lebrecht</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yejin Choi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hannaneh Hajishirzi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ali Farhadi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jesse Dodge</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We present OLMoTrace, the first system that traces the outputs
 of language models back to their full, multi-trillion-token training 
data in real time. OLMoTrace finds and shows verbatim matches between 
segments of language model output and documents in the training text 
corpora. Powered by an extended version of infini-gram (Liu et al., 
2024), our system returns tracing results within a few seconds. 
OLMoTrace can help users understand the behavior of language models 
through the lens of their training data. We showcase how it can be used 
to explore fact checking, hallucination, and the creativity of language 
models. OLMoTrace is publicly available and fully open-source.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-04-09</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>OLMoTrace</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2504.07096">http://arxiv.org/abs/2504.07096</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/17/2025, 11:51:42 AM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2504.07096 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2504.07096">10.48550/arXiv.2504.07096</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2504.07096</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/17/2025, 11:51:42 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/17/2025, 11:51:42 AM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_DY2Z59CB">
<p class="plaintext">Comment: Under submission at ACL 2025 demo track</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_WDEP2898">Preprint PDF					</li>
					<li id="item_VLMMCH56">Snapshot					</li>
				</ul>
			</li>


			<li id="item_I6M35R7H" class="item preprint">
			<h2>AgentRewardBench: Evaluating Automatic Evaluations of Web Agent Trajectories</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xing Han Lù</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Amirhossein Kazemnejad</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nicholas Meade</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Arkil Patel</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dongchan Shin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alejandra Zambrano</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Karolina Stańczak</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Peter Shaw</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Christopher J. Pal</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Siva Reddy</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Web agents enable users to perform tasks on web browsers 
through natural language interaction. Evaluating web agents trajectories
 is an important problem, since it helps us determine whether the agent 
successfully completed the tasks. Rule-based methods are widely used for
 this purpose, but they are challenging to extend to new tasks and may 
not always recognize successful trajectories. We may achieve higher 
accuracy through human evaluation, but the process would be 
substantially slower and more expensive. Automatic evaluations with LLMs
 may avoid the challenges of designing new rules and manually annotating
 trajectories, enabling faster and cost-effective evaluation. However, 
it is unclear how effective they are at evaluating web agents. To this 
end, we propose AgentRewardBench, the first benchmark to assess the 
effectiveness of LLM judges for evaluating web agents. AgentRewardBench 
contains 1302 trajectories across 5 benchmarks and 4 LLMs. Each 
trajectory in AgentRewardBench is reviewed by an expert, who answers 
questions pertaining to the success, side effects, and repetitiveness of
 the agent. Using our benchmark, we evaluate 12 LLM judges and find that
 no single LLM excels across all benchmarks. We also find that the 
rule-based evaluation used by common benchmarks tends to underreport the
 success rate of web agents, highlighting a key weakness of rule-based 
evaluation and the need to develop more flexible automatic evaluations. 
We release the benchmark at: https://agent-reward-bench.github.io</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-04-11</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>AgentRewardBench</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2504.08942">http://arxiv.org/abs/2504.08942</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/17/2025, 11:42:57 AM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2504.08942 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2504.08942">10.48550/arXiv.2504.08942</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2504.08942</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/17/2025, 11:42:57 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/17/2025, 11:42:57 AM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_S6I4NR8U">Preprint PDF					</li>
					<li id="item_ZCJUBAEG">Snapshot					</li>
				</ul>
			</li>


			<li id="item_SGHEI4K6" class="item preprint">
			<h2>Superalignment with Dynamic Human Values</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Florian Mai</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David Kaczér</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nicholas Kluge Corrêa</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lucie Flek</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Two core challenges of alignment are 1) scalable oversight and
 2) accounting for the dynamic nature of human values. While solutions 
like recursive reward modeling address 1), they do not simultaneously 
account for 2). We sketch a roadmap for a novel algorithmic framework 
that trains a superhuman reasoning model to decompose complex tasks into
 subtasks that are still amenable to human-level guidance. Our approach 
relies on what we call the part-to-complete generalization hypothesis, 
which states that the alignment of subtask solutions generalizes to the 
alignment of complete solutions. We advocate for the need to measure 
this generalization and propose ways to improve it in the future.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-03-17</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2503.13621">http://arxiv.org/abs/2503.13621</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/25/2025, 5:43:06 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2503.13621 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2503.13621">10.48550/arXiv.2503.13621</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2503.13621</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/2/2025, 2:13:04 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/2/2025, 2:13:04 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_8L7BGTWQ">
<p class="plaintext">Comment: Published at the ICLR 2025 Workshop on Bidirectional Human-AI Alignment (BiAlign)</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_MP838VRM">Preprint PDF					</li>
					<li id="item_A9TMZCDE">Snapshot					</li>
				</ul>
			</li>


			<li id="item_J7DVUVU7" class="item preprint">
			<h2>Subversion Strategy Eval: Can language models statelessly strategize to subvert control protocols?</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alex Mallen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Charlie Griffin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Misha Wagner</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alessandro Abate</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Buck Shlegeris</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>An AI control protocol is a plan for usefully deploying AI 
systems that aims to prevent an AI from intentionally causing some 
unacceptable outcome. This paper investigates how well AI systems can 
generate and act on their own strategies for subverting control 
protocols whilst operating statelessly (without shared memory between 
contexts). To do this, an AI system may need to reliably generate 
optimal plans in each context, take actions with well-calibrated 
probabilities, and coordinate plans with other instances of itself 
without communicating. We develop Subversion Strategy Eval, a suite of 
eight environments, covering a range of protocols and strategic 
capabilities, and six sets of affordances that help isolate individual 
capabilities. We implement the evaluation in Inspect-AI and release it 
open-source. We evaluate Claude 3.5 models, including helpful-only 
versions, as well as OpenAI reasoning models. None of the models 
demonstrate substantial capability in strategizing to subvert control 
protocols statelessly. However, providing models with additional 
affordances, such as the ability to share a plan between contexts, can 
substantially improve performance. We hope our evaluations can act as a 
leading indicator for when models are capable of subverting control 
protocols and also relax the worst-case assumption of perfect strategic 
ability in AI control evaluations.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-03-24</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Subversion Strategy Eval</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2412.12480">http://arxiv.org/abs/2412.12480</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/25/2025, 5:37:00 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2412.12480 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2412.12480">10.48550/arXiv.2412.12480</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2412.12480</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/2/2025, 2:13:04 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/2/2025, 2:13:04 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_XTCD3HQZ">Preprint PDF					</li>
					<li id="item_U7U6TGKU">Snapshot					</li>
				</ul>
			</li>


			<li id="item_SA64674G" class="item preprint">
			<h2>The Necessity of AI Audit Standards Boards</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David Manheim</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sammy Martin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mark Bailey</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mikhail Samin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ross Greutzmacher</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Auditing of AI systems is a promising way to understand and 
manage ethical problems and societal risks associated with contemporary 
AI systems, as well as some anticipated future risks. Efforts to develop
 standards for auditing Artificial Intelligence (AI) systems have 
therefore understandably gained momentum. However, we argue that 
creating auditing standards is not just insufficient, but actively 
harmful by proliferating unheeded and inconsistent standards, especially
 in light of the rapid evolution and ethical and safety challenges of 
AI. Instead, the paper proposes the establishment of an AI Audit 
Standards Board, responsible for developing and updating auditing 
methods and standards in line with the evolving nature of AI 
technologies. Such a body would ensure that auditing practices remain 
relevant, robust, and responsive to the rapid advancements in AI. The 
paper argues that such a governance structure would also be helpful for 
maintaining public trust in AI and for promoting a culture of safety and
 ethical responsibility within the AI industry. Throughout the paper, we
 draw parallels with other industries, including safety-critical 
industries like aviation and nuclear energy, as well as more prosaic 
ones such as financial accounting and pharmaceuticals. AI auditing 
should emulate those fields, and extend beyond technical assessments to 
include ethical considerations and stakeholder engagement, but we 
explain that this is not enough; emulating other fields' governance 
mechanisms for these processes, and for audit standards creation, is a 
necessity. We also emphasize the importance of auditing the entire 
development process of AI systems, not just the final products...</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-04-11</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2404.13060">http://arxiv.org/abs/2404.13060</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/11/2025, 1:09:42 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2404.13060 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2404.13060">10.48550/arXiv.2404.13060</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2404.13060</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_RV8UJEAA">Preprint PDF					</li>
					<li id="item_ZEA9IXZ5">Snapshot					</li>
				</ul>
			</li>


			<li id="item_RZP9AKD6" class="item journalArticle">
			<h2>Artificial Intelligence Index Report 2025</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nestor Maslej</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Zotero</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Artificial Intelligence</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_YWRSKXDX">PDF					</li>
				</ul>
			</li>


			<li id="item_SB7XAHUL" class="item webpage">
			<h2>Details about METR’s preliminary evaluation of OpenAI’s o3 and o4-mini</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>METR</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Resources for testing dangerous autonomous capabilities in frontier models</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-04-16T00:00:00+00:00</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://metr.github.io/autonomy-evals-guide/openai-o3-report/">https://metr.github.io/autonomy-evals-guide/openai-o3-report/</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/17/2025, 11:41:15 AM</td>
					</tr>
					<tr>
					<th>Website Title</th>
						<td>METR’s Autonomy Evaluation Resources</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/17/2025, 11:41:15 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/17/2025, 11:41:15 AM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_PS684LR3">Snapshot					</li>
				</ul>
			</li>


			<li id="item_AUKF4V5I" class="item preprint">
			<h2>ASI existential risk: reconsidering alignment as a goal</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Michael Nielson</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>April 14th, 2025</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Zotero</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 10:05:28 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 10:06:07 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_ZFDIZD3X">PDF					</li>
				</ul>
			</li>


			<li id="item_DH5KJLCZ" class="item preprint">
			<h2>The Geometry of Categorical and Hierarchical Concepts in Large Language Models</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kiho Park</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yo Joong Choe</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yibo Jiang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Victor Veitch</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The linear representation hypothesis is the informal idea that
 semantic concepts are encoded as linear directions in the 
representation spaces of large language models (LLMs). Previous work has
 shown how to make this notion precise for representing binary concepts 
that have natural contrasts (e.g., {male, female}) as directions in 
representation space. However, many natural concepts do not have natural
 contrasts (e.g., whether the output is about an animal). In this work, 
we show how to extend the formalization of the linear representation 
hypothesis to represent features (e.g., is_animal) as vectors. This 
allows us to immediately formalize the representation of categorical 
concepts as polytopes in the representation space. Further, we use the 
formalization to prove a relationship between the hierarchical structure
 of concepts and the geometry of their representations. We validate 
these theoretical results on the Gemma and LLaMA-3 large language 
models, estimating representations for 900+ hierarchically related 
concepts using data from WordNet.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-02-18</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2406.01506">http://arxiv.org/abs/2406.01506</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/11/2025, 1:09:50 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2406.01506 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2406.01506">10.48550/arXiv.2406.01506</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2406.01506</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Machine Learning</li>
					<li>Statistics - Machine Learning</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_W4UF9RE4">
<p class="plaintext">Comment: Accepted for an oral presentation at ICLR 2025. Best Paper Award at the ICML 2024 Workshop on Mechanistic Interpretability. Code is available at https://github.com/KihoPark/LLM_Categorical_Hierarchical_Representations</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_EGUUNC39">Preprint PDF					</li>
					<li id="item_MKQ2RNZV">Snapshot					</li>
				</ul>
			</li>


			<li id="item_RS8ERWVT" class="item journalArticle">
			<h2>Investigating Aﬀective Use and Emotional Well-being on ChatGPT</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jason Phang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Michael Lampe</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lama Ahmad</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sandhini Agarwal</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Cathy Mengying Fang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Auren R Liu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Valdemar Danry</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Eunhae Lee</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Samantha W T Chan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Pat Pataranutaporn</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Pattie Maes</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>As AI chatbots see increased adoption and integration into 
everyday life, questions have been raised about the potential impact of 
human-like or anthropomorphic AI on users. In this work, we investigate 
the extent to which interactions with ChatGPT (with a focus on Advanced 
Voice Mode) may impact users’ emotional well-being, behaviors and 
experiences through two parallel studies. To study the aﬀective use of 
AI chatbots, we perform large-scale automated analysis of ChatGPT 
platform usage in a privacy-preserving manner, analyzing over 4 million 
conversations for aﬀective cues and surveying over 4,000 users on their 
perceptions of ChatGPT. To investigate whether there is a relationship 
between model usage and emotional well-being, we conduct an 
Institutional Review Board (IRB)-approved randomized controlled trial 
(RCT) on close to 1,000 participants over 28 days, examining changes in 
their emotional well-being as they interact with ChatGPT under diﬀerent 
experimental settings. In both on-platform data analysis and the RCT, we
 observe that very high usage correlates with increased self-reported 
indicators of dependence. From our RCT, we ﬁnd that the impact of 
voice-based interactions on emotional well-being to be highly nuanced, 
and inﬂuenced by factors such as the user’s initial emotional state and 
total usage duration. Overall, our analysis reveals that a small number 
of users are responsible for a disproportionate share of the most 
aﬀective cues.</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Zotero</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/2/2025, 2:13:04 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/2/2025, 2:13:04 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_4VG4AFNB">PDF					</li>
				</ul>
			</li>


			<li id="item_FRBZPEDQ" class="item journalArticle">
			<h2>An Approach to Technical AGI Safety and Security</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rohin Shah</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alex Irpan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alexander Matt Turner</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Anna Wang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Arthur Conmy</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David Lindner</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jonah Brown-Cohen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lewis Ho</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Neel Nanda</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Raluca Ada Popa</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rishub Jain</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rory Greig</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Scott Emmons</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sebastian Farquhar</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sébastien Krier</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Senthooran Rajamanoharan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sophie Bridgers</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tobi Ijitoye</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tom Everitt</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Victoria Krakovna</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Vikrant Varma</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Vladimir Mikulik</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zachary Kenton</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dave Orr</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Shane Legg</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Noah Goodman</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Allan Dafoe</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Four Flynn</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Anca Dragan</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Zotero</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_W3U6UKS5">PDF					</li>
				</ul>
			</li>


			<li id="item_6PAK94E2" class="item preprint">
			<h2>Palatable Conceptions of Disembodied Being: Terra Incognita in the Space of Possible Minds</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Murray Shanahan</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Is it possible to articulate a conception of consciousness 
that is compatible with the exotic characteristics of contemporary, 
disembodied AI systems, and that can stand up to philosophical scrutiny?
 How would subjective time and selfhood show up for an entity that 
conformed to such a conception? Trying to answer these questions, even 
metaphorically, stretches the language of consciousness to breaking 
point. Ultimately, the attempt yields something like emptiness, in the 
Buddhist sense, and helps to undermine our dualistic inclinations 
towards subjectivity and selfhood.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-03-20</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Palatable Conceptions of Disembodied Being</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2503.16348">http://arxiv.org/abs/2503.16348</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/25/2025, 5:46:40 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2503.16348 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2503.16348">10.48550/arXiv.2503.16348</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2503.16348</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/2/2025, 2:13:04 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/2/2025, 2:13:04 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_JG4Q9JHG">Preprint PDF					</li>
					<li id="item_RW4CGRDZ">Snapshot					</li>
				</ul>
			</li>


			<li id="item_34TLSYQE" class="item preprint">
			<h2>Steering off Course: Reliability Challenges in Steering Language Models</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Patrick Queiroz Da Silva</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hari Sethuraman</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dheeraj Rajagopal</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hannaneh Hajishirzi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sachin Kumar</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Steering methods for language models (LMs) have gained 
traction as lightweight alternatives to fine-tuning, enabling targeted 
modifications to model activations. However, prior studies primarily 
report results on a few models, leaving critical gaps in understanding 
the robustness of these methods. In this work, we systematically examine
 three prominent steering methods -- DoLa, function vectors, and task 
vectors. In contrast to the original studies, which evaluated a handful 
of models, we test up to 36 models belonging to 14 families with sizes 
ranging from 1.5B to 70B parameters. Our experiments reveal substantial 
variability in the effectiveness of the steering approaches, with a 
large number of models showing no improvement and at times degradation 
in steering performance. Our analysis demonstrate fundamental flaws in 
the assumptions underlying these methods, challenging their reliability 
as scalable steering solutions.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-04-06</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Steering off Course</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2504.04635">http://arxiv.org/abs/2504.04635</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/9/2025, 11:09:20 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2504.04635 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2504.04635">10.48550/arXiv.2504.04635</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2504.04635</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_V7RZFXQ2">Preprint PDF					</li>
					<li id="item_KAT57G88">Snapshot					</li>
				</ul>
			</li>


			<li id="item_KECRVMYS" class="item journalArticle">
			<h2>Negative Results for SAEs On Downstream Tasks and Deprioritising SAE Research (GDM Mech Interp Team Progress Update #2)</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lewis Smith</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Senthooran Rajamanoharan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Arthur Conmy</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>CallumMcDougall</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tom Lieberum</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>János Kramár</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rohin Shah</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Neel Nanda</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>TL;DR

To validate whether SAEs were a worthwhile technique, we explored 
whether they were useful on the downstream task of OOD generalisation 
when detecting harmful intent in user prompts
Negative result: SAEs underperformed linear probes
Corollary: Linear probes are actually really good and cheap and perform 
great
As a result of this and parallel work, we are deprioritising fundamental
 SAE research for the moment and exploring other directions, though SAEs
 will remain a tool in our toolkit
We do not think that SAEs are useless or that no one should work on 
them, but we also do not think that SAEs will be a game-changer for 
interpretability, and speculate that the field is over-invested in them.
Training SAEs specialised for chat data closed about half the gap but 
was still worse than linear probes
We tried several ways to train chat SAEs, all did about as well. By 
default, we recommend taking an SAE on pretraining data and finetuning 
it on a bit of chat data
Other results:
We found SAEs fairly helpful for debugging low quality datasets 
(noticing spurious correlations)
We present a variant of JumpReLU with an alternative sparsity penalty to
 get rid of high-frequency latents
We argue that a standard auto-interp approach of computing the average 
interpretability of a uniformly sampled SAE latent can be misleading as 
it doesn’t penalise models which have high frequency, but not very 
interpretable, latents, and explore weighting the interpretability score
 by latent frequency.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-03-26</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>www.alignmentforum.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.alignmentforum.org/posts/4uXCAJNuPKtKBsi28/negative-results-for-saes-on-downstream-tasks">https://www.alignmentforum.org/posts/4uXCAJNuPKtKBsi28/negative-results-for-saes-on-downstream-tasks</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/11/2025, 12:27:49 PM</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_8QKQ828D">Snapshot					</li>
				</ul>
			</li>


			<li id="item_JIGA7C3Z" class="item preprint">
			<h2>Value Profiles for Encoding Human Variation</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Taylor Sorensen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Pushkar Mishra</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Roma Patel</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Michael Henry Tessler</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Michiel Bakker</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Georgina Evans</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Iason Gabriel</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Noah Goodman</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Verena Rieser</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Modelling human variation in rating tasks is crucial for 
enabling AI systems for personalization, pluralistic model alignment, 
and computational social science. We propose representing individuals 
using value profiles -- natural language descriptions of underlying 
values compressed from in-context demonstrations -- along with a 
steerable decoder model to estimate ratings conditioned on a value 
profile or other rater information. To measure the predictive 
information in rater representations, we introduce an 
information-theoretic methodology. We find that demonstrations contain 
the most information, followed by value profiles and then demographics. 
However, value profiles offer advantages in terms of scrutability, 
interpretability, and steerability due to their compressed natural 
language format. Value profiles effectively compress the useful 
information from demonstrations (&gt;70% information preservation). 
Furthermore, clustering value profiles to identify similarly behaving 
individuals better explains rater variation than the most predictive 
demographic groupings. Going beyond test set performance, we show that 
the decoder models interpretably change ratings according to semantic 
profile differences, are well-calibrated, and can help explain 
instance-level disagreement by simulating an annotator population. These
 results demonstrate that value profiles offer novel, predictive ways to
 describe individual variation beyond demographics or group information.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-03-19</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2503.15484">http://arxiv.org/abs/2503.15484</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/25/2025, 5:59:26 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2503.15484 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2503.15484">10.48550/arXiv.2503.15484</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2503.15484</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/2/2025, 2:13:04 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/2/2025, 2:13:04 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Machine Learning</li>
					<li>Computer Science - Human-Computer Interaction</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_2ECIYQWR">Preprint PDF					</li>
					<li id="item_MKH64DPW">Snapshot					</li>
				</ul>
			</li>


			<li id="item_IQZBJDDE" class="item preprint">
			<h2>PaperBench: Evaluating AI's Ability to Replicate AI Research</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Giulio Starace</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Oliver Jaffe</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dane Sherburn</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>James Aung</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jun Shern Chan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Leon Maksin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rachel Dias</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Evan Mays</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Benjamin Kinsella</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Wyatt Thompson</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Johannes Heidecke</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Amelia Glaese</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tejal Patwardhan</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We introduce PaperBench, a benchmark evaluating the ability of
 AI agents to replicate state-of-the-art AI research. Agents must 
replicate 20 ICML 2024 Spotlight and Oral papers from scratch, including
 understanding paper contributions, developing a codebase, and 
successfully executing experiments. For objective evaluation, we develop
 rubrics that hierarchically decompose each replication task into 
smaller sub-tasks with clear grading criteria. In total, PaperBench 
contains 8,316 individually gradable tasks. Rubrics are co-developed 
with the author(s) of each ICML paper for accuracy and realism. To 
enable scalable evaluation, we also develop an LLM-based judge to 
automatically grade replication attempts against rubrics, and assess our
 judge's performance by creating a separate benchmark for judges. We 
evaluate several frontier models on PaperBench, finding that the 
best-performing tested agent, Claude 3.5 Sonnet (New) with open-source 
scaffolding, achieves an average replication score of 21.0%. Finally, we
 recruit top ML PhDs to attempt a subset of PaperBench, finding that 
models do not yet outperform the human baseline. We open-source our code
 (https://github.com/openai/preparedness) to facilitate future research 
in understanding the AI engineering capabilities of AI agents.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-04-07</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>PaperBench</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2504.01848">http://arxiv.org/abs/2504.01848</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/11/2025, 12:29:38 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2504.01848 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2504.01848">10.48550/arXiv.2504.01848</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2504.01848</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Artificial Intelligence</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_LCESE6MJ">
<p class="plaintext">Comment: 30 pages, 14 figures</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_YI8M5CZZ">Preprint PDF					</li>
					<li id="item_4C38NCT9">Snapshot					</li>
				</ul>
			</li>


			<li id="item_SWNV92DS" class="item preprint">
			<h2>Crossing the Reward Bridge: Expanding RL with Verifiable Rewards Across Diverse Domains</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yi Su</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dian Yu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Linfeng Song</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Juntao Li</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Haitao Mi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zhaopeng Tu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Min Zhang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dong Yu</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Reinforcement learning with verifiable rewards (RLVR) has 
demonstrated significant success in enhancing mathematical reasoning and
 coding performance of large language models (LLMs), especially when 
structured reference answers are accessible for verification. However, 
its extension to broader, less structured domains remains unexplored. In
 this work, we investigate the effectiveness and scalability of RLVR 
across diverse real-world domains including medicine, chemistry, 
psychology, economics, and education, where structured reference answers
 are typically unavailable. We reveal that binary verification judgments
 on broad-domain tasks exhibit high consistency across various LLMs 
provided expert-written reference answers exist. Motivated by this 
finding, we utilize a generative scoring technique that yields soft, 
model-based reward signals to overcome limitations posed by binary 
verifications, especially in free-form, unstructured answer scenarios. 
We further demonstrate the feasibility of training cross-domain 
generative reward models using relatively small (7B) LLMs without the 
need for extensive domain-specific annotation. Through comprehensive 
experiments, our RLVR framework establishes clear performance gains, 
significantly outperforming state-of-the-art open-source aligned models 
such as Qwen2.5-72B and DeepSeek-R1-Distill-Qwen-32B across domains in 
free-form settings. Our approach notably enhances the robustness, 
flexibility, and scalability of RLVR, representing a substantial step 
towards practical reinforcement learning applications in complex, 
noisy-label scenarios.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-04-01</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Crossing the Reward Bridge</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2503.23829">http://arxiv.org/abs/2503.23829</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/11/2025, 5:31:32 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2503.23829 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2503.23829">10.48550/arXiv.2503.23829</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2503.23829</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_5FR6HYMG">Preprint PDF					</li>
					<li id="item_2LS3I3AJ">Snapshot					</li>
				</ul>
			</li>


			<li id="item_7LJVWRVQ" class="item preprint">
			<h2>Bad Habits: Policy Confounding and Out-of-Trajectory Generalization in RL</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Miguel Suau</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Matthijs T. J. Spaan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Frans A. Oliehoek</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Reinforcement learning agents tend to develop habits that are 
effective only under specific policies. Following an initial exploration
 phase where agents try out different actions, they eventually converge 
onto a particular policy. As this occurs, the distribution over 
state-action trajectories becomes narrower, leading agents to repeatedly
 experience the same transitions. This repetitive exposure fosters 
spurious correlations between certain observations and rewards. Agents 
may then pick up on these correlations and develop simplistic habits 
tailored to the specific set of trajectories dictated by their policy. 
The problem is that these habits may yield incorrect outcomes when 
agents are forced to deviate from their typical trajectories, prompted 
by changes in the environment. This paper presents a mathematical 
characterization of this phenomenon, termed policy confounding, and 
illustrates, through a series of examples, the circumstances under which
 it occurs.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-06-24</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Bad Habits</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2306.02419">http://arxiv.org/abs/2306.02419</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/11/2025, 1:10:00 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2306.02419 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2306.02419">10.48550/arXiv.2306.02419</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2306.02419</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_7CFJ8MYU">Preprint PDF					</li>
					<li id="item_QXZ23RRT">Snapshot					</li>
				</ul>
			</li>


			<li id="item_KML4YVKZ" class="item preprint">
			<h2>Gemma 3 Technical Report</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Gemma Team</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Aishwarya Kamath</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Johan Ferret</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Shreya Pathak</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nino Vieillard</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ramona Merhej</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sarah Perrin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tatiana Matejovicova</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alexandre Ramé</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Morgane Rivière</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Louis Rouillard</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Thomas Mesnard</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Geoffrey Cideron</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jean-bastien Grill</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sabela Ramos</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Edouard Yvinec</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Michelle Casbon</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Etienne Pot</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ivo Penchev</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Gaël Liu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Francesco Visin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kathleen Kenealy</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lucas Beyer</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xiaohai Zhai</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Anton Tsitsulin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Robert Busa-Fekete</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alex Feng</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Noveen Sachdeva</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Benjamin Coleman</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yi Gao</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Basil Mustafa</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Iain Barr</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Emilio Parisotto</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David Tian</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Matan Eyal</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Colin Cherry</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jan-Thorsten Peter</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Danila Sinopalnikov</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Surya Bhupatiraju</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rishabh Agarwal</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mehran Kazemi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dan Malkin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ravin Kumar</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David Vilar</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Idan Brusilovsky</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jiaming Luo</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Andreas Steiner</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Abe Friesen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Abhanshu Sharma</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Abheesht Sharma</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Adi Mayrav Gilady</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Adrian Goedeckemeyer</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alaa Saade</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alex Feng</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alexander Kolesnikov</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alexei Bendebury</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alvin Abdagic</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Amit Vadi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>András György</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>André Susano Pinto</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Anil Das</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ankur Bapna</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Antoine Miech</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Antoine Yang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Antonia Paterson</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ashish Shenoy</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ayan Chakrabarti</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bilal Piot</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bo Wu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bobak Shahriari</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bryce Petrini</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Charlie Chen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Charline Le Lan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Christopher A. Choquette-Choo</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>C. J. Carey</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Cormac Brick</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Daniel Deutsch</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Danielle Eisenbud</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dee Cattle</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Derek Cheng</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dimitris Paparas</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Divyashree Shivakumar Sreepathihalli</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Doug Reid</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dustin Tran</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dustin Zelle</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Eric Noland</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Erwin Huizenga</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Eugene Kharitonov</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Frederick Liu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Gagik Amirkhanyan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Glenn Cameron</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hadi Hashemi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hanna Klimczak-Plucińska</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Harman Singh</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Harsh Mehta</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Harshal Tushar Lehri</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hussein Hazimeh</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ian Ballantyne</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Idan Szpektor</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ivan Nardini</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jean Pouget-Abadie</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jetha Chan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Joe Stanton</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>John Wieting</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jonathan Lai</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jordi Orbay</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Joseph Fernandez</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Josh Newlan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ju-yeong Ji</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jyotinder Singh</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kat Black</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kathy Yu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kevin Hui</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kiran Vodrahalli</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Klaus Greff</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Linhai Qiu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Marcella Valentine</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Marina Coelho</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Marvin Ritter</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Matt Hoffman</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Matthew Watson</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mayank Chaturvedi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Michael Moynihan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Min Ma</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nabila Babar</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Natasha Noy</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nathan Byrd</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nick Roy</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nikola Momchev</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nilay Chauhan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Noveen Sachdeva</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Oskar Bunyan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Pankil Botarda</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Paul Caron</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Paul Kishan Rubenstein</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Phil Culliton</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Philipp Schmid</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Pier Giuseppe Sessa</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Pingmei Xu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Piotr Stanczyk</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Pouya Tafti</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rakesh Shivanna</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Renjie Wu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Renke Pan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Reza Rokni</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rob Willoughby</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rohith Vallu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ryan Mullins</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sammy Jerome</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sara Smoot</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sertan Girgin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Shariq Iqbal</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Shashir Reddy</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Shruti Sheth</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Siim Põder</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sijal Bhatnagar</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sindhu Raghuram Panyam</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sivan Eiger</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Susan Zhang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tianqi Liu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Trevor Yacovone</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tyler Liechty</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Uday Kalra</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Utku Evci</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Vedant Misra</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Vincent Roseberry</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Vlad Feinberg</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Vlad Kolesnikov</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Woohyun Han</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Woosuk Kwon</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xi Chen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yinlam Chow</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yuvein Zhu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zichuan Wei</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zoltan Egyed</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Victor Cotruta</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Minh Giang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Phoebe Kirk</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Anand Rao</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kat Black</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nabila Babar</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jessica Lo</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Erica Moreira</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Luiz Gustavo Martins</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Omar Sanseviero</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lucas Gonzalez</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zach Gleicher</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tris Warkentin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Vahab Mirrokni</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Evan Senter</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Eli Collins</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Joelle Barral</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zoubin Ghahramani</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Raia Hadsell</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yossi Matias</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>D. Sculley</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Slav Petrov</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Noah Fiedel</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Noam Shazeer</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Oriol Vinyals</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jeff Dean</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Demis Hassabis</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Koray Kavukcuoglu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Clement Farabet</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Elena Buchatskaya</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jean-Baptiste Alayrac</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rohan Anil</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dmitry</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lepikhin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sebastian Borgeaud</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Olivier Bachem</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Armand Joulin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alek Andreev</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Cassidy Hardin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Robert Dadashi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Léonard Hussenot</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We introduce Gemma 3, a multimodal addition to the Gemma 
family of lightweight open models, ranging in scale from 1 to 27 billion
 parameters. This version introduces vision understanding abilities, a 
wider coverage of languages and longer context - at least 128K tokens. 
We also change the architecture of the model to reduce the KV-cache 
memory that tends to explode with long context. This is achieved by 
increasing the ratio of local to global attention layers, and keeping 
the span on local attention short. The Gemma 3 models are trained with 
distillation and achieve superior performance to Gemma 2 for both 
pre-trained and instruction finetuned versions. In particular, our novel
 post-training recipe significantly improves the math, chat, 
instruction-following and multilingual abilities, making Gemma3-4B-IT 
competitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to 
Gemini-1.5-Pro across benchmarks. We release all our models to the 
community.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-03-25</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2503.19786">http://arxiv.org/abs/2503.19786</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/11/2025, 12:33:53 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2503.19786 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2503.19786">10.48550/arXiv.2503.19786</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2503.19786</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Artificial Intelligence</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_RA7HQSBM">Preprint PDF					</li>
					<li id="item_9IIE7SRC">Snapshot					</li>
				</ul>
			</li>


			<li id="item_SLDRXLJR" class="item preprint">
			<h2>AI Governance through Markets</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Philip Moreira Tomei</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rupal Jain</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Matija Franklin</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This paper argues that market governance mechanisms should be 
considered a key approach in the governance of artificial intelligence 
(AI), alongside traditional regulatory frameworks. While current 
governance approaches have predominantly focused on regulation, we 
contend that market-based mechanisms offer effective incentives for 
responsible AI development. We examine four emerging vectors of market 
governance: insurance, auditing, procurement, and due diligence, 
demonstrating how these mechanisms can affirm the relationship between 
AI risk and financial risk while addressing capital allocation 
inefficiencies. While we do not claim that market forces alone can 
adequately protect societal interests, we maintain that standardised AI 
disclosures and market mechanisms can create powerful incentives for 
safe and responsible AI development. This paper urges regulators, 
economists, and machine learning researchers to investigate and 
implement market-based approaches to AI governance.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-03-05</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2501.17755">http://arxiv.org/abs/2501.17755</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/11/2025, 1:09:46 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2501.17755 [econ]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2501.17755">10.48550/arXiv.2501.17755</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2501.17755</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Economics - General Economics</li>
					<li>Quantitative Finance - Economics</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_9RJVCGC7">Full Text PDF					</li>
					<li id="item_VQRFQ2S7">Snapshot					</li>
				</ul>
			</li>


			<li id="item_HSYBDH3F" class="item preprint">
			<h2>An Illusion of Progress? Assessing the Current State of Web Agents</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tianci Xue</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Weijian Qi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tianneng Shi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Chan Hee Song</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Boyu Gou</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dawn Song</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Huan Sun</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yu Su</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>As digitalization and cloud technologies evolve, the web is 
becoming increasingly important in the modern society. Autonomous web 
agents based on large language models (LLMs) hold a great potential in 
work automation. It is therefore important to accurately measure and 
monitor the progression of their capabilities. In this work, we conduct a
 comprehensive and rigorous assessment of the current state of web 
agents. Our results depict a very different picture of the competency of
 current agents, suggesting over-optimism in previously reported 
results. This gap can be attributed to shortcomings in existing 
benchmarks. We introduce Online-Mind2Web, an online evaluation benchmark
 consisting of 300 diverse and realistic tasks spanning 136 websites. It
 enables us to evaluate web agents under a setting that approximates how
 real users use these agents. To facilitate more scalable evaluation and
 development, we also develop a novel LLM-as-a-Judge automatic 
evaluation method and show that it can achieve around 85% agreement with
 human judgment, substantially higher than existing methods. Finally, we
 present the first comprehensive comparative analysis of current web 
agents, highlighting both their strengths and limitations to inspire 
future research.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-04-02</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>An Illusion of Progress?</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2504.01382">http://arxiv.org/abs/2504.01382</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/11/2025, 5:31:52 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2504.01382 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2504.01382">10.48550/arXiv.2504.01382</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2504.01382</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Artificial Intelligence</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_ZVL4ZEPE">
<p class="plaintext">Comment: 22 pages, 16 figures, 4 tables</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_YJN4V9XD">Preprint PDF					</li>
					<li id="item_M26LZN8L">Snapshot					</li>
				</ul>
			</li>


			<li id="item_PF36Q4WI" class="item journalArticle">
			<h2>The AI Sc ientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yutaro Yamada</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Robert Tjarko Lange</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Cong Lu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Shengran Hu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Chris Lu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jakob Foerster</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David Ha</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Zotero</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_YZN34HIZ">PDF					</li>
				</ul>
			</li>


			<li id="item_NRLE4PHW" class="item preprint">
			<h2>Survey on Evaluation of LLM-based Agents</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Asaf Yehudai</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lilach Eden</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alan Li</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Guy Uziel</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yilun Zhao</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Roy Bar-Haim</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Arman Cohan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Michal Shmueli-Scheuer</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The emergence of LLM-based agents represents a paradigm shift 
in AI, enabling autonomous systems to plan, reason, use tools, and 
maintain memory while interacting with dynamic environments. This paper 
provides the first comprehensive survey of evaluation methodologies for 
these increasingly capable agents. We systematically analyze evaluation 
benchmarks and frameworks across four critical dimensions: (1) 
fundamental agent capabilities, including planning, tool use, 
self-reflection, and memory; (2) application-specific benchmarks for 
web, software engineering, scientific, and conversational agents; (3) 
benchmarks for generalist agents; and (4) frameworks for evaluating 
agents. Our analysis reveals emerging trends, including a shift toward 
more realistic, challenging evaluations with continuously updated 
benchmarks. We also identify critical gaps that future research must 
address-particularly in assessing cost-efficiency, safety, and 
robustness, and in developing fine-grained, and scalable evaluation 
methods. This survey maps the rapidly evolving landscape of agent 
evaluation, reveals the emerging trends in the field, identifies current
 limitations, and proposes directions for future research.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-03-20</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2503.16416">http://arxiv.org/abs/2503.16416</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/25/2025, 5:38:55 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2503.16416 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2503.16416">10.48550/arXiv.2503.16416</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2503.16416</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/2/2025, 2:13:04 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/2/2025, 2:13:04 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_RNA7D26M">Full Text PDF					</li>
					<li id="item_KYPUDJGE">Snapshot					</li>
				</ul>
			</li>


			<li id="item_SHEGTIIT" class="item webpage">
			<h2>On the Biology of a Large Language Model</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Authors Affiliations Published Not published yet DOI No DOI yet</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We investigate the internal mechanisms used by Claude 3.5 
Haiku — Anthropic's lightweight production model — in a variety of 
contexts, using our circuit tracing methodology.</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://transformer-circuits.pub/2025/attribution-graphs/biology.html">https://transformer-circuits.pub/2025/attribution-graphs/biology.html</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/11/2025, 12:35:22 PM</td>
					</tr>
					<tr>
					<th>Website Title</th>
						<td>Transformer Circuits</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_JNHKR2GV">Snapshot					</li>
				</ul>
			</li>


			<li id="item_PKN9Q85U" class="item preprint">
			<h2>Sample, Scrutinize and Scale: Effective Inference-Time Search by Scaling Verification</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Eric Zhao</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Pranjal Awasthi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sreenivas Gollapudi</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Sampling-based search, a simple paradigm for utilizing 
test-time compute, involves generating multiple candidate responses and 
selecting the best one -- typically by having models self-verify each 
response for correctness. In this paper, we study the scaling trends 
governing sampling-based search. Among our findings is that simply 
scaling up a minimalist implementation of sampling-based search, using 
only random sampling and direct self-verification, provides a practical 
inference method that, for example, elevates the reasoning capabilities 
of Gemini v1.5 Pro above that of o1-Preview on popular benchmarks. We 
partially attribute the scalability of sampling-based search to a 
phenomenon of implicit scaling, where sampling a larger pool of 
responses in turn improves self-verification accuracy. We further 
identify two useful principles for improving self-verification 
capabilities with test-time compute: (1) comparing across responses 
provides helpful signals about the locations of errors and 
hallucinations, and (2) different model output styles are useful for 
different contexts -- chains of thought are useful for reasoning but 
harder to verify. We also find that, though accurate verification can be
 elicited, frontier models demonstrate remarkably weak out-of-box 
verification capabilities and introduce a benchmark to measure progress 
on these deficiencies.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-02-20</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Sample, Scrutinize and Scale</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2502.01839">http://arxiv.org/abs/2502.01839</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/25/2025, 9:28:34 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2502.01839 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2502.01839">10.48550/arXiv.2502.01839</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2502.01839</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/2/2025, 2:13:04 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/2/2025, 2:13:04 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_VBEXXM4T">Preprint PDF					</li>
					<li id="item_IU67AE47">Snapshot					</li>
				</ul>
			</li>


			<li id="item_DP7JS6DB" class="item preprint">
			<h2>CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web Application Vulnerabilities</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yuxuan Zhu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Antony Kellermann</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dylan Bowman</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Philip Li</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Akul Gupta</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Adarsh Danda</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Richard Fang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Conner Jensen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Eric Ihli</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jason Benn</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jet Geronimo</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Avi Dhir</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sudhit Rao</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kaicheng Yu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Twm Stone</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Daniel Kang</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large language model (LLM) agents are increasingly capable of 
autonomously conducting cyberattacks, posing significant threats to 
existing applications. This growing risk highlights the urgent need for a
 real-world benchmark to evaluate the ability of LLM agents to exploit 
web application vulnerabilities. However, existing benchmarks fall short
 as they are limited to abstracted Capture the Flag competitions or lack
 comprehensive coverage. Building a benchmark for real-world 
vulnerabilities involves both specialized expertise to reproduce 
exploits and a systematic approach to evaluating unpredictable threats. 
To address this challenge, we introduce CVE-Bench, a real-world 
cybersecurity benchmark based on critical-severity Common 
Vulnerabilities and Exposures. In CVE-Bench, we design a sandbox 
framework that enables LLM agents to exploit vulnerable web applications
 in scenarios that mimic real-world conditions, while also providing 
effective evaluation of their exploits. Our evaluation shows that the 
state-of-the-art agent framework can resolve up to 13% of 
vulnerabilities.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-04-01</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>CVE-Bench</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2503.17332">http://arxiv.org/abs/2503.17332</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/11/2025, 1:09:54 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2503.17332 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2503.17332">10.48550/arXiv.2503.17332</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2503.17332</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Cryptography and Security</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_IDYTL82T">
<p class="plaintext">Comment: 15 pages, 4 figures, 5 tables</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_BP5A3NDW">Preprint PDF					</li>
					<li id="item_RDU6LB6H">Snapshot					</li>
				</ul>
			</li>


			<li id="item_28A7NFK3" class="item attachment">
			<h2>GR00T_1_Whitepaper.pdf</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Attachment</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://d1qx31qr3h6wln.cloudfront.net/publications/GR00T_1_Whitepaper.pdf">https://d1qx31qr3h6wln.cloudfront.net/publications/GR00T_1_Whitepaper.pdf</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/25/2025, 9:20:45 PM</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/2/2025, 2:13:04 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/2/2025, 2:13:04 PM</td>
					</tr>
				</tbody></table>
			</li>


			<li id="item_IM7UVG98" class="item attachment">
			<h2>[external] a long list of open problems and concrete projects in evals</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Attachment</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
			</li>


			<li id="item_WT5QGYBS" class="item webpage">
			<h2>Introducing Docent</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://transluce.org/introducing-docent">https://transluce.org/introducing-docent</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/11/2025, 12:31:23 PM</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_6Y6E3JKZ">Snapshot					</li>
				</ul>
			</li>


			<li id="item_55W8HDMM" class="item attachment">
			<h2>Report-Technologist-Economist-Disconnect-ARI04072025-1.pdf</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Attachment</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://ari.us/wp-content/uploads/2025/04/Report-Technologist-Economist-Disconnect-ARI04072025-1.pdf">https://ari.us/wp-content/uploads/2025/04/Report-Technologist-Economist-Disconnect-ARI04072025-1.pdf</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/9/2025, 11:27:27 PM</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
			</li>


			<li id="item_F47G3TRF" class="item webpage">
			<h2>Stripe Press — The Scaling Era</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>An inside view of the AI revolution, from the people and companies making it happen.</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-US</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://stripe.com/scaling">https://stripe.com/scaling</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/11/2025, 2:47:51 PM</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_YDXRRU3B">Snapshot					</li>
				</ul>
			</li>


			<li id="item_QEF5GGJI" class="item webpage">
			<h2>Taking a responsible path to AGI - Google DeepMind</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://deepmind.google/discover/blog/taking-a-responsible-path-to-agi/">https://deepmind.google/discover/blog/taking-a-responsible-path-to-agi/</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/11/2025, 2:46:31 PM</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/14/2025, 9:44:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_8JCTMUKV">Taking a responsible path to AGI - Google DeepMind					</li>
				</ul>
			</li>


			<li id="item_BDCZT5NE" class="item webpage">
			<h2>Investigating truthfulness in a pre-release o3 model | Transluce AI</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://transluce.org/investigating-o3-truthfulness">https://transluce.org/investigating-o3-truthfulness</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/17/2025, 11:40:51 AM</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/17/2025, 11:40:51 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/17/2025, 11:40:51 AM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_P9BW2KBN">Investigating truthfulness in a pre-release o3 model | Transluce AI					</li>
				</ul>
			</li>

		</ul>
	
</body></html>