<!DOCTYPE html>
<html><head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<meta http-equiv="Content-Security-Policy" content="default-src 'none'; img-src data:; style-src 'unsafe-inline' data:">
		<title>Zotero Report</title>
		<link rel="stylesheet" type="text/css" href="data:text/css;base64,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">
		<link rel="stylesheet" type="text/css" media="screen,projection" href="data:text/css;base64,LyogR2VuZXJpYyBzdHlsZXMgKi8KYm9keSB7Cglmb250OiA2Mi41JSBHZW9yZ2lhLCBUaW1lcywgc2VyaWY7Cgl3aWR0aDogNzgwcHg7CgltYXJnaW46IDAgYXV0bzsKfQoKaDIgewoJZm9udC1zaXplOiAxLjVlbTsKCWxpbmUtaGVpZ2h0OiAxLjVlbTsKCWZvbnQtZmFtaWx5OiBHZW9yZ2lhLCBUaW1lcywgc2VyaWY7Cn0KCnAgewoJbGluZS1oZWlnaHQ6IDEuNWVtOwp9CgphOmFueS1saW5rIHsKCWNvbG9yOiAjOTAwOwp9CgphOmhvdmVyLCBhOmFjdGl2ZSB7Cgljb2xvcjogIzc3NzsKfQoKQG1lZGlhIChwcmVmZXJzLWNvbG9yLXNjaGVtZTogZGFyaykgewoJYTphbnktbGluayB7CgkJY29sb3I6ICNmMDA7Cgl9CgoJYTpob3ZlciwgYTphY3RpdmUgewoJCWNvbG9yOiAjOTk5OwoJfQp9CgoKdWwucmVwb3J0IHsKCWZvbnQtc2l6ZTogMS40ZW07Cgl3aWR0aDogNjgwcHg7CgltYXJnaW46IDAgYXV0bzsKCXBhZGRpbmc6IDIwcHggMjBweDsKfQoKLyogTWV0YWRhdGEgdGFibGUgKi8KdGFibGUgewoJYm9yZGVyOiAxcHggI2NjYyBzb2xpZDsKCW92ZXJmbG93OiBhdXRvOwoJd2lkdGg6IDEwMCU7CgltYXJnaW46IC4xZW0gYXV0byAuNzVlbTsKCXBhZGRpbmc6IDAuNWVtOwp9Cg==">
		<link rel="stylesheet" type="text/css" media="print" href="data:text/css;base64,Ym9keSB7Cglmb250OiAxMnB0ICJUaW1lcyBOZXcgUm9tYW4iLCBUaW1lcywgR2VvcmdpYSwgc2VyaWY7CgltYXJnaW46IDA7Cgl3aWR0aDogYXV0bzsKfQoKLyogUGFnZSBCcmVha3MgKHBhZ2UtYnJlYWstaW5zaWRlIG9ubHkgcmVjb2duaXplZCBieSBPcGVyYSkgKi8KaDEsIGgyLCBoMywgaDQsIGg1LCBoNiB7CglwYWdlLWJyZWFrLWFmdGVyOiBhdm9pZDsKCXBhZ2UtYnJlYWstaW5zaWRlOiBhdm9pZDsKfQoKdWwsIG9sLCBkbCB7CglwYWdlLWJyZWFrLWluc2lkZTogYXZvaWQ7Cgljb2xvci1hZGp1c3Q6IGV4YWN0Owp9CgpoMiB7Cglmb250LXNpemU6IDEuM2VtOwoJbGluZS1oZWlnaHQ6IDEuM2VtOwp9CgphIHsKCWNvbG9yOiBpbmhlcml0OwoJdGV4dC1kZWNvcmF0aW9uOiBub25lOwp9Cg==">
	</head>
	<body>
		<ul class="report combineChildItems">
			<li id="item_4P7JITMS" class="item webpage">
			<h2>Better Feeds: Algorithms That Put People First</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This report, prepared by the KGI Expert Working Group on 
Recommender Systems, offers comprehensive insights and policy guidance 
aimed at optimizing recommender systems for long-term user value and 
high-quality experiences. Drawing on a multidisciplinary research base 
and industry expertise, the report highlights key challenges in the 
current design and regulation of recommender systems and proposes 
actionable solutions for policymakers and product designers.

A key concern is that some platforms optimize their recommender systems 
to maximize certain forms of predicted engagement, which can prioritize 
clicks and likes over stronger signals of long-term user value. 
Maximizing the chances that users will click, like, share, and view 
content this week, this month, and this quarter aligns well with the 
business interests of tech platforms monetized through advertising. 
Product teams are rewarded for showing short-term gains in platform 
usage, and financial markets and investors reward companies that can 
deliver large audiences to advertisers.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-03-04</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-US</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Better Feeds</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://kgi.georgetown.edu/research-and-commentary/better-feeds/">https://kgi.georgetown.edu/research-and-commentary/better-feeds/</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/13/2025, 8:35:10 AM</td>
					</tr>
					<tr>
					<th>Website Title</th>
						<td>Knight-Georgetown Institute</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/13/2025, 8:35:10 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>3/13/2025, 8:35:52 AM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_HPL563Q4">Better-Feeds_-Algorithms-That-Put-People-First.pdf					</li>
					<li id="item_29SDCEAS">Snapshot					</li>
				</ul>
			</li>


			<li id="item_PSZMT46C" class="item preprint">
			<h2>Towards a Theory of AI Personhood</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Francis Rhys Ward</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>I am a person and so are you. Philosophically we sometimes 
grant personhood to non-human animals, and entities such as sovereign 
states or corporations can legally be considered persons. But when, if 
ever, should we ascribe personhood to AI systems? In this paper, we 
outline necessary conditions for AI personhood, focusing on agency, 
theory-of-mind, and self-awareness. We discuss evidence from the machine
 learning literature regarding the extent to which contemporary AI 
systems, such as language models, satisfy these conditions, finding the 
evidence surprisingly inconclusive. If AI systems can be considered 
persons, then typical framings of AI alignment may be incomplete. 
Whereas agency has been discussed at length in the literature, other 
aspects of personhood have been relatively neglected. AI agents are 
often assumed to pursue fixed goals, but AI persons may be self-aware 
enough to reflect on their aims, values, and positions in the world and 
thereby induce their goals to change. We highlight open research 
directions to advance the understanding of AI personhood and its 
relevance to alignment. Finally, we reflect on the ethical 
considerations surrounding the treatment of AI systems. If AI systems 
are persons, then seeking control and alignment may be ethically 
untenable.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-01-23</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2501.13533">http://arxiv.org/abs/2501.13533</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/13/2025, 8:07:31 AM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2501.13533 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2501.13533">10.48550/arXiv.2501.13533</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2501.13533</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/13/2025, 8:07:31 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>3/13/2025, 8:07:33 AM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_XXEC3KFA">
<p class="plaintext">Comment: AAAI-25 AI Alignment Track</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_LFVMQM4C">Preprint PDF					</li>
					<li id="item_BRY8YRUU">Snapshot					</li>
				</ul>
			</li>


			<li id="item_243F4FWM" class="item journalArticle">
			<h2>Authorship and ChatGPT: a Conservative View</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>René van Woudenberg</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Chris Ranalli</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Daniel Bracker</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Is ChatGPT an author?&nbsp;Given its capacity to generate 
something that reads like human-written text in response to prompts, it 
might seem natural to ascribe authorship to ChatGPT. However, we argue 
that ChatGPT is not an author. ChatGPT fails to meet the criteria of 
authorship because it lacks the ability to perform illocutionary speech 
acts such as promising or asserting, lacks the fitting mental states 
like knowledge, belief, or intention, and cannot take responsibility for
 the texts it produces. Three perspectives are compared: liberalism 
(which ascribes authorship to ChatGPT), conservatism (which denies 
ChatGPT's authorship for normative and metaphysical reasons), and 
moderatism (which treats ChatGPT as if it possesses authorship without 
committing to the existence of mental states like knowledge, belief, or 
intention). We conclude that conservatism provides a more nuanced 
understanding of authorship in AI than liberalism and moderatism, 
without denying the significant potential, influence, or utility of AI 
technologies such as ChatGPT.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-02-26</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Authorship and ChatGPT</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Springer Link</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1007/s13347-024-00715-1">https://doi.org/10.1007/s13347-024-00715-1</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/12/2025, 8:59:33 AM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>37</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>34</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Philosophy &amp; Technology</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/s13347-024-00715-1">10.1007/s13347-024-00715-1</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Philos. Technol.</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2210-5441</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/12/2025, 8:59:33 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>3/12/2025, 8:59:42 AM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>ChatGPT</li>
					<li>AI</li>
					<li>Intention</li>
					<li>Agency</li>
					<li>Assertion</li>
					<li>Authorship</li>
					<li>Conservativism</li>
					<li>Normativity</li>
					<li>Promising</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_5HHLIT65">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_AB2UT2TX" class="item journalArticle">
			<h2>The Epistemic Cost of Opacity: How the Use of Artificial 
Intelligence Undermines the Knowledge of Medical Doctors in High-Stakes 
Contexts</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Eva Schmidt</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Paul Martin Putora</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rianne Fijten</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Artificial intelligent (AI) systems used in medicine are often
 very reliable and accurate, but at the price of their being 
increasingly opaque. This raises the question whether a system’s opacity
 undermines the ability of medical doctors to acquire knowledge on the 
basis of its outputs. We investigate this question by focusing on a case
 in which a patient’s risk of recurring breast cancer is predicted by an
 opaque AI system. We argue that, given the system’s opacity, as well as
 the possibility of malfunctioning AI systems, practitioners’ inability 
to check the correctness of their outputs, and the high stakes of such 
cases, the knowledge of medical practitioners is indeed undermined. They
 are lucky to form true beliefs based on the AI systems’ outputs, and 
knowledge is incompatible with luck. We supplement this claim with a 
specific version of the safety condition on knowledge, Safety*. We argue
 that, relative to the perspective of the medical doctor in our example 
case, his relevant beliefs could easily be false, and this despite his 
evidence that the AI system functions reliably. Assuming that Safety* is
 necessary for knowledge, the practitioner therefore doesn’t know. We 
address three objections to our proposal before turning to practical 
suggestions for improving the epistemic situation of medical doctors.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-01-13</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>The Epistemic Cost of Opacity</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Springer Link</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1007/s13347-024-00834-9">https://doi.org/10.1007/s13347-024-00834-9</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/15/2025, 8:50:27 AM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>38</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>5</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Philosophy &amp; Technology</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/s13347-024-00834-9">10.1007/s13347-024-00834-9</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Philos. Technol.</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2210-5441</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/15/2025, 8:50:27 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>3/15/2025, 8:50:29 AM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Artificial Intelligence</li>
					<li>Black-box AI</li>
					<li>Explainable AI</li>
					<li>Healthcare</li>
					<li>Medical AI</li>
					<li>Medical Ethics</li>
					<li>Safety Condition</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_I4JFT8GM">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_PZI6F8K9" class="item journalArticle">
			<h2>Manipulative Underspeciﬁcation</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Justin D’Ambrosio</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In conversation, speakers often felicitously underspecify the 
content of their speech acts, leaving audiences uncertain about what 
they mean. This paper discusses how such underspeciﬁcation and the 
resulting uncertainty can be used deliberately, and manipulatively, to 
achieve a range of noncommunicative conversational goals—including 
minimizing conversational conﬂict, manufacturing acceptance or perceived
 agreement, and gaining or bolstering status. I argue that speakers who 
manipulatively underspecify their speech acts in this way are engaged in
 a mock speech act that I call pied piping. In pied piping, a speaker 
leaves open a range of interpretations for a speech act while preserving
 both plausible deniability and plausible assertability; depending on 
how the audience responds, the speaker can retroactively commit to any 
of the interpretations left open, and so try to retroactively update the
 common ground. I go on to develop a model of how pied-piping functions 
that incorporates game-theoretic elements into the more traditional 
common-ground framework in order to capture the uncertainty of updating.</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Zotero</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/12/2025, 10:43:53 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>3/12/2025, 10:43:57 AM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_NNMQXBPJ">PDF					</li>
				</ul>
			</li>


			<li id="item_YIQ6YG9R" class="item preprint">
			<h2>AGI, Governments, and Free Societies</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Justin B. Bullock</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Samuel Hammond</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Seb Krier</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This paper examines how artificial general intelligence (AGI) 
could fundamentally reshape the delicate balance between state capacity 
and individual liberty that sustains free societies. Building on 
Acemoglu and Robinson's 'narrow corridor' framework, we argue that AGI 
poses distinct risks of pushing societies toward either a 'despotic 
Leviathan' through enhanced state surveillance and control, or an 
'absent Leviathan' through the erosion of state legitimacy relative to 
AGI-empowered non-state actors. Drawing on public administration theory 
and recent advances in AI capabilities, we analyze how these dynamics 
could unfold through three key channels: the automation of discretionary
 decision-making within agencies, the evolution of bureaucratic 
structures toward system-level architectures, and the transformation of 
democratic feedback mechanisms. Our analysis reveals specific failure 
modes that could destabilize liberal institutions. Enhanced state 
capacity through AGI could enable unprecedented surveillance and 
control, potentially entrenching authoritarian practices. Conversely, 
rapid diffusion of AGI capabilities to non-state actors could undermine 
state legitimacy and governability. We examine how these risks manifest 
differently at the micro level of individual bureaucratic decisions, the
 meso level of organizational structure, and the macro level of 
democratic processes. To preserve the narrow corridor of liberty, we 
propose a governance framework emphasizing robust technical safeguards, 
hybrid institutional designs that maintain meaningful human oversight, 
and adaptive regulatory mechanisms.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-02-14</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2503.05710">http://arxiv.org/abs/2503.05710</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/12/2025, 10:40:59 AM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2503.05710 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2503.05710">10.48550/arXiv.2503.05710</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2503.05710</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/12/2025, 10:40:59 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>3/12/2025, 10:41:12 AM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_NVS8MKBS">
<p class="plaintext">Comment: 40 pages</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_QPF24PIX">Preprint PDF					</li>
					<li id="item_26RK5HR6">Snapshot					</li>
				</ul>
			</li>


			<li id="item_H4RHBR7G" class="item preprint">
			<h2>Are Biological Systems More Intelligent Than Artificial Intelligence?</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Michael Timothy Bennett</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Are biological self-organising systems more `intelligent' than
 artificial intelligence? If so, why? We frame intelligence as 
adaptability, and explore this question using a mathematical formalism 
of causal learning. We compare systems by how they delegate control, 
illustrating how this applies with examples of computational, 
biological, human organisational and economic systems. We formally show 
the scale-free, dynamic, bottom-up architecture of biological 
self-organisation allows for more efficient adaptation than the static 
top-down architecture typical of computers, because adaptation can take 
place at lower levels of abstraction. Artificial intelligence rests on a
 static, human-engineered `stack'. It only adapts at high levels of 
abstraction. To put it provocatively, a static computational stack is 
like an inflexible bureaucracy. Biology is more `intelligent' because it
 delegates adaptation down the stack. We call this 
multilayer-causal-learning. It inherits a flaw of biological systems. 
Cells become cancerous when isolated from the collective informational 
structure, reverting to primitive transcriptional behaviour. We show 
states analogous to cancer occur when collectives are too tightly 
constrained. To adapt to adverse conditions control should be delegated 
to the greatest extent, like the doctrine of mission-command. Our result
 shows how to design more robust systems and lays a mathematical 
foundation for future empirical research.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-02-03</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/e6fky_v2">https://osf.io/e6fky_v2</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/15/2025, 8:30:54 AM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31219/osf.io/e6fky_v2">10.31219/osf.io/e6fky_v2</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/15/2025, 8:30:54 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>3/15/2025, 8:30:57 AM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_E5NUQ8L3">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_LWV92VFG" class="item preprint">
			<h2>Keep the Future Human: Why and How We Should Close the Gates to AGI and Superintelligence, and What We Should Build Instead</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Anthony Aguirre</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Dramatic advances in artificial intelligence over the past 
decade (for narrow-purpose AI) and the last several years (for 
general-purpose AI) have transformed AI from a niche academic field to 
the core business strategy of many of the world's largest companies, 
with hundreds of billions of dollars in annual investment in the 
techniques and technologies for advancing AI's capabilities. We now come
 to a critical juncture. As the capabilities of new AI systems begin to 
match and exceed those of humans across many cognitive domains, humanity
 must decide: how far do we go, and in what direction? This essay argues
 that we should keep the future human by closing the "gates" to 
smarter-than-human, autonomous, general-purpose AI -- sometimes called 
"AGI" -- and especially to the highly-superhuman version sometimes 
called "superintelligence." Instead, we should focus on powerful, 
trustworthy AI tools that can empower individuals and transformatively 
improve human societies' abilities to do what they do best.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-03-07</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Keep the Future Human</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2311.09452">http://arxiv.org/abs/2311.09452</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/13/2025, 8:18:13 AM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2311.09452 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2311.09452">10.48550/arXiv.2311.09452</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2311.09452</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/13/2025, 8:18:13 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>3/13/2025, 8:18:16 AM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_RF44FAEE">
<p class="plaintext">Comment: 62 pages, 2 figures, 3 appendices. This is a total rewrite and major expansion of a previous version entitled "Close the Gates."</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_EGEWBR3T">Preprint PDF					</li>
					<li id="item_TXP5N43Q">Snapshot					</li>
				</ul>
			</li>

		</ul>
	
</body></html>