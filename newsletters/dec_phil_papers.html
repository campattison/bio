<!DOCTYPE html>
<html><head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<meta http-equiv="Content-Security-Policy" content="script-src 'none'; media-src 'none'">
		<title>Zotero Report</title>
		<link rel="stylesheet" type="text/css" href="data:text/css;base64,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">
		<link rel="stylesheet" type="text/css" media="screen,projection" href="data:text/css;base64,LyogR2VuZXJpYyBzdHlsZXMgKi8KYm9keSB7Cglmb250OiA2Mi41JSBHZW9yZ2lhLCBUaW1lcywgc2VyaWY7Cgl3aWR0aDogNzgwcHg7CgltYXJnaW46IDAgYXV0bzsKfQoKaDIgewoJZm9udC1zaXplOiAxLjVlbTsKCWxpbmUtaGVpZ2h0OiAxLjVlbTsKCWZvbnQtZmFtaWx5OiBHZW9yZ2lhLCBUaW1lcywgc2VyaWY7Cn0KCnAgewoJbGluZS1oZWlnaHQ6IDEuNWVtOwp9CgphOmxpbmssIGE6dmlzaXRlZCB7Cgljb2xvcjogIzkwMDsKfQoKYTpob3ZlciwgYTphY3RpdmUgewoJY29sb3I6ICM3Nzc7Cn0KCgp1bC5yZXBvcnQgewoJZm9udC1zaXplOiAxLjRlbTsKCXdpZHRoOiA2ODBweDsKCW1hcmdpbjogMCBhdXRvOwoJcGFkZGluZzogMjBweCAyMHB4Owp9CgovKiBNZXRhZGF0YSB0YWJsZSAqLwp0YWJsZSB7Cglib3JkZXI6IDFweCAjY2NjIHNvbGlkOwoJb3ZlcmZsb3c6IGF1dG87Cgl3aWR0aDogMTAwJTsKCW1hcmdpbjogLjFlbSBhdXRvIC43NWVtOwoJcGFkZGluZzogMC41ZW07Cn0K">
		<link rel="stylesheet" type="text/css" media="print" href="data:text/css;base64,Ym9keSB7Cglmb250OiAxMnB0ICJUaW1lcyBOZXcgUm9tYW4iLCBUaW1lcywgR2VvcmdpYSwgc2VyaWY7CgltYXJnaW46IDA7Cgl3aWR0aDogYXV0bzsKCWNvbG9yOiBibGFjazsKfQoKLyogUGFnZSBCcmVha3MgKHBhZ2UtYnJlYWstaW5zaWRlIG9ubHkgcmVjb2duaXplZCBieSBPcGVyYSkgKi8KaDEsIGgyLCBoMywgaDQsIGg1LCBoNiB7CglwYWdlLWJyZWFrLWFmdGVyOiBhdm9pZDsKCXBhZ2UtYnJlYWstaW5zaWRlOiBhdm9pZDsKfQoKdWwsIG9sLCBkbCB7CglwYWdlLWJyZWFrLWluc2lkZTogYXZvaWQ7Cgljb2xvci1hZGp1c3Q6IGV4YWN0Owp9CgpoMiB7Cglmb250LXNpemU6IDEuM2VtOwoJbGluZS1oZWlnaHQ6IDEuM2VtOwp9CgphIHsKCWNvbG9yOiAjMDAwOwoJdGV4dC1kZWNvcmF0aW9uOiBub25lOwp9Cg==">
	</head>
	<body>
		<ul class="report combineChildItems">
			<li id="item_D3P2HND7" class="item preprint">
			<h2>Conscious artificial intelligence and biological naturalism</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Seth Anil</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>As artificial intelligence (AI) continues to develop, it is 
natural to ask whether AI systems can be not only intelligent, but also 
conscious. I consider why some people think AI might develop 
consciousness, identifying some biases that lead us astray. I ask what 
it would take for conscious AI to be a realistic prospect, pushing back 
against some common assumptions such as the notion that computation 
provides a sufficient basis for consciousness. I’ll instead make the 
case for taking seriously the possibility that consciousness might 
depend on our nature as living organisms – a form of biological 
naturalism. I will end by exploring some wider issues including testing 
for consciousness in AI, and ethical considerations arising from AI that
 either actually is, or convincingly seems to be, conscious.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-06-30</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/tz6an">https://osf.io/tz6an</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>11/12/2024, 8:54:33 AM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31234/osf.io/tz6an">10.31234/osf.io/tz6an</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>11/12/2024, 8:54:33 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>11/16/2024, 3:29:54 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>AI</li>
					<li>artificial consciousness</li>
					<li>artificial intelligence</li>
					<li>biological naturalism</li>
					<li>computational functionalism</li>
					<li>consciousness</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_F8NL3Z6M">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_LZNBTJGN" class="item preprint">
			<h2>Biased AI can Influence Political Decision-Making</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jillian Fisher</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Shangbin Feng</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Robert Aron</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Thomas Richardson</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yejin Choi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Daniel W. Fisher</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jennifer Pan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yulia Tsvetkov</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Katharina Reinecke</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>As modern AI models become integral to everyday tasks, 
concerns about their inherent biases and their potential impact on human
 decision-making have emerged. While bias in models are well-documented,
 less is known about how these biases influence human decisions. This 
paper presents two interactive experiments investigating the effects of 
partisan bias in AI language models on political decision-making. 
Participants interacted freely with either a biased liberal, biased 
conservative, or unbiased control model while completing political 
decision-making tasks. We found that participants exposed to politically
 biased models were significantly more likely to adopt opinions and make
 decisions aligning with the AI's bias, regardless of their personal 
political partisanship. However, we also discovered that prior knowledge
 about AI could lessen the impact of the bias, highlighting the possible
 importance of AI education for robust bias mitigation. Our findings not
 only highlight the critical effects of interacting with biased AI and 
its ability to impact public discourse and political conduct, but also 
highlights potential techniques for mitigating these risks in the 
future.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-11-04</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2410.06415">http://arxiv.org/abs/2410.06415</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>11/11/2024, 8:54:31 AM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2410.06415</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2410.06415">10.48550/arXiv.2410.06415</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2410.06415</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>11/11/2024, 8:54:31 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>11/16/2024, 1:56:40 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Human-Computer Interaction</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_C832DUEU">Preprint PDF					</li>
					<li id="item_L52JFPE9">Snapshot					</li>
				</ul>
			</li>


			<li id="item_ZJTN2U8G" class="item preprint">
			<h2>Take Caution in Using LLMs as Human Surrogates: Scylla Ex Machina</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yuan Gao</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dokyun Lee</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Gordon Burtch</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sina Fazelpour</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Recent studies suggest large language models (LLMs) can 
exhibit human-like reasoning, aligning with human behavior in economic 
experiments, surveys, and political discourse. This has led many to 
propose that LLMs can be used as surrogates for humans in social science
 research. However, LLMs differ fundamentally from humans, relying on 
probabilistic patterns, absent the embodied experiences or survival 
objectives that shape human cognition. We assess the reasoning depth of 
LLMs using the 11-20 money request game. Almost all advanced approaches 
fail to replicate human behavior distributions across many models, 
except in one case involving fine-tuning using a substantial amount of 
human behavior data. Causes of failure are diverse, relating to input 
language, roles, and safeguarding. These results caution against using 
LLMs to study human behaviors or as human surrogates.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-10-25</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Take Caution in Using LLMs as Human Surrogates</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2410.19599">http://arxiv.org/abs/2410.19599</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>10/30/2024, 9:09:40 AM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2410.19599</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2410.19599">10.48550/arXiv.2410.19599</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2410.19599</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>10/30/2024, 9:09:40 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>10/30/2024, 9:09:42 AM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computers and Society</li>
					<li>Computer Science - Human-Computer Interaction</li>
					<li>Economics - General Economics</li>
					<li>Quantitative Finance - Economics</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_TM6FJVSP">Preprint PDF					</li>
					<li id="item_WQ9NVG3N">Snapshot					</li>
				</ul>
			</li>


			<li id="item_Y423ZIRS" class="item preprint">
			<h2>How Transformers Solve Propositional Logic Problems: A Mechanistic Analysis</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Guan Zhe Hong</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nishanth Dikkala</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Enming Luo</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Cyrus Rashtchian</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rina Panigrahy</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large language models (LLMs) have shown amazing performance on
 tasks that require planning and reasoning. Motivated by this, we 
investigate the internal mechanisms that underpin a network's ability to
 perform complex logical reasoning. We first construct a synthetic 
propositional logic problem that serves as a concrete test-bed for 
network training and evaluation. Crucially, this problem demands 
nontrivial planning to solve, but we can train a small transformer to 
achieve perfect accuracy. Building on our set-up, we then pursue an 
understanding of precisely how a three-layer transformer, trained from 
scratch, solves this problem. We are able to identify certain "planning"
 and "reasoning" circuits in the network that necessitate cooperation 
between the attention blocks to implement the desired logic. To expand 
our findings, we then study a larger model, Mistral 7B. Using activation
 patching, we characterize internal components that are critical in 
solving our logic problem. Overall, our work systemically uncovers novel
 aspects of small and large transformers, and continues the study of how
 they plan and reason.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-11-06</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>How Transformers Solve Propositional Logic Problems</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2411.04105">http://arxiv.org/abs/2411.04105</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>11/7/2024, 2:05:21 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2411.04105</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2411.04105">10.48550/arXiv.2411.04105</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2411.04105</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>11/7/2024, 2:05:21 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>11/7/2024, 2:05:21 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_JYWJSKHS">Preprint PDF					</li>
					<li id="item_KZ44HGUB">Snapshot					</li>
				</ul>
			</li>


			<li id="item_RJINRSYZ" class="item preprint">
			<h2>Imagining and building wise machines: The centrality of AI metacognition</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Samuel G. B. Johnson</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Amir-Hossein Karimi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yoshua Bengio</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nick Chater</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tobias Gerstenberg</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kate Larson</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sydney Levine</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Melanie Mitchell</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Iyad Rahwan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bernhard Schölkopf</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Igor Grossmann</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Recent advances in artificial intelligence (AI) have produced 
systems capable of increasingly sophisticated performance on cognitive 
tasks. However, AI systems still struggle in critical ways: 
unpredictable and novel environments (robustness), lack of transparency 
in their reasoning (explainability), challenges in communication and 
commitment (cooperation), and risks due to potential harmful actions 
(safety). We argue that these shortcomings stem from one overarching 
failure: AI systems lack wisdom. Drawing from cognitive and social 
sciences, we define wisdom as the ability to navigate intractable 
problems - those that are ambiguous, radically uncertain, novel, 
chaotic, or computationally explosive - through effective task-level and
 metacognitive strategies. While AI research has focused on task-level 
strategies, metacognition - the ability to reflect on and regulate one's
 thought processes - is underdeveloped in AI systems. In humans, 
metacognitive strategies such as recognizing the limits of one's 
knowledge, considering diverse perspectives, and adapting to context are
 essential for wise decision-making. We propose that integrating 
metacognitive capabilities into AI systems is crucial for enhancing 
their robustness, explainability, cooperation, and safety. By focusing 
on developing wise AI, we suggest an alternative to aligning AI with 
specific human values - a task fraught with conceptual and practical 
difficulties. Instead, wise AI systems can thoughtfully navigate complex
 situations, account for diverse human values, and avoid harmful 
actions. We discuss potential approaches to building wise AI, including 
benchmarking metacognitive abilities and training AI systems to employ 
wise reasoning. Prioritizing metacognition in AI research will lead to 
systems that act not only intelligently but also wisely in complex, 
real-world situations.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-11-04</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Imagining and building wise machines</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2411.02478">http://arxiv.org/abs/2411.02478</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>11/6/2024, 9:58:17 AM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2411.02478</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2411.02478">10.48550/arXiv.2411.02478</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2411.02478</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>11/6/2024, 9:58:17 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>11/6/2024, 9:58:17 AM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computers and Society</li>
					<li>Computer Science - Human-Computer Interaction</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_3V5IJFSD">Preprint PDF					</li>
					<li id="item_NC6M8TFV">Snapshot					</li>
				</ul>
			</li>


			<li id="item_8V27GD7V" class="item preprint">
			<h2>Can LLMs make trade-offs involving stipulated pain and pleasure states?</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Geoff Keeling</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Winnie Street</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Martyna Stachaczyk</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Daria Zakharova</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Iulia M. Comsa</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Anastasiya Sakovych</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Isabella Logothesis</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zejia Zhang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Blaise Agüera y Arcas</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jonathan Birch</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Pleasure and pain play an important role in human decision 
making by providing a common currency for resolving motivational 
conflicts. While Large Language Models (LLMs) can generate detailed 
descriptions of pleasure and pain experiences, it is an open question 
whether LLMs can recreate the motivational force of pleasure and pain in
 choice scenarios - a question which may bear on debates about LLM 
sentience, understood as the capacity for valenced experiential states. 
We probed this question using a simple game in which the stated goal is 
to maximise points, but where either the points-maximising option is 
said to incur a pain penalty or a non-points-maximising option is said 
to incur a pleasure reward, providing incentives to deviate from 
points-maximising behaviour. Varying the intensity of the pain penalties
 and pleasure rewards, we found that Claude 3.5 Sonnet, Command R+, 
GPT-4o, and GPT-4o mini each demonstrated at least one trade-off in 
which the majority of responses switched from points-maximisation to 
pain-minimisation or pleasure-maximisation after a critical threshold of
 stipulated pain or pleasure intensity is reached. LLaMa 3.1-405b 
demonstrated some graded sensitivity to stipulated pleasure rewards and 
pain penalties. Gemini 1.5 Pro and PaLM 2 prioritised pain-avoidance 
over points-maximisation regardless of intensity, while tending to 
prioritise points over pleasure regardless of intensity. We discuss the 
implications of these findings for debates about the possibility of LLM 
sentience.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-11-01</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2411.02432">http://arxiv.org/abs/2411.02432</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>11/6/2024, 9:58:54 AM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2411.02432</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2411.02432">10.48550/arXiv.2411.02432</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2411.02432</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>11/6/2024, 9:58:54 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>11/6/2024, 9:58:54 AM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_ASP4TG94">Preprint PDF					</li>
					<li id="item_393M3XAV">Snapshot					</li>
				</ul>
			</li>


			<li id="item_G3S8CQVR" class="item preprint">
			<h2>Lecture I: Governing the Algorithmic City</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Seth Lazar</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>A century ago, John Dewey observed that '[s]team and 
electricity have done more to alter the conditions under which men 
associate together than all the agencies which affected human 
relationships before our time'. In the last few decades, computing 
technologies have had a similar effect. Political philosophy's central 
task is to help us decide how to live together, by analysing our social 
relations, diagnosing their failings, and articulating ideals to guide 
their revision. But these profound social changes have left scarcely a 
dent in the model of social relations that (analytical) political 
philosophers assume. This essay aims to reverse that trend. It first 
builds a model of our novel social relations as they are now, and as 
they are likely to evolved, and then explores how those differences 
affect our theories of how to live together. I introduce the 
'Algorithmic City', the network of algorithmically-mediated social 
relations, then characterise the intermediary power by which it is 
governed. I show how algorithmic governance raises new challenges for 
political philosophy concerning the justification of authority, the 
foundations of procedural legitimacy, and the possibility of 
justificatory neutrality.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-10-17</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Lecture I</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2410.20720">http://arxiv.org/abs/2410.20720</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>11/18/2024, 10:28:27 AM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2410.20720</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2410.20720">10.48550/arXiv.2410.20720</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2410.20720</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>11/18/2024, 10:28:27 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>11/18/2024, 10:28:32 AM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_QY6XZ72H">Preprint PDF					</li>
					<li id="item_JVZSHAEX">Snapshot					</li>
				</ul>
			</li>


			<li id="item_Z7FC4RHL" class="item preprint">
			<h2>Can LLMs advance democratic values?</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Seth Lazar</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lorenzo Manuali</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>LLMs are among the most advanced tools ever devised for 
analysing and generating linguistic content. Democratic deliberation and
 decision-making involve, at several distinct stages, the production and
 analysis of language. So it is natural to ask whether our best tools 
for manipulating language might prove instrumental to one of our most 
important linguistic tasks. Researchers and practitioners have recently 
asked whether LLMs can support democratic deliberation by leveraging 
abilities to summarise content, as well as to aggregate opinion over 
summarised content, and indeed to represent voters by predicting their 
preferences over unseen choices. In this paper, we assess whether using 
LLMs to perform these and related functions really advances the 
democratic values that inspire these experiments. We suggest that the 
record is decidedly mixed. In the presence of background inequality of 
power and resources, as well as deep moral and political disagreement, 
we should be careful not to use LLMs in ways that automate 
non-instrumentally valuable components of the democratic process, or 
else threaten to supplant fair and transparent decision-making 
procedures that are necessary to reconcile competing interests and 
values. However, while we argue that LLMs should be kept well clear of 
formal democratic decision-making processes, we think that they can be 
put to good use in strengthening the informal public sphere: the arena 
that mediates between democratic governments and the polities that they 
serve, in which political communities seek information, form civic 
publics, and hold their leaders to account.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-10-17</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2410.08418">http://arxiv.org/abs/2410.08418</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>11/19/2024, 8:35:14 AM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2410.08418</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2410.08418">10.48550/arXiv.2410.08418</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2410.08418</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>11/19/2024, 8:35:14 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>11/19/2024, 8:35:14 AM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_A6ZRXRD2">Preprint PDF					</li>
					<li id="item_99UCQF9V">Snapshot					</li>
				</ul>
			</li>


			<li id="item_TDQTHHWV" class="item journalArticle">
			<h2>Disagreement, AI alignment, and bargaining</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Harry R. Lloyd</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>New AI technologies have the potential to cause unintended 
harms in diverse domains including warfare, judicial sentencing, 
medicine and governance. One strategy for realising the benefits of AI 
whilst avoiding its potential dangers is to ensure that new AIs are 
properly ‘aligned’ with some form of ‘alignment target.’ One danger of 
this strategy is that–dependent on the alignment target chosen–our AIs 
might optimise for objectives that reflect the values only of a certain 
subset of society, and that do not take into account alternative views 
about what constitutes desirable and safe behaviour for AI agents. In 
response to this problem, several AI ethicists have suggested alignment 
targets that are designed to be sensitive to widespread normative 
disagreement amongst the relevant stakeholders. Authors inspired by 
voting theory have suggested that AIs should be aligned with the 
verdicts of actual or simulated ‘moral parliaments’ whose members 
represent the normative views of the relevant stakeholders. Other 
authors inspired by decision theory and the philosophical literature on 
moral uncertainty have suggested that AIs should maximise socially 
expected choiceworthiness. In this paper, I argue that both of these 
proposals face several important problems. In particular, they fail to 
select attractive ‘compromise options’ in cases where such options are 
available. I go on to propose and defend an alternative, 
bargaining-theoretic alignment target, which avoids the problems 
associated with the voting- and decision-theoretic approaches.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-11-18</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://link.springer.com/10.1007/s11098-024-02224-5">https://link.springer.com/10.1007/s11098-024-02224-5</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>11/19/2024, 8:29:08 AM</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Philosophical Studies</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/s11098-024-02224-5">10.1007/s11098-024-02224-5</a></td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Philos Stud</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0031-8116, 1573-0883</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>11/19/2024, 8:29:08 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>11/19/2024, 8:29:08 AM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_GZVN3DRS">Lloyd - 2024 - Disagreement, AI alignment, and bargaining.pdf					</li>
				</ul>
			</li>


			<li id="item_DFXBN7M9" class="item preprint">
			<h2>Taking AI Welfare Seriously</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Robert Long</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jeff Sebo</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Patrick Butlin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kathleen Finlinson</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kyle Fish</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jacqueline Harding</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jacob Pfau</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Toni Sims</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jonathan Birch</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David Chalmers</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In this report, we argue that there is a realistic possibility
 that some AI systems will be conscious and/or robustly agentic in the 
near future. That means that the prospect of AI welfare and moral 
patienthood, i.e. of AI systems with their own interests and moral 
significance, is no longer an issue only for sci-fi or the distant 
future. It is an issue for the near future, and AI companies and other 
actors have a responsibility to start taking it seriously. We also 
recommend three early steps that AI companies and other actors can take:
 They can (1) acknowledge that AI welfare is an important and difficult 
issue (and ensure that language model outputs do the same), (2) start 
assessing AI systems for evidence of consciousness and robust agency, 
and (3) prepare policies and procedures for treating AI systems with an 
appropriate level of moral concern. To be clear, our argument in this 
report is not that AI systems definitely are, or will be, conscious, 
robustly agentic, or otherwise morally significant. Instead, our 
argument is that there is substantial uncertainty about these 
possibilities, and so we need to improve our understanding of AI welfare
 and our ability to make wise decisions about this issue. Otherwise 
there is a significant risk that we will mishandle decisions about AI 
welfare, mistakenly harming AI systems that matter morally and/or 
mistakenly caring for AI systems that do not.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-11-04</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2411.00986">http://arxiv.org/abs/2411.00986</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>11/17/2024, 6:29:26 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2411.00986 
version: 1</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2411.00986">10.48550/arXiv.2411.00986</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2411.00986</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>11/17/2024, 6:29:26 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>11/17/2024, 6:29:26 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computers and Society</li>
					<li>Quantitative Biology - Neurons and Cognition</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_L57JLMHM">Preprint PDF					</li>
					<li id="item_BT5KQS8I">Snapshot					</li>
				</ul>
			</li>


			<li id="item_G256V8MS" class="item journalArticle">
			<h2>The Code That Binds Us: Navigating the Appropriateness of Human-AI Assistant Relationships</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Arianna Manzini</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Geoff Keeling</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lize Alberts</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Shannon Vallor</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Meredith Ringel Morris</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Iason Gabriel</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The development of increasingly agentic and human-like AI 
assistants, capable of performing a wide range of tasks on user's behalf
 over time, has sparked heightened interest in the nature and bounds of 
human interactions with AI. Such systems may indeed ground a transition 
from task-oriented interactions with AI, at discrete time intervals, to 
ongoing relationships -- where users develop a deeper sense of 
connection with and attachment to the technology. This paper 
investigates what it means for relationships between users and advanced 
AI assistants to be appropriate and proposes a new framework to evaluate
 both users' relationships with AI and developers' design choices. We 
first provide an account of advanced AI assistants, motivating the 
question of appropriate relationships by exploring several distinctive 
features of this technology. These include anthropomorphic cues and the 
longevity of interactions with users, increased AI agency, generality 
and context ambiguity, and the forms and depth of dependence the 
relationship could engender. Drawing upon various ethical traditions, we
 then consider a series of values, including benefit, flourishing, 
autonomy and care, that characterise appropriate human interpersonal 
relationships. These values guide our analysis of how the distinctive 
features of AI assistants may give rise to inappropriate relationships 
with users. Specifically, we discuss a set of concrete risks arising 
from user--AI assistant relationships that: (1) cause direct emotional 
or physical harm to users, (2) limit opportunities for user personal 
development, (3) exploit user emotional dependence, and (4) generate 
material dependencies without adequate commitment to user needs. We 
conclude with a set of recommendations to address these risks.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-10-16</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>The Code That Binds Us</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>ojs.aaai.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://ojs.aaai.org/index.php/AIES/article/view/31694">https://ojs.aaai.org/index.php/AIES/article/view/31694</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>10/28/2024, 9:54:41 AM</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>Copyright (c) 2024 Association for the Advancement of Artificial Intelligence</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>7</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>943-957</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>10/28/2024, 9:54:41 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>10/28/2024, 9:54:41 AM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_7GEVPLMG">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_H453Q7XS" class="item journalArticle">
			<h2>Guidelines for ethical use and acknowledgement of large language models in academic writing</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sebastian Porsdam Mann</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Anuraag A. Vazirani</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mateo Aboy</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Brian D. Earp</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Timo Minssen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>I. Glenn Cohen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Julian Savulescu</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In this Comment, we propose a cumulative set of three 
essential criteria for the ethical use of LLMs in academic writing, and 
present a statement that researchers can quote when submitting 
LLM-assisted manuscripts in order to testify to their adherence to them.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-11-13</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>www.nature.com</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.nature.com/articles/s42256-024-00922-7">https://www.nature.com/articles/s42256-024-00922-7</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>11/15/2024, 2:40:59 PM</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>2024 Springer Nature Limited</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: Nature Publishing Group</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1-3</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Nature Machine Intelligence</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1038/s42256-024-00922-7">10.1038/s42256-024-00922-7</a></td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Nat Mach Intell</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2522-5839</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>11/15/2024, 2:40:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>11/15/2024, 2:40:59 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Ethics</li>
					<li>Policy</li>
					<li>Publishing</li>
				</ul>
			</li>


			<li id="item_4V8V48TE" class="item journalArticle">
			<h2>Promotionalism, orthogonality, and instrumental convergence</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nathaniel Sharadin</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Suppose there are no in-principle restrictions on the contents
 of arbitrarily intelligent agents’ goals. According to “instrumental 
convergence” arguments, potentially scary things follow. I do two things
 in this paper. First, focusing on the influential version of the 
instrumental convergence argument due to Nick Bostrom, I explain why 
such arguments require an account of “promotion”, i.e., an account of 
what it is to “promote” a goal. Then, I consider whether extant accounts
 of promotion in the literature—in particular, probabilistic and 
fit-based views of promotion—can be used to support dangerous 
instrumental convergence. I argue that neither account of promotion can 
do the work. The opposite is true: accepting either account of promotion
 undermines support for instrumental convergence arguments’ 
existentially worrying conclusions. The conclusion is that we needn’t be
 scared—at least not because of arguments concerning instrumental 
convergence.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-10-21</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Springer Link</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1007/s11098-024-02212-9">https://doi.org/10.1007/s11098-024-02212-9</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>11/19/2024, 8:30:19 AM</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Philosophical Studies</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/s11098-024-02212-9">10.1007/s11098-024-02212-9</a></td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Philos Stud</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1573-0883</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>11/19/2024, 8:30:19 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>11/19/2024, 8:30:19 AM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Artificial intelligence</li>
					<li>Existential risk</li>
					<li>Instrumental convergence</li>
					<li>Instrumental rationality</li>
					<li>Orthogonality</li>
					<li>Promotion</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_TDYS598M">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_YF6IVMWE" class="item journalArticle">
			<h2>Beyond Preferences in AI Alignment</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tan Zhi-Xuan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Micah Carroll</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Matija Franklin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hal Ashton</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The dominant practice of AI alignment assumes (1) that 
preferences are an adequate representation of human values, (2) that 
human rationality can be understood in terms of maximizing the 
satisfaction of preferences, and (3) that AI systems should be aligned 
with the preferences of one or more humans to ensure that they behave 
safely and in accordance with our values. Whether implicitly followed or
 explicitly endorsed, these commitments constitute what we term a 
preferentist approach to AI alignment. In this paper, we characterize 
and challenge the preferentist approach, describing conceptual and 
technical alternatives that are ripe for further research. We first 
survey the limits of rational choice theory as a descriptive model, 
explaining how preferences fail to capture the thick semantic content of
 human values, and how utility representations neglect the possible 
incommensurability of those values. We then critique the normativity of 
expected utility theory (EUT) for humans and AI, drawing upon arguments 
showing how rational agents need not comply with EUT, while highlighting
 how EUT is silent on which preferences are normatively acceptable. 
Finally, we argue that these limitations motivate a reframing of the 
targets of AI alignment: Instead of alignment with the preferences of a 
human user, developer, or humanity-writ-large, AI systems should be 
aligned with normative standards appropriate to their social roles, such
 as the role of a general-purpose assistant. Furthermore, these 
standards should be negotiated and agreed upon by all relevant 
stakeholders. On this alternative conception of alignment, a 
multiplicity of AI systems will be able to serve diverse ends, aligned 
with normative standards that promote mutual benefit and limit harm 
despite our plural and divergent values.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-11-09</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Springer Link</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1007/s11098-024-02249-w">https://doi.org/10.1007/s11098-024-02249-w</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>11/19/2024, 8:28:47 AM</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Philosophical Studies</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/s11098-024-02249-w">10.1007/s11098-024-02249-w</a></td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Philos Stud</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1573-0883</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>11/19/2024, 8:28:47 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>11/19/2024, 8:28:55 AM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Artificial Intelligence</li>
					<li>Artificial intelligence</li>
					<li>AI alignment</li>
					<li>Decision theory</li>
					<li>Preferences</li>
					<li>Rational choice theory</li>
					<li>Value theory</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_AVTXV4KG">Full Text PDF					</li>
				</ul>
			</li>

		</ul>
	
</body></html>