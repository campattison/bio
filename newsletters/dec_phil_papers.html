<!DOCTYPE html>
<html><head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<meta http-equiv="Content-Security-Policy" content="script-src 'none'; media-src 'none'">
		<title>Zotero Report</title>
		<link rel="stylesheet" type="text/css" href="data:text/css;base64,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">
		<link rel="stylesheet" type="text/css" media="screen,projection" href="data:text/css;base64,LyogR2VuZXJpYyBzdHlsZXMgKi8KYm9keSB7Cglmb250OiA2Mi41JSBHZW9yZ2lhLCBUaW1lcywgc2VyaWY7Cgl3aWR0aDogNzgwcHg7CgltYXJnaW46IDAgYXV0bzsKfQoKaDIgewoJZm9udC1zaXplOiAxLjVlbTsKCWxpbmUtaGVpZ2h0OiAxLjVlbTsKCWZvbnQtZmFtaWx5OiBHZW9yZ2lhLCBUaW1lcywgc2VyaWY7Cn0KCnAgewoJbGluZS1oZWlnaHQ6IDEuNWVtOwp9CgphOmxpbmssIGE6dmlzaXRlZCB7Cgljb2xvcjogIzkwMDsKfQoKYTpob3ZlciwgYTphY3RpdmUgewoJY29sb3I6ICM3Nzc7Cn0KCgp1bC5yZXBvcnQgewoJZm9udC1zaXplOiAxLjRlbTsKCXdpZHRoOiA2ODBweDsKCW1hcmdpbjogMCBhdXRvOwoJcGFkZGluZzogMjBweCAyMHB4Owp9CgovKiBNZXRhZGF0YSB0YWJsZSAqLwp0YWJsZSB7Cglib3JkZXI6IDFweCAjY2NjIHNvbGlkOwoJb3ZlcmZsb3c6IGF1dG87Cgl3aWR0aDogMTAwJTsKCW1hcmdpbjogLjFlbSBhdXRvIC43NWVtOwoJcGFkZGluZzogMC41ZW07Cn0K">
		<link rel="stylesheet" type="text/css" media="print" href="data:text/css;base64,Ym9keSB7Cglmb250OiAxMnB0ICJUaW1lcyBOZXcgUm9tYW4iLCBUaW1lcywgR2VvcmdpYSwgc2VyaWY7CgltYXJnaW46IDA7Cgl3aWR0aDogYXV0bzsKCWNvbG9yOiBibGFjazsKfQoKLyogUGFnZSBCcmVha3MgKHBhZ2UtYnJlYWstaW5zaWRlIG9ubHkgcmVjb2duaXplZCBieSBPcGVyYSkgKi8KaDEsIGgyLCBoMywgaDQsIGg1LCBoNiB7CglwYWdlLWJyZWFrLWFmdGVyOiBhdm9pZDsKCXBhZ2UtYnJlYWstaW5zaWRlOiBhdm9pZDsKfQoKdWwsIG9sLCBkbCB7CglwYWdlLWJyZWFrLWluc2lkZTogYXZvaWQ7Cgljb2xvci1hZGp1c3Q6IGV4YWN0Owp9CgpoMiB7Cglmb250LXNpemU6IDEuM2VtOwoJbGluZS1oZWlnaHQ6IDEuM2VtOwp9CgphIHsKCWNvbG9yOiAjMDAwOwoJdGV4dC1kZWNvcmF0aW9uOiBub25lOwp9Cg==">
	</head>
	<body>
		<ul class="report combineChildItems">
			<li id="item_AVVE6NDV" class="item preprint">
			<h2>On the Ethical Considerations of Generative Agents</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>N'yoma Diamond</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Soumya Banerjee</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The Generative Agents framework recently developed by Park et 
al. has enabled numerous new technical solutions and problem-solving 
approaches. Academic and industrial interest in generative agents has 
been explosive as a result of the effectiveness of generative agents 
toward emulating human behaviour. However, it is necessary to consider 
the ethical challenges and concerns posed by this technique and its 
usage. In this position paper, we discuss the extant literature that 
evaluate the ethical considerations regarding generative agents and 
similar generative tools, and identify additional concerns of 
significant importance. We also suggest guidelines and necessary future 
research on how to mitigate some of the ethical issues and systemic 
risks associated with generative agents.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-11-28</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2411.19211">http://arxiv.org/abs/2411.19211</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/3/2024, 8:35:33 AM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2411.19211</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2411.19211">10.48550/arXiv.2411.19211</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2411.19211</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/3/2024, 8:35:33 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>12/3/2024, 8:35:37 AM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computers and Society</li>
					<li>Computer Science - Emerging Technologies</li>
					<li>Computer Science - Multiagent Systems</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_ANK5AD4J">Preprint PDF					</li>
					<li id="item_3EY69CDW">Snapshot					</li>
				</ul>
			</li>


			<li id="item_WS2KT2HQ" class="item journalArticle">
			<h2>The linguistic dead zone of value-aligned agency, natural and artificial</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Travis LaCroix</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The value alignment problem for artificial intelligence (AI) 
asks how we can ensure that the “values”—i.e., objective functions—of 
artificial systems are aligned with the values of humanity. In this 
paper, I argue that linguistic communication is a necessary condition 
for robust value alignment. I discuss the consequences that the truth of
 this claim would have for research programmes that attempt to ensure 
value alignment for AI systems—or, more loftily, those programmes that 
seek to design robustly beneficial or ethical artificial agents.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-12-04</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Springer Link</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1007/s11098-024-02257-w">https://doi.org/10.1007/s11098-024-02257-w</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/12/2024, 8:59:26 AM</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Philosophical Studies</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/s11098-024-02257-w">10.1007/s11098-024-02257-w</a></td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Philos Stud</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1573-0883</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/12/2024, 8:59:26 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>12/12/2024, 8:59:38 AM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>AI</li>
					<li>Artificial intelligence</li>
					<li>Artificial Intelligence</li>
					<li>Communication systems</li>
					<li>Coordination</li>
					<li>Incentives</li>
					<li>Information transfer</li>
					<li>Language</li>
					<li>Linguistic communication</li>
					<li>Machine learning</li>
					<li>Normative theory</li>
					<li>Objective functions</li>
					<li>Objectives</li>
					<li>Preferences</li>
					<li>Principal-agent problems</li>
					<li>The value alignment problem</li>
					<li>Values</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_CX9GKQFQ">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_MUHAABZD" class="item preprint">
			<h2>Are Large Language Models Consistent over Value-laden Questions?</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jared Moore</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tanvi Deshpande</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Diyi Yang</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large language models (LLMs) appear to bias their survey 
answers toward certain values. Nonetheless, some argue that LLMs are too
 inconsistent to simulate particular values. Are they? To answer, we 
first define value consistency as the similarity of answers across (1) 
paraphrases of one question, (2) related questions under one topic, (3) 
multiple-choice and open-ended use-cases of one question, and (4) 
multilingual translations of a question to English, Chinese, German, and
 Japanese. We apply these measures to a few large ($&gt;=34b$), open 
LLMs including llama-3, as well as gpt-4o, using eight thousand 
questions spanning more than 300 topics. Unlike prior work, we find that
 models are relatively consistent across paraphrases, use-cases, 
translations, and within a topic. Still, some inconsistencies remain. 
Models are more consistent on uncontroversial topics (e.g., in the U.S.,
 "Thanksgiving") than on controversial ones ("euthanasia"). Base models 
are both more consistent compared to fine-tuned models and are uniform 
in their consistency across topics, while fine-tuned models are more 
inconsistent about some topics ("euthanasia") than others ("women's 
rights") like our human subjects (n=165).</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-07-03</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2407.02996">http://arxiv.org/abs/2407.02996</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/1/2024, 8:44:06 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2407.02996 
version: 1</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2407.02996">10.48550/arXiv.2407.02996</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2407.02996</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/1/2024, 8:44:06 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>12/1/2024, 8:44:09 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_TI935VUJ">Preprint PDF					</li>
					<li id="item_KG4KBYQT">Snapshot					</li>
				</ul>
			</li>


			<li id="item_AFHY9W7H" class="item preprint">
			<h2>The Method of Critical AI Studies, A Propaedeutic</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Fabian Offert</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ranjodh Singh Dhaliwal</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We outline some common methodological issues in the field of 
critical AI studies, including a tendency to overestimate the 
explanatory power of individual samples (the benchmark casuistry), a 
dependency on theoretical frameworks derived from earlier 
conceptualizations of computation (the black box casuistry), and a 
preoccupation with a cause-and-effect model of algorithmic harm (the 
stack casuistry). In the face of these issues, we call for, and point 
towards, a future set of methodologies that might take into account 
existing strengths in the humanistic close analysis of cultural objects.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-11-28</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2411.18833">http://arxiv.org/abs/2411.18833</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/4/2024, 5:20:56 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2411.18833</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2411.18833">10.48550/arXiv.2411.18833</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2411.18833</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/4/2024, 5:20:56 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>12/4/2024, 5:21:00 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_55QWGQEY">Preprint PDF					</li>
					<li id="item_G6H9SQ56">Snapshot					</li>
				</ul>
			</li>


			<li id="item_W9SXS78V" class="item preprint">
			<h2>Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yujin Potter</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Shiyang Lai</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Junsol Kim</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>James Evans</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dawn Song</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>How could LLMs influence our democracy? We investigate LLMs' 
political leanings and the potential influence of LLMs on voters by 
conducting multiple experiments in a U.S. presidential election context.
 Through a voting simulation, we first demonstrate 18 open- and 
closed-weight LLMs' political preference for a Democratic nominee over a
 Republican nominee. We show how this leaning towards the Democratic 
nominee becomes more pronounced in instruction-tuned models compared to 
their base versions by analyzing their responses to candidate-policy 
related questions. We further explore the potential impact of LLMs on 
voter choice by conducting an experiment with 935 U.S. registered 
voters. During the experiments, participants interacted with LLMs 
(Claude-3, Llama-3, and GPT-4) over five exchanges. The experiment 
results show a shift in voter choices towards the Democratic nominee 
following LLM interaction, widening the voting margin from 0.7% to 4.6%,
 even though LLMs were not asked to persuade users to support the 
Democratic nominee during the discourse. This effect is larger than many
 previous studies on the persuasiveness of political campaigns, which 
have shown minimal effects in presidential elections. Many users also 
expressed a desire for further political interaction with LLMs. Which 
aspects of LLM interactions drove these shifts in voter choice requires 
further study. Lastly, we explore how a safety method can make LLMs more
 politically neutral, while raising the question of whether such 
neutrality is truly the path forward.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-11-11</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Hidden Persuaders</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2410.24190">http://arxiv.org/abs/2410.24190</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>11/18/2024, 4:02:40 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2410.24190</td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2410.24190</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>11/18/2024, 4:02:40 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>11/18/2024, 4:02:40 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_XNSWAU7S">Full Text PDF					</li>
					<li id="item_4KIFKYQQ">Snapshot					</li>
				</ul>
			</li>

		</ul>
	
</body></html>