Happy February!

This month, AI research has taken significant strides, with new developments in capability evaluations, AI governance, and interpretability. OpenAI unveiled Operator, a model capable of interacting with standard computer interfaces, while DeepSeek-R1 demonstrated efficient reinforcement learning techniques that rival larger models. Meanwhile, the AI policy landscape is shifting, with France and India co-chairing the AI Action Summit, and the U.S. revising AI regulations.

February Highlights

• Opportunities: Open Philanthropy is seeking proposals to improve AI capability evaluations, offering funding between $200,000 and $5 million. The Cooperative AI Summer School, DISI, and the Transformative AI Course are accepting applications. Several postdoctoral and research positions are open, including at UIUC, GovAI, and Trinity College Dublin.

• New Papers: This month has seen a raft of new philosophy papers dealing with AI. Among them, Seth Lazar’s Governing the Algorithmic City explores the political philosophy of AI-mediated governance, Elsa Kugelberg’s Dating Apps and the Digital Sexual Sphere discusses mediation in deeply personal areas, and Jacqueline Harding and Nathaniel Sharadin’s What is it for a Machine Learning Model to Have a Capability? introduces a new framework for evaluating AI abilities. Other highlights include work on AI deception, the ethics of AI suicide prevention, and a new conditional analysis of AI moral patienthood.

Events:

Philosophy of Artificial Intelligence Network Talks (PAINT)

Dates: Biweekly starting March 3rd, 2025

Time: Mondays at 8:30 am PT / 11:30 am ET / 4:30 pm London / 5:30 pm Berlin

Location: Online

Link: PAINT Series Website

PAINT is a new biweekly international speaker series connecting philosophers working on AI across moral and political philosophy, epistemology, philosophy of mind, and more, led by Sina Fazelpour, Karina Vold and Kathleen Creel. The inaugural lineup includes Emily Sullivan, Jacqueline Harding, Catherine Stinson, Cameron Buckner, Raphaël Milliere, and others.

AI for Animals 2025 

Date: March 1 & 2, 2025Location: University of California, BerkeleyLink: https://www.aiforanimals.org

Sessions will cover topics like animal consideration in AI models, advocacy and technology, and interspecies communication. Other areas include animal law, veterinary medicine, precision livestock farming, digital minds, artificial agents, and more. Featured guests include Jeff Sebo, Rob Long, Jonathan Birch, Peter Singer, and many others.

Workshop on Advancing Fairness in Machine Learning

Date: April 9-10, 2025

Location: Center for Cyber Social Dynamics, University of Kansas, Lawrence, United States

Link: https://philevents.org/event/show/130478

Hosted by the Center for Cyber Social Dynamics, this multidisciplinary workshop aims to foster dialogue on fairness in machine learning across technical, legal, social, and philosophical domains. Topics include algorithmic bias, fairness metrics, ethical foundations, real-world applications, and legal frameworks.

Workshop on Bidirectional Human-AI Alignment

 Date: April 27, 2025 (ICLR Workshop, Hybrid)

 Location: Hybrid (In-person & Virtual)

Website: https://bialign-workshop.github.io/#/

This interdisciplinary workshop redefines the challenge of human-AI alignment by emphasizing a bidirectional approach—not only aligning AI with human specifications but also empowering humans to critically engage with AI systems. Featuring research from Machine Learning (ML), Human-Computer Interaction (HCI), Natural Language Processing (NLP), and related fields, the workshop explores dynamic, evolving interactions between humans and AI.

International Conference on Large-Scale AI Risks

Date: May 26-28, 2025

Location: KU Leuven, Belgium

Link: https://www.kuleuven.be/ethics-kuleuven/chair-ai/conference-ai-risks

Hosted by KU Leuven, this conference focuses on exploring and mitigating the risks posed by large-scale AI systems. It brings together experts in AI safety, governance, and ethics to discuss emerging challenges and policy frameworks.

1st Workshop on Sociotechnical AI Governance (STAIG@CHI 2025)

Date: To be held at CHI 2025 (exact date TBA)

Location: Yokohama, Japan

Link: https://chi-staig.github.io/

STAIG@CHI 2025 aims to build a community that tackles AI governance from a sociotechnical perspective, bringing together researchers and practitioners to drive actionable strategies.

ACM Conference on Fairness, Accountability, and Transparency (FAccT 2025)

Date: June 23-26, 2025 (tentative dates)

Location: Athens, Greece

Link: https://facctconference.org/2025/

FAccT is a premier interdisciplinary conference dedicated to the study of responsible computing. The 2025 edition in Athens will bring together researchers across fields—philosophy, law, technical AI, social sciences—to advance the goals of fairness, accountability, and transparency in computing systems.

Open Opportunities

Request for Proposals: Improving Capability Evaluations

Deadline: April 1, 2025 (rolling review)

Funding Amount: $200,000 to $5 million (6 months to 2 years)

Eligibility: Open to individuals and organizations, including academia, nonprofits, and for-profits

Website: Open Philanthropy RFP

Open Philanthropy is seeking proposals to advance the science of AI capability evaluations, particularly in areas relevant to global catastrophic risk. Reliable AI evaluations are crucial for governance approaches such as “if-then commitments,” where developers take precautionary measures if their models meet certain capability thresholds. However, the current evaluation framework is not mature enough for the high-stakes role it is expected to play.

The Paris Conference on AI & Digital Ethics

Date: June 16-17, 2025

Location: International Conference Centre, Sorbonne University, Paris

Link: https://paris-conference.com/call-for-papers-2025/

Abstract Deadline: March 15, 2025

The third edition of this cross-disciplinary conference focuses on threats to political systems and emerging solutions to rebuild trust in AI-powered societies. The conference features four tracks: controlling cyber-influence, countering information manipulation, exploring blockchain for civic trust, and rebuilding social cohesion. Open to PhD holders and candidates from academia and industry across computational philosophy, ethics, political theory, computer science, international relations, and law. Selected papers will be published in the conference's academic journal. Full paper (5,000 words) due July 31, 2025.

CFP: Artificial Intelligence and Collective Agency

Dates: July 3–4, 2025

Deadline: March 27, 2025

Location: Institute for Ethics in AI, Oxford University (Online & In-Person)

Link: https://philevents.org/event/show/132182?ref=email

The Artificial Intelligence and Collective Agency workshop explores philosophical and interdisciplinary perspectives on AI and group agency. Topics include analogies between AI and corporate or state entities, responsibility gaps, and the role of AI in collective decision-making. Open to researchers in philosophy, business ethics, law, and computer science, as well as policy and industry professionals. Preference for early-career scholars. 

Neel Nanda MATS 8.0 - Admission Procedure + FAQ

Training Phase: March 31 - May 2, 2025 (remote)

Research Phase: June 16 - August 22, 2025 (in-person in Berkeley, remote option available)

Application Deadline: Friday, February 28, 2025, 11:59 PM PT

Link: Neel Nanda MATS 8.0 - Admission Procedure + FAQ

Neel Nanda’s Mechanistic Interpretability (Mech Interp) Research Stream is open for applications for its eighth iteration as part of the MATS program. This is an intensive, high-impact research mentorship for individuals interested in mechanistic interpretability. The program consists of a 5-week online training phase followed by a 10-week in-person research phase in Berkeley for the top scholars.

Cooperative AI Summer School 2025

Date: July 9–13, 2025

Deadline: March 7, 2025

Location: Marlow, near London

Link: https://www.cooperativeai.com/summer-school/summer-school-2025

Applications are now open for the Cooperative AI Summer School, designed for students and early-career professionals in AI, computer science, social sciences, and related fields. This program offers a unique opportunity to engage with leading researchers and peers on topics at the intersection of AI and cooperation.

Intro to Transformative AI 5-Day Course

Location: Remote

Link: https://bluedot.org/intro-to-tai

Deadline: Rolling (Next cohorts: March 3-7, 10-14, 17-21, 24-28)

BlueDot Impact offers an intensive course on transformative AI fundamentals and implications. The program features expert-facilitated group discussions and curated materials over 5 days, requiring 15 hours total commitment. Participants join small discussion groups to explore AI safety concepts. No technical background needed. The course is free with optional donations and includes a completion certificate.

Diverse Intelligences Summer Institute 2025

Location: St Andrews, Scotland

Link: https://disi.org/apply/

Deadline: Rolling from March 1, 2025

The Diverse Intelligences Summer Institute (DISI) invites applications for their summer 2025 program, running July 6-27. Two tracks are available: The Fellows Program seeks scholars from fields including biology, anthropology, AI, cognitive science, computer science, and philosophy for interdisciplinary research. The Storytellers Program welcomes artists working in visual arts, writing, theater, dance, music, and podcasting. Both tracks focus on exploring diverse intelligences through collaborative work. Applications reviewed on a rolling basis starting March 1.

ARIA Mathematics for Safe AI Funding Call

Location: UK (primarily)

Link: https://aria.org.uk/opportunities/mathematics-for-safe-ai

Deadline: February 11, 2025 (13:00 GMT)

The Advanced Research and Invention Agency (ARIA) is offering opportunity seed funding of £10k-£500k for projects in mathematics and AI safety. Led by Programme Director David Dalrymple, the initiative seeks proposals for mathematical approaches to ensure AI systems interact safely with real-world systems. Projects can include formal methods, verified software, mathematical theories of AI, and field-building efforts. Open to individuals and teams globally, though UK-based work preferred. All funded work must be open-source. Applications require a 3-page proposal and basic administrative details.

ACM FAccT 2025 Doctoral Colloquium

Location: Athens, Greece (In-person)

Link: https://facctconference.org/2025/callfordc

Deadline: February 12, 2025 (AoE)

The ACM Conference on Fairness, Accountability and Transparency (FAccT) invites applications for their 2025 Doctoral Colloquium on June 23. Open to PhD, JD, MFA, and other terminal degree students researching fairness, accountability, and transparency in socio-technical systems. Fields include computer science, philosophy, sociology, law, and psychology. The program features mentoring sessions and panels with senior researchers. Applications require a research summary (250 words), career goals statement (250 words), and CV. Priority travel funding available for accepted students. Co-chaired by Kendra Albert, Emily Black, and Roger A. Søraa.

There are several free short-courses on AI agents released this month too: HuggingFace's AI Agents Course offers a comprehensive free program through May 2025 covering agent fundamentals, frameworks like LangChain, and real-world applications, with certification options available upon completion. DeepLearning.AI's Building Towards Computer Use provides a focused 1.5-hour introduction to computer-using AI applications with hands-on examples taught by Anthropic's Colt Steele. Advanced LLM Agents MOOC builds on its successful Fall 2024 run with an in-depth Spring 2025 program for developers and researchers looking to master state-of-the-art LLM agent development.

Jobs

University of Guelph Assistant Professor, Ethics or Applied Ethics

Location: Guelph, CA, N1G 2W1

Link: https://careers.uoguelph.ca/job/Guelph-Assistant-Professor

Deadline: Rolling from March 6, 2025 on. 

The area of specialization is Ethics or Applied Ethics, and the successful candidate will engage in research related to their specialization and teach courses ranging from large service courses to small graduate seminars. The teaching commitment for this position is 5 semester courses per year at the undergraduate and graduate levels.

John and Daria Barry Postdoctoral Associate in AI Ethics

Location: University of Illinois at Urbana-Champaign

Link: https://philosophy.illinois.edu/academics/illinois-forum-human-flourishing-digital-age

Deadline: February 16, 2025, 6:00pm CST

One-year postdoctoral position (with possible renewal) in Ethics and/or Philosophy of AI at UIUC's Forum on Human Flourishing in a Digital Age. Teaching load is 1-2 (three introductory courses total). Minimum salary $62,000. Position requires PhD in Philosophy, Computer Science, or related field. Application materials include letter, CV, writing sample, teaching portfolio, and three references. The successful candidate will contribute to research, teaching, and public engagement activities exploring human flourishing in a digitally-mediated world.

Sloan Foundation Metascience and AI Postdoctoral Fellowship Location: Various eligible institutions (US/Canada preferred) Link: https://sloan.org/programs/digital-technology/aipostdoc-rfpDeadline: April 10, 2025, 5:00pm ET

Two-year postdoctoral fellowship ($250,000 total) for social sciences and humanities researchers studying AI's implications for science and research. Fellows must have completed PhD by start date and not hold a permanent/tenure-track position. Research focuses on how AI is changing research practices, epistemic/ethical implications, and policy responses. Key areas include AI's impact on scientific methods, research pace, explainability, and human-AI collaboration in science. Includes fully-funded 2026 summer school. Application requires research vision statement, approach description, career development plan, CV, mentor support letter, and budget. UK-based applicants should apply through parallel UKRI program. 

GovAI Research Scholar Position and Research Fellow Position

Location: London, UK (remote options available)

Link: https://www.governance.ai/post/research-scholar

Deadline: February 16, 2025 (23:59 GMT)

The Centre for the Governance of AI (GovAI) is offering a one-year visiting Research Scholar position designed to support AI governance researchers and practitioners. The role offers freedom to pursue policy, social science, or technical research; engage with policymakers; and manage applied projects. Focus areas include frontier AI safety frameworks, regulation, international governance, technical governance, and risk assessment. 

The Centre is also seeking Research Fellows to conduct research on important questions in AI governance. This two-year position offers substantial flexibility in project selection, with opportunities for policy research, academic papers, and direct policy advising.

Post-doctoral Researcher Positions (2)

Location: Trinity College Dublin, Ireland

Email: https://aial.ie/pages/hiring/post-doc-researcher/

Deadline: Rolling basis

The AI Accountability Lab (AIAL) is seeking two full-time post-doctoral fellows for a 2-year term to work with Dr. Abeba Birhane on policy translation and AI evaluation. The policy translation role focuses on investigating regulatory loopholes and producing policy insights, while the AI evaluation position involves designing and executing audits of AI systems for bias and harm. Candidates should submit a letter of motivation, CV, and representative work.

Legal Volunteer Project: AI Capability Evaluation

Location: Remote

Link: https://docs.google.com/forms/d/e/1FAIpQLSdjs_8keFGvIlHg-UD-HqkssnstZp6_hXI3WeGNogtzDZawYw/viewform

Deadline: Ongoing

Good Ancestors, an Australian charity focused on AI safety, is partnering with a major AI safety research institute to develop sophisticated legal puzzles that test AI systems’ ability to identify vulnerabilities in legislation. Legal experts are invited to contribute 3-10 hours per puzzle to design and document specific modifications to legislative frameworks, with flexible remote scheduling available.

Links

AI Model Breakthroughs and Insights

DeepSeek-R1’s reinforcement learning-driven approach has demonstrated that efficient training techniques can rival large-scale models. Its success raises questions about the effectiveness of U.S. export controls and the potential spread of AI models with built-in content restrictions. In other learning news, several new papers including one from Stanford and another from GAIR point to the efficacy of using carefully curated, small datasets for reasoning model training. Finally, AllenAI shipped a local AI chat app, and glimpses of GPT-5 are available here. 

Tools, Platforms, and Development

On the technical front, OpenAI has unveiled Operator, an AI capable of interacting with standard computer interfaces using virtual screens and keyboards. On its heels came DeepResearch which is capable of providing long and detailed research reports. Meanwhile, Anthropic has launched Citations, a new feature that grounds AI-generated responses in source documents to reduce hallucinations. As AI capabilities continue to evolve, researchers have introduced “Humanity’s Last Exam”, new benchmark designed to assess AI’s ability to handle expert-level reasoning tasks and MathArena a new math benchmark released in reaction to concerns about contaminated math competitions. IssueBench (for clarifying bias in LLM writing assistance) and ENIGMAEVAL (multimodal reasoning) also released this month, . 

Broader Reflections and News on AI

A major AI Action Summit co-chaired by France and India took place in Paris on February 10-11, focusing on AI opportunity. This follows previous summits at Bletchley Park and Seoul focused on safety. J.D. Vance signaled shifts away from AI safety initiatives in the US, Trump’s latest executive order aims to roll back AI regulations, removing mandatory model reporting requirements while promoting economic competitiveness and national security. Shifting language away from “safety” can also be seen in the AISI’s (now AI Security Institute) renaming. 

Amid these policy changes, OpenAI has introduced ChatGPT Gov, a secure AI platform designed for U.S. government agencies, operating within Microsoft’s cloud infrastructure. At the same time, a New York Times report exposes the vast scope of AI-powered surveillance in U.S. immigration enforcement and Elon Musk makes a bid for OpenAI that may be aimed at raising costs for OpenAI as they push toward for-profit status.

In other news, OpenAI has revealed The Stargate Project, a $500 billion initiative backed by SoftBank, Microsoft, and NVIDIA to build AI supercomputing centers across the U.S. Meanwhile, discussions about AI autonomy take a new turn as Andy Ayrey’s AI foundation establishes legal oversight for Truth Terminal, a project seeking to give AI systems financial and intellectual independence. As these developments unfold, Sebastian Raschka’s review of 2024’s biggest AI breakthroughs suggests that 2025 will prioritize efficiency and multimodal capabilities over sheer scale.

Papers

Dating Apps and the Digital Sexual Sphere

Author: Elsa Kugelberg | American Political Science Review

This paper examines dating apps as powerful intermediaries in the “digital sexual sphere,” shaping intimacy initiation through architecture, moderation, and amplification. Applying a liberal egalitarian framework, Kugelberg argues that apps must respect users’ claims to noninterference, equal standing, and choice improvement. While dating apps offer opportunities for justice, their design often reinforces existing inequalities, warranting regulation and reform.

Governing the Algorithmic City

Author: Seth Lazar | Philosophy & Public Affairs

A landmark paper examining how algorithmic systems that mediate our social relationships raise novel questions for political philosophy. Lazar introduces the concept of the "Algorithmic City"—the network of algorithmically mediated social relations that now shape much of our lives. He argues that while this algorithmic governance shouldn't be eliminated, it must be justified against standards of procedural legitimacy, proper authority, and substantive justification. The paper shows how political philosophy must update its theories of authority, procedural legitimacy, and justificatory neutrality to account for algorithmic governance's distinctive features. 

What is it for a Machine Learning Model to Have a Capability?

Authors: Jacqueline Harding & Nathaniel Sharadin | British Journal for the Philosophy of Science (Preprint)

Harding and Sharadin examine what it means for an ML model to possess a capability, developing a Conditional Analysis of Model Abilities (CAMA): a model has a capability to X if it would reliably succeed at X if it tried. They operationalize this framework to distinguish genuine abilities from coincidental successes, offering a principled approach to model evaluation. CAMA clarifies ML assessment practices and improves fairness in inter-model comparisons.

Construct Validity in Automated Counterterrorism Analysis

Author: Adrian K. Yee | Philosophy of Science

This paper examines the use of machine learning models in counterterrorism analysis, arguing that their application suffers from significant methodological flaws. Yee critiques the operationalization of “terrorist” in artificial intelligence systems, highlighting issues of construct legitimacy, criterion validity, and construct validity. He contends that machine learning models should not be used to identify general classes of terrorists or predict future attacks due to the high risks of false positives and methodological bias. Instead, AI should, at most, be limited to identifying specific individuals with sufficient supporting data.

Models of Rational Agency in Human-Centered AI: The Realist and Constructivist Alternatives

Authors: Jacob Sparks, Ava Thomas Wright | AI and Ethics (Preprint)

This paper examines different approaches to modeling human rational agency in Human-Centered AI systems, arguing that the dominant economic model of human rationality is insufficient compared to realist and constructivist alternatives. Using chatbot fine-tuning as a case study, the authors demonstrate how different philosophical models of human rationality lead to distinct design choices with important practical implications for AI development.

Deception and Manipulation in Generative AI

Author: Christian Tarsney | Philosophical Studies

A timely analysis of AI deception, arguing for stricter standards on AI-generated content compared to human communication. Tarsney develops new frameworks for identifying deception and manipulation based on their influence on human beliefs and choices under “semi-ideal” conditions. The paper proposes solutions such as “extreme transparency” requirements and “defensive systems” that provide contextual information about AI-generated content—particularly relevant for AI safety and alignment research.

Key Concepts and Current Beliefs about AI Moral Patienthood

Author: Robert Long | Preprint

Originally an internal document for Eleos AI Research, this paper offers a foundational framework for assessing AI moral status and welfare. Long examines how AI systems might exhibit consciousness, sentience, and agency—three key features potentially relevant to moral patienthood. The work highlights the need for precise evaluation methods and outlines promising research directions for the emerging field of AI welfare studies.

Propositional Interpretability in Artificial Intelligence

Author: David J. Chalmers | Preprint

 This paper introduces propositional interpretability, a framework for explaining AI behavior by mapping its internal states to propositional attitudes—such as beliefs, desires, and probabilities—essential for AI safety, ethics, and cognitive science. Chalmers explores thought logging as a key challenge and evaluates existing interpretability methods (e.g., causal tracing, probing, sparse auto-encoders) for their potential to systematically track an AI system’s reasoning over time.

Gradual Disempowerment: Systemic Existential Risks from Incremental AI Development

Authors: Jan Kulveit, Raymond Douglas, Nora Ammann, Deger Turan, David Krueger, David Duvenaud | Preprint 

This paper introduces the concept of 'gradual disempowerment' as an alternative to sudden AI takeover scenarios. The authors analyze how incremental AI advances could systematically erode human influence over crucial societal systems through mutually reinforcing effects across economic, cultural, and political domains. They argue that as AI increasingly replaces human participation in these systems, both explicit control mechanisms (like voting) and implicit alignments with human interests may weaken, potentially leading to irreversible loss of human agency. The work emphasizes the need for technical and governance approaches specifically addressing this gradual erosion of human influence.

Actions Speak Louder than Words: Agent Decisions Reveal Implicit Biases in Language Models

Authors: Yuxuan Li, Hirokazu Shirado, Sauvik Das | Preprint 

This methodologically innovative study reveals how language models may harbor implicit biases even when explicit bias has been reduced through alignment. The authors develop a novel technique for uncovering these biases by examining how LLM-generated agents make decisions across different sociodemographic categories. Their findings show that advanced models, despite improvements in reducing explicit bias, can exhibit stronger implicit biases that align directionally with real-world disparities but appear amplified. The work has important implications for AI fairness research and highlights the need for new approaches to addressing deeply embedded biases in language models.

The Logic of Dynamical Systems is Relevant

Authors: Levin Hornischer and Francesco Berto | Mind (Preprint)

This paper establishes a deep connection between dynamical systems theory and relevant logic. The authors show that relevant logic provides the natural logic for reasoning about dynamical systems and their perturbations, while dynamical systems offer a novel applied interpretation of the abstract Routley-Meyer semantics for relevant logic. The work demonstrates how the infamous ternary relation in relevant logic can be naturally understood in terms of system perturbation and evolution, providing both philosophical illumination and practical applications for AI safety and verification.

Suicide, Social Media, and Artificial Intelligence

Authors: Susan Kennedy and Erick Jose Ramirez | Oxford Handbook of Philosophy of Suicide Preprint

A comprehensive examination of the ethical challenges surrounding algorithmic suicide prevention on social media platforms. The authors argue that suicide is a complex phenomenon with varied meanings and rationality across cultures, making it ill-suited for algorithmic intervention. They show how current AI approaches to suicide prevention necessarily embed controversial normative assumptions about suicide's relationship to mental illness and rationality. The paper raises crucial questions about the ethics of imposing culturally-specific values through algorithmic systems deployed globally.

Fully Autonomous AI Agents Should Not be Developed

Authors: Margaret Mitchell, Avijit Ghosh, Alexandra Sasha Luccioni, Giada Pistilli (Hugging Face) | Preprint

This paper argues against the development of fully autonomous AI agents, emphasizing that increased autonomy leads to heightened risks to safety, security, and privacy. The authors outline a hierarchical framework of AI agent levels and analyze the ethical trade-offs at each stage. Their findings suggest that full autonomy—where AI systems can write and execute their own code without predefined constraints—poses significant risks, including misalignment with human values, loss of human control, and emergent harmful behaviors. Instead, they advocate for semi-autonomous systems that retain human oversight, ensuring better risk mitigation while still benefiting from AI-driven automation.

AI Language Model Rivals Expert Ethicist in Perceived Moral Expertise  Authors: Danica Dillion, Debanjan Mondal, Niket Tandon, Kurt Gray | Nature Scientific Reports

This study explores how AI language models are perceived in terms of moral expertise. As large language models (LLMs) increasingly contribute to decision-making, understanding their role in moral reasoning is essential. This research builds on prior assessments of AI moral alignment and introduces new evaluations of AI-generated moral justifications and advice.

Need a quick survey of DeepSeek v3 and DeepSeek R1? Take a look at this Simmons Institute lecture. For broader introductions to ChatGPT and related models, take a look at Andrej Karpathy’s deep dive here.

Curated by Cameron Pattison and Seth Lazar with contributions from the MINT Lab team.

Thanks for reading Normative Philosophy of Computing Newsletter! Subscribe for free to stay up to date.